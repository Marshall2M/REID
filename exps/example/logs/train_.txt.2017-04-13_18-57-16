+ echo Logging output to exps/example/logs/train_.txt.2017-04-13_18-57-16
Logging output to exps/example/logs/train_.txt.2017-04-13_18-57-16
+ python3 ./tools/train_net.py --gpu 0 --cfg exps/example/config.yml --exp_dir example
/home/mopkobka/CourseWork/venv-gml-nn/lib/python3.5/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.
  "This module will be removed in 0.20.", DeprecationWarning)
Called with args:
Namespace(cfg_file='exps/example/config.yml', exp_dir='example', gpu_id=0)
Using config:
{'DATA_DIR': '/home/mopkobka/CourseWork/gml-nn-detector/data',
 'DEDUP_BOXES': 0.0625,
 'DRAW_NET': False,
 'EPS': 1e-14,
 'EXP_DIR': 'example',
 'GPU_ID': 0,
 'MATLAB': 'matlab',
 'MODEL': {'ATTACHED_NETS': [{'ANCHOR_RATIOS': [1.0],
                              'ANCHOR_SCALES': [3, 6, 9, 12],
                              'ANCHOR_SHIFT_NUM_XY': [[1, 1],
                                                      [1, 1],
                                                      [1, 1],
                                                      [1, 1]],
                              'ARCHITECTURE': "m.add_layer(ConvWithActivation(p('conv1'), "
                                              '128, 3))\n'
                                              "m.add_layer(ConvWithActivation(p('conv2'), "
                                              '128, 3, dropout=0.3),\n'
                                              '            parent_layer=-1)\n'
                                              '\n'
                                              "m.add_layer(ConvWithActivation(p('output_cls'), "
                                              '80, 3),\n'
                                              '            '
                                              "parent_layer=p('conv2'))\n"
                                              '\n'
                                              "m.add_layer(ConvWithActivation(p('output_box'), "
                                              '80, 3),\n'
                                              '            '
                                              "parent_layer=p('conv2'))",
                              'BATCHSIZE': 256,
                              'DISABLED': True,
                              'FG_FRACTION': 0.5,
                              'NAME': 'rpn_small',
                              'NEGATIVE_OVERLAP': 0.4,
                              'NMS_THRESH': 0.7,
                              'PARENT_LAYER': 'relu2_2',
                              'POSITIVE_OVERLAP': 0.6,
                              'POST_NMS_TOP_N': 500,
                              'PRE_NMS_TOP_N': 12000,
                              'SQUARE_TARGETS': True,
                              'TOP_NEGATIVE_FRACTION': 0.2,
                              'TYPE': 'RPN'},
                             {'ANCHOR_RATIOS': [1.0],
                              'ANCHOR_SCALES': [2.5,
                                                3.025,
                                                3.66025,
                                                4.4289024999999995,
                                                5.358972025,
                                                6.484356150249999,
                                                7.846070941802498,
                                                9.493745839581022,
                                                11.487432465893036,
                                                13.899793283730574,
                                                16.818749873313998,
                                                20.350687346709933,
                                                24.62433168951902,
                                                29.795441344318014],
                              'ANCHOR_SHIFT_NUM_XY': [[2, 2],
                                                      [2, 2],
                                                      [2, 2],
                                                      [2, 2],
                                                      [1, 1],
                                                      [1, 1],
                                                      [1, 1],
                                                      [1, 1],
                                                      [1, 1],
                                                      [1, 1],
                                                      [1, 1],
                                                      [1, 1],
                                                      [1, 1],
                                                      [1, 1]],
                              'ARCHITECTURE': "m.add_layer(ConvWithActivation(p('conv1'), "
                                              '512, 3))\n'
                                              "m.add_layer(ConvWithActivation(p('conv2'), "
                                              '512, 3, dropout=0.3),\n'
                                              '            parent_layer=-1)\n'
                                              '\n'
                                              "m.add_layer(ConvWithActivation(p('output_cls'), "
                                              '384, 3),\n'
                                              '            '
                                              "parent_layer=p('conv2'))\n"
                                              '\n'
                                              "m.add_layer(ConvWithActivation(p('output_box'), "
                                              '384, 3),\n'
                                              '            '
                                              "parent_layer=p('conv2'))",
                              'BATCHSIZE': 256,
                              'FG_FRACTION': 0.5,
                              'NAME': 'rpn_big',
                              'NEGATIVE_OVERLAP': 0.4,
                              'NMS_THRESH': 0.7,
                              'PARENT_LAYER': 'relu5_3',
                              'POSITIVE_OVERLAP': 0.6,
                              'POST_NMS_TOP_N': 500,
                              'PRE_NMS_TOP_N': 12000,
                              'SQUARE_TARGETS': True,
                              'TOP_NEGATIVE_FRACTION': 0.2,
                              'TYPE': 'RPN'}],
           'NAME': 'faces_example',
           'PRETRAINED_MODEL_CONFIG': '/home/mopkobka/CourseWork/gml-nn-detector/models/vgg16/model.prototxt',
           'WEIGHTS_PATH': '/home/mopkobka/CourseWork/DockerShared/gml-nn/data/NET/VGG16.v2.caffemodel'},
 'MODELS_DIR': '/home/mopkobka/CourseWork/gml-nn-detector/models',
 'PIXEL_MEANS': array([[[ 102.9801,  115.9465,  122.7717]]]),
 'RNG_SEED': 777,
 'ROOT_DIR': '/home/mopkobka/CourseWork/gml-nn-detector',
 'RPN': {'ANCHOR_BASE_SIZE': 16,
         'ANCHOR_RATIOS': [0.5, 1, 2],
         'ANCHOR_SCALES': [8, 16, 32],
         'ANCHOR_SHIFT_NUM_XY': [(1, 1)]},
 'TEST': {'BBOX_REG': True,
          'DATASETS': [{'MAX_SIZE': 1400,
                        'OUTPUT_FILE': 'videoset.json',
                        'PATH': '/home/local/work/data/faces/BigSampleFNTest_p3lr11/imgs',
                        'SCALES': [350, 1000],
                        'TYPE': 'IMAGES_DIR'},
                       {'MAX_SIZE': 700,
                        'OUTPUT_FILE': 'fddb.json',
                        'PATH': '/home/local/work/data/faces/FDDB/FDDB_p3lr11/imgs',
                        'SCALES': [500],
                        'TYPE': 'IMAGES_DIR'}],
          'DENSE_SCAN': False,
          'FINAL_NMS': 0.5,
          'HAS_RPN': False,
          'MAX_PER_IMAGE': 200,
          'NMS': 0.3,
          'PROPOSAL_METHOD': 'selective_search',
          'RPN_MIN_SIZE': 16,
          'RPN_NMS_THRESH': 0.7,
          'RPN_ONLY': False,
          'RPN_POST_NMS_TOP_N': 300,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SVM': False,
          'VIZUALIZATION': {'DRAW_BOXES': True,
                            'DRAW_SCORES': False,
                            'ENABLE': False,
                            'ONLY_WITH_OBJECTS': False,
                            'SCORE_THRESH': 0.5},
          'WITHOUT_UPSAMPLE': True},
 'TRAIN': {'ASPECT_GROUPING': True,
           'BBOX_INSIDE_WEIGHTS': [1.0, 1.0, 1.0, 1.0],
           'BBOX_NORMALIZE_MEANS': [0.0, 0.0, 0.0, 0.0],
           'BBOX_NORMALIZE_STDS': [0.1, 0.1, 0.2, 0.2],
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_REG': True,
           'BBOX_THRESH': 0.5,
           'DATASET': 'default',
           'DATASETS': [{'MAX_SIZE': 700,
                         'PATH': '/home/mopkobka/CourseWork/DockerShared/gml-nn/tiny_dataset',
                         'SCALES': [500],
                         'TYPE': 'BBOX_JSON_MARKING'}],
           'DOUBLE_GENERATE': False,
           'ENABLE_SMART_ORDER': False,
           'GENERATED_FRACTION': 0.0,
           'HAS_RPN': True,
           'PROPOSAL_METHOD': 'selective_search',
           'RATIO_JITTERING': 0.0,
           'REDISTRIBUTE_CLASSES': False,
           'RPN_BATCHSIZE': 256,
           'RPN_BBOX_INSIDE_WEIGHTS': [1.0, 1.0, 1.0, 1.0],
           'RPN_CLOBBER_POSITIVES': False,
           'RPN_FG_FRACTION': 0.5,
           'RPN_LINEAR_START_TNF': 0.1,
           'RPN_LINEAR_TNF_K': -1.0,
           'RPN_MIN_SIZE': 16,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_PERIODIC_TN': 0,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POSITIVE_WEIGHT': -1.0,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 12000,
           'RPN_SQUARE_TARGETS': False,
           'RPN_SQUARE_TARGETS_KY': False,
           'RPN_TOP_NEGATIVE_FRACTION': 0.0,
           'SCALE_JITTERING': 0.0,
           'SNAPSHOT_INFIX': '',
           'SNAPSHOT_ITERS': 5000,
           'SOLVER': {'BASE_LR': 0.001,
                      'DISPLAY': {'AVERAGE_LOSS': 100, 'PERIOD': 20},
                      'IMS_PER_BATCH': 2,
                      'LR_POLICY': {'GAMMA': 0.1,
                                    'STEPS': [70000, 120000],
                                    'TYPE': 'multistep'},
                      'MOMENTUM': 0.9,
                      'TRAIN_ITERS': 90000,
                      'WEIGHT_DECAY': 0.0005},
           'SO_FORCE_BATCHSIZE': 500,
           'SO_FORCE_ROUNDS': 8,
           'SO_GOOD_SKIP_PROB': 0.85,
           'SO_TAIL_LEN': 1000,
           'USE_FLIPPED': True,
           'USE_LMDB': False,
           'USE_PREFETCH': False},
 'USE_GPU_NMS': True,
 'WEIGHTS_PATH': 'default'}
Output will be saved to `/home/mopkobka/CourseWork/gml-nn-detector/exps/example/output/train_13_04_2017_18_57`
Created solver path: /tmp/tmpcuue4ie1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> /tmp/tmpcuue4ie1
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0413 18:57:18.037873 14690 solver.cpp:48] Initializing solver from parameters: 
base_lr: 0.001
display: 20
lr_policy: "multistep"
gamma: 0.1
momentum: 0.9
weight_decay: 0.0005
snapshot: 0
snapshot_prefix: "faces_example"
net_param {
  name: "GeneratedModel_13_04_2017_18_57"
  layer {
    name: "input"
    type: "Python"
    top: "input_data"
    top: "input_im_info"
    top: "input_gt_boxes"
    top: "input_ignored_boxes"
    python_param {
      module: "layers.roi_data_layer"
      layer: "RoIDataLayer"
      param_str: "{\"num_classes\": 2}"
    }
  }
  layer {
    name: "conv1_1"
    type: "Convolution"
    bottom: "input_data"
    top: "conv1_1"
    param {
      lr_mult: 0
      decay_mult: 0
    }
    param {
      lr_mult: 0
      decay_mult: 0
    }
    convolution_param {
      num_output: 64
      pad: 1
      kernel_size: 3
    }
  }
  layer {
    name: "relu1_1"
    type: "ReLU"
    bottom: "conv1_1"
    top: "conv1_1"
  }
  layer {
    name: "conv1_2"
    type: "Convolution"
    bottom: "conv1_1"
    top: "conv1_2"
    param {
      lr_mult: 0
      decay_mult: 0
    }
    param {
      lr_mult: 0
      decay_mult: 0
    }
    convolution_param {
      num_output: 64
      pad: 1
      kernel_size: 3
    }
  }
  layer {
    name: "relu1_2"
    type: "ReLU"
    bottom: "conv1_2"
    top: "conv1_2"
  }
  layer {
    name: "pool1"
    type: "Pooling"
    bottom: "conv1_2"
    top: "pool1"
    pooling_param {
      pool: MAX
      kernel_size: 2
      stride: 2
    }
  }
  layer {
    name: "conv2_1"
    type: "Convolution"
    bottom: "pool1"
    top: "conv2_1"
    param {
      lr_mult: 0
      decay_mult: 0
    }
    param {
      lr_mult: 0
      decay_mult: 0
    }
    convolution_param {
      num_output: 128
      pad: 1
      kernel_size: 3
    }
  }
  layer {
    name: "relu2_1"
    type: "ReLU"
    bottom: "conv2_1"
    top: "conv2_1"
  }
  layer {
    name: "conv2_2"
    type: "Convolution"
    bottom: "conv2_1"
    top: "conv2_2"
    param {
      lr_mult: 0
      decay_mult: 0
    }
    param {
      lr_mult: 0
      decay_mult: 0
    }
    convolution_param {
      num_output: 128
      pad: 1
      kernel_size: 3
    }
  }
  layer {
    name: "relu2_2"
    type: "ReLU"
    bottom: "conv2_2"
    top: "conv2_2"
  }
  layer {
    name: "pool2"
    type: "Pooling"
    bottom: "conv2_2"
    top: "pool2"
    pooling_param {
      pool: MAX
      kernel_size: 2
      stride: 2
    }
  }
  layer {
    name: "conv3_1"
    type: "Convolution"
    bottom: "pool2"
    top: "conv3_1"
    param {
      lr_mult: 1
    }
    param {
      lr_mult: 2
    }
    convolution_param {
      num_output: 256
      pad: 1
      kernel_size: 3
    }
  }
  layer {
    name: "relu3_1"
    type: "ReLU"
    bottom: "conv3_1"
    top: "conv3_1"
  }
  layer {
    name: "conv3_2"
    type: "Convolution"
    bottom: "conv3_1"
    top: "conv3_2"
    param {
      lr_mult: 1
    }
    param {
      lr_mult: 2
    }
    convolution_param {
      num_output: 256
      pad: 1
      kernel_size: 3
    }
  }
  layer {
    name: "relu3_2"
    type: "ReLU"
    bottom: "conv3_2"
    top: "conv3_2"
  }
  layer {
    name: "conv3_3"
    type: "Convolution"
    bottom: "conv3_2"
    top: "conv3_3"
    param {
      lr_mult: 1
    }
    param {
      lr_mult: 2
    }
    convolution_param {
      num_output: 256
      pad: 1
      kernel_size: 3
    }
  }
  layer {
    name: "relu3_3"
    type: "ReLU"
    bottom: "conv3_3"
    top: "conv3_3"
  }
  layer {
    name: "pool3"
    type: "Pooling"
    bottom: "conv3_3"
    top: "pool3"
    pooling_param {
      pool: MAX
      kernel_size: 2
      stride: 2
    }
  }
  layer {
    name: "conv4_1"
    type: "Convolution"
    bottom: "pool3"
    top: "conv4_1"
    param {
      lr_mult: 1
    }
    param {
      lr_mult: 2
    }
    convolution_param {
      num_output: 512
      pad: 1
      kernel_size: 3
    }
  }
  layer {
    name: "relu4_1"
    type: "ReLU"
    bottom: "conv4_1"
    top: "conv4_1"
  }
  layer {
    name: "conv4_2"
    type: "Convolution"
    bottom: "conv4_1"
    top: "conv4_2"
    param {
      lr_mult: 1
    }
    param {
      lr_mult: 2
    }
    convolution_param {
      num_output: 512
      pad: 1
      kernel_size: 3
    }
  }
  layer {
    name: "relu4_2"
    type: "ReLU"
    bottom: "conv4_2"
    top: "conv4_2"
  }
  layer {
    name: "conv4_3"
    type: "Convolution"
    bottom: "conv4_2"
    top: "conv4_3"
    param {
      lr_mult: 1
    }
    param {
      lr_mult: 2
    }
    convolution_param {
      num_output: 512
      pad: 1
      kernel_size: 3
    }
  }
  layer {
    name: "relu4_3"
    type: "ReLU"
    bottom: "conv4_3"
    top: "conv4_3"
  }
  layer {
    name: "pool4"
    type: "Pooling"
    bottom: "conv4_3"
    top: "pool4"
    pooling_param {
      pool: MAX
      kernel_size: 2
      stride: 2
    }
  }
  layer {
    name: "conv5_1"
    type: "Convolution"
    bottom: "pool4"
    top: "conv5_1"
    param {
      lr_mult: 1
    }
    param {
      lr_mult: 2
    }
    convolution_param {
      num_output: 512
      pad: 1
      kernel_size: 3
    }
  }
  layer {
    name: "relu5_1"
    type: "ReLU"
    bottom: "conv5_1"
    top: "conv5_1"
  }
  layer {
    name: "conv5_2"
    type: "Convolution"
    bottom: "conv5_1"
    top: "conv5_2"
    param {
      lr_mult: 1
    }
    param {
      lr_mult: 2
    }
    convolution_param {
      num_output: 512
      pad: 1
      kernel_size: 3
    }
  }
  layer {
    name: "relu5_2"
    type: "ReLU"
    bottom: "conv5_2"
    top: "conv5_2"
  }
  layer {
    name: "conv5_3"
    type: "Convolution"
    bottom: "conv5_2"
    top: "conv5_3"
    param {
      lr_mult: 1
    }
    param {
      lr_mult: 2
    }
    convolution_param {
      num_output: 512
      pad: 1
      kernel_size: 3
    }
  }
  layer {
    name: "relu5_3"
    type: "ReLU"
    bottom: "conv5_3"
    top: "conv5_3"
  }
  layer {
    name: "rpn_big_conv1"
    type: "Convolution"
    bottom: "conv5_3"
    top: "rpn_big_conv1"
    param {
      lr_mult: 1
    }
    param {
      lr_mult: 2
    }
    convolution_param {
      num_output: 512
      pad: 1
      kernel_size: 3
      stride: 1
      weight_filler {
        type: "gaussian"
        mean: 0
        std: 0.01
      }
      bias_filler {
        type: "constant"
      }
    }
  }
  layer {
    name: "rpn_big_conv1_relu"
    type: "ReLU"
    bottom: "rpn_big_conv1"
    top: "rpn_big_conv1"
    relu_param {
      negative_slope: 0
    }
  }
  layer {
    name: "rpn_big_conv2"
    type: "Convolution"
    bottom: "rpn_big_conv1"
    top: "rpn_big_conv2"
    param {
      lr_mult: 1
    }
    param {
      lr_mult: 2
    }
    convolution_param {
      num_output: 512
      pad: 1
      kernel_size: 3
      stride: 1
      weight_filler {
        type: "gaussian"
        mean: 0
        std: 0.01
      }
      bias_filler {
        type: "constant"
      }
    }
  }
  layer {
    name: "rpn_big_conv2_relu"
    type: "ReLU"
    bottom: "rpn_big_conv2"
    top: "rpn_big_conv2"
    relu_param {
      negative_slope: 0
    }
  }
  layer {
    name: "rpn_big_conv2_drop"
    type: "Dropout"
    bottom: "rpn_big_conv2"
    top: "rpn_big_conv2"
    dropout_param {
      dropout_ratio: 0.3
    }
  }
  layer {
    name: "rpn_big_output_cls"
    type: "Convolution"
    bottom: "rpn_big_conv2"
    top: "rpn_big_output_cls"
    param {
      lr_mult: 1
    }
    param {
      lr_mult: 2
    }
    convolution_param {
      num_output: 384
      pad: 1
      kernel_size: 3
      stride: 1
      weight_filler {
        type: "gaussian"
        mean: 0
        std: 0.01
      }
      bias_filler {
        type: "constant"
      }
    }
  }
  layer {
    name: "rpn_big_output_cls_relu"
    type: "ReLU"
    bottom: "rpn_big_output_cls"
    top: "rpn_big_output_cls"
    relu_param {
      negative_slope: 0
    }
  }
  layer {
    name: "rpn_big_cls_score"
    type: "Convolution"
    bottom: "rpn_big_output_cls"
    top: "rpn_big_cls_score"
    param {
      lr_mult: 1
    }
    param {
      lr_mult: 2
    }
    convolution_param {
      num_output: 52
      pad: 0
      kernel_size: 3
      stride: 1
      weight_filler {
        type: "gaussian"
        mean: 0
        std: 0.01
      }
      bias_filler {
        type: "constant"
      }
    }
  }
  layer {
    name: "rpn_big_cls_score_reshape"
    type: "Reshape"
    bottom: "rpn_big_cls_score"
    top: "rpn_big_cls_score_reshape"
    reshape_param {
      shape {
        dim: 0
        dim: 2
        dim: -1
        dim: 0
      }
    }
  }
  layer {
    name: "rpn_big_cls_prob"
    type: "Softmax"
    bottom: "rpn_big_cls_score_reshape"
    top: "rpn_big_cls_prob"
  }
  layer {
    name: "rpn_big_cls_prob_reshape"
    type: "Reshape"
    bottom: "rpn_big_cls_prob"
    top: "rpn_big_cls_prob_reshape"
    reshape_param {
      shape {
        dim: 0
        dim: 52
        dim: -1
        dim: 0
      }
    }
  }
  layer {
    name: "rpn_big_anchor_target"
    type: "Python"
    bottom: "rpn_big_cls_score"
    bottom: "input_gt_boxes"
    bottom: "input_ignored_boxes"
    bottom: "input_im_info"
    bottom: "input_data"
    bottom: "rpn_big_cls_prob_reshape"
    top: "rpn_big_anchor_target_labels"
    top: "rpn_big_anchor_target_bbox_targets"
    top: "rpn_big_anchor_target_inside_weights"
    top: "rpn_big_anchor_target_outside_weights"
    python_param {
      module: "rpn.anchor_target_layer"
      layer: "AnchorTargetLayer"
      param_str: "{\"anchor_scales\": [2.5, 3.025, 3.66025, 4.4289024999999995, 5.358972025, 6.484356150249999, 7.846070941802498, 9.493745839581022, 11.487432465893036, 13.899793283730574, 16.818749873313998, 20.350687346709933, 24.62433168951902, 29.795441344318014], \"num_classes\": 1, \"square_targets\": true, \"batchsize\": 256, \"anchor_shift_num_xy\": [[2, 2], [2, 2], [2, 2], [2, 2], [1, 1], [1, 1], [1, 1], [1, 1], [1, 1], [1, 1], [1, 1], [1, 1], [1, 1], [1, 1]], \"feat_stride\": 16, \"name\": \"rpn_big\", \"negative_overlap\": 0.4, \"tn_fraction\": 0.2, \"positive_overlap\": 0.6, \"fg_fraction\": 0.5, \"anchor_ratios\": [1.0]}"
    }
  }
  layer {
    name: "rpn_big_loss_cls"
    type: "SoftmaxWithLoss"
    bottom: "rpn_big_cls_score_reshape"
    bottom: "rpn_big_anchor_target_labels"
    top: "rpn_big_loss_cls"
    loss_weight: 1
    propagate_down: true
    propagate_down: false
    loss_param {
      ignore_label: -1
      normalization: VALID
    }
  }
  layer {
    name: "rpn_big_output_box"
    type: "Convolution"
    bottom: "rpn_big_conv2"
    top: "rpn_big_output_box"
    param {
      lr_mult: 1
    }
    param {
      lr_mult: 2
    }
    convolution_param {
      num_output: 384
      pad: 1
      kernel_size: 3
      stride: 1
      weight_filler {
        type: "gaussian"
        mean: 0
        std: 0.01
      }
      bias_filler {
        type: "constant"
      }
    }
  }
  layer {
    name: "rpn_big_output_box_relu"
    type: "ReLU"
    bottom: "rpn_big_output_box"
    top: "rpn_big_output_box"
    relu_param {
      negative_slope: 0
    }
  }
  layer {
    name: "rpn_big_bbox_pred"
    type: "Convolution"
    bottom: "rpn_big_output_box"
    top: "rpn_big_bbox_pred"
    param {
      lr_mult: 1
    }
    param {
      lr_mult: 2
    }
    convolution_param {
      num_output: 104
      pad: 0
      kernel_size: 3
      stride: 1
      weight_filler {
        type: "gaussian"
        mean: 0
        std: 0.01
      }
      bias_filler {
        type: "constant"
      }
    }
  }
  layer {
    name: "rpn_big_loss_bbox"
    type: "Python"
    bottom: "rpn_big_bbox_pred"
    bottom: "rpn_big_anchor_target_bbox_targets"
    bottom: "rpn_big_anchor_target_inside_weights"
    bottom: "rpn_big_anchor_target_outside_weights"
    top: "rpn_big_loss_bbox"
    loss_weight: 1
    python_param {
      module: "layers.smooth_l1_loss"
      layer: "SmoothL1LossLayer"
      param_str: "{\"sigma\": 3}"
    }
  }
  layer {
    name: "proposal"
    type: "Python"
    bottom: "rpn_big_cls_prob_reshape"
    bottom: "rpn_big_bbox_pred"
    bottom: "input_im_info"
    top: "rpn_rois"
    python_param {
      module: "rpn.proposal_layer"
      layer: "ProposalLayer"
      param_str: "{\"anchor_shift_num_xy\": [[2, 2], [2, 2], [2, 2], [2, 2], [1, 1], [1, 1], [1, 1], [1, 1], [1, 1], [1, 1], [1, 1], [1, 1], [1, 1], [1, 1]], \"feat_stride\": 16, \"num_classes\": 1, \"anchor_scales\": [2.5, 3.025, 3.66025, 4.4289024999999995, 5.358972025, 6.484356150249999, 7.846070941802498, 9.493745839581022, 11.487432465893036, 13.899793283730574, 16.818749873313998, 20.350687346709933, 24.62433168951902, 29.795441344318014], \"square_targets\": true, \"name\": \"rpn_big\", \"batchsize\": 256, \"anchor_ratios\": [1.0]}"
    }
  }
  layer {
    name: "roi-data"
    type: "Python"
    bottom: "rpn_rois"
    bottom: "input_gt_boxes"
    top: "rois"
    top: "labels"
    top: "bbox_targets"
    top: "bbox_inside_weights"
    top: "bbox_outside_weights"
    python_param {
      module: "rpn.proposal_target_layer"
      layer: "ProposalTargetLayer"
      param_str: "{\"num_classes\": 2, \"bg_aux_label\": 5532}"
    }
  }
  layer {
    name: "roi-pool"
    type: "ROIPooling"
    bottom: "conv5_3"
    bottom: "rois"
    top: "roi-pool"
    roi_pooling_param {
      pooled_h: 7
      pooled_w: 7
      spatial_scale: 0.0625
    }
  }
  layer {
    name: "fc6"
    type: "InnerProduct"
    bottom: "roi-pool"
    top: "fc6"
    param {
      lr_mult: 1
    }
    param {
      lr_mult: 2
    }
    inner_product_param {
      num_output: 4096
    }
  }
  layer {
    name: "relu6"
    type: "ReLU"
    bottom: "fc6"
    top: "fc6"
  }
  layer {
    name: "drop6"
    type: "Dropout"
    bottom: "fc6"
    top: "fc6"
    dropout_param {
      dropout_ratio: 0.5
    }
  }
  layer {
    name: "fc7"
    type: "InnerProduct"
    bottom: "fc6"
    top: "fc7"
    param {
      lr_mult: 1
    }
    param {
      lr_mult: 2
    }
    inner_product_param {
      num_output: 4096
    }
  }
  layer {
    name: "relu7"
    type: "ReLU"
    bottom: "fc7"
    top: "fc7"
  }
  layer {
    name: "drop7"
    type: "Dropout"
    bottom: "fc7"
    top: "fc7"
    dropout_param {
      dropout_ratio: 0.5
    }
  }
  layer {
    name: "bbox_pred"
    type: "InnerProduct"
    bottom: "fc7"
    top: "bbox_pred"
    param {
      lr_mult: 1
    }
    param {
      lr_mult: 2
    }
    inner_product_param {
      num_output: 8
      weight_filler {
        type: "gaussian"
        std: 0.001
      }
      bias_filler {
        type: "constant"
        value: 0
      }
    }
  }
  layer {
    name: "loss_bbox"
    type: "SmoothL1Loss"
    bottom: "bbox_pred"
    bottom: "bbox_targets"
    bottom: "bbox_inside_weights"
    bottom: "bbox_outside_weights"
    top: "loss_bbox"
    loss_weight: 1
  }
  layer {
    name: "feat"
    type: "InnerProduct"
    bottom: "fc7"
    top: "feat"
    param {
      lr_mult: 1
      decay_mult: 1
    }
    param {
      lr_mult: 2
      decay_mult: 0
    }
    inner_product_param {
      num_output: 256
      weight_filler {
        type: "gaussian"
        std: 0.01
      }
      bias_filler {
        type: "constant"
        value: 0
      }
    }
  }
  layer {
    name: "relu8"
    type: "ReLU"
    bottom: "feat"
    top: "feat"
  }
  layer {
    name: "drop8"
    type: "Dropout"
    bottom: "feat"
    top: "feat"
    dropout_param {
      dropout_ratio: 0.5
    }
  }
  layer {
    name: "pid_score"
    type: "InnerProduct"
    bottom: "feat"
    top: "pid_score"
    param {
      lr_mult: 1
      decay_mult: 1
    }
    param {
      lr_mult: 2
      decay_mult: 0
    }
    inner_product_param {
      num_output: 5533
      weight_filler {
        type: "gaussian"
        std: 0.001
      }
      bias_filler {
        type: "constant"
        value: 0
      }
    }
  }
  layer {
    name: "pid_loss"
    type: "SoftmaxWithLoss"
    bottom: "pid_score"
    bottom: "labels"
    top: "pid_loss"
    loss_weight: 1
    propagate_down: true
    propagate_down: false
    loss_param {
      ignore_label: -1
      normalize: true
    }
  }
}
average_loss: 100
stepvalue: 70000
stepvalue: 120000
iter_size: 2
I0413 18:57:18.037988 14690 solver.cpp:86] Creating training net specified in net_param.
I0413 18:57:18.038457 14690 net.cpp:58] Initializing net from parameters: 
name: "GeneratedModel_13_04_2017_18_57"
state {
  phase: TRAIN
}
layer {
  name: "input"
  type: "Python"
  top: "input_data"
  top: "input_im_info"
  top: "input_gt_boxes"
  top: "input_ignored_boxes"
  python_param {
    module: "layers.roi_data_layer"
    layer: "RoIDataLayer"
    param_str: "{\"num_classes\": 2}"
  }
}
layer {
  name: "conv1_1"
  type: "Convolution"
  bottom: "input_data"
  top: "conv1_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "relu1_1"
  type: "ReLU"
  bottom: "conv1_1"
  top: "conv1_1"
}
layer {
  name: "conv1_2"
  type: "Convolution"
  bottom: "conv1_1"
  top: "conv1_2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "relu1_2"
  type: "ReLU"
  bottom: "conv1_2"
  top: "conv1_2"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1_2"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2_1"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "relu2_1"
  type: "ReLU"
  bottom: "conv2_1"
  top: "conv2_1"
}
layer {
  name: "conv2_2"
  type: "Convolution"
  bottom: "conv2_1"
  top: "conv2_2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "relu2_2"
  type: "ReLU"
  bottom: "conv2_2"
  top: "conv2_2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2_2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv3_1"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3_1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "relu3_1"
  type: "ReLU"
  bottom: "conv3_1"
  top: "conv3_1"
}
layer {
  name: "conv3_2"
  type: "Convolution"
  bottom: "conv3_1"
  top: "conv3_2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "relu3_2"
  type: "ReLU"
  bottom: "conv3_2"
  top: "conv3_2"
}
layer {
  name: "conv3_3"
  type: "Convolution"
  bottom: "conv3_2"
  top: "conv3_3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "relu3_3"
  type: "ReLU"
  bottom: "conv3_3"
  top: "conv3_3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3_3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv4_1"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4_1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 512
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "relu4_1"
  type: "ReLU"
  bottom: "conv4_1"
  top: "conv4_1"
}
layer {
  name: "conv4_2"
  type: "Convolution"
  bottom: "conv4_1"
  top: "conv4_2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 512
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "relu4_2"
  type: "ReLU"
  bottom: "conv4_2"
  top: "conv4_2"
}
layer {
  name: "conv4_3"
  type: "Convolution"
  bottom: "conv4_2"
  top: "conv4_3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 512
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "relu4_3"
  type: "ReLU"
  bottom: "conv4_3"
  top: "conv4_3"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4_3"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv5_1"
  type: "Convolution"
  bottom: "pool4"
  top: "conv5_1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 512
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "relu5_1"
  type: "ReLU"
  bottom: "conv5_1"
  top: "conv5_1"
}
layer {
  name: "conv5_2"
  type: "Convolution"
  bottom: "conv5_1"
  top: "conv5_2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 512
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "relu5_2"
  type: "ReLU"
  bottom: "conv5_2"
  top: "conv5_2"
}
layer {
  name: "conv5_3"
  type: "Convolution"
  bottom: "conv5_2"
  top: "conv5_3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 512
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "relu5_3"
  type: "ReLU"
  bottom: "conv5_3"
  top: "conv5_3"
}
layer {
  name: "rpn_big_conv1"
  type: "Convolution"
  bottom: "conv5_3"
  top: "rpn_big_conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 512
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      mean: 0
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "rpn_big_conv1_relu"
  type: "ReLU"
  bottom: "rpn_big_conv1"
  top: "rpn_big_conv1"
  relu_param {
    negative_slope: 0
  }
}
layer {
  name: "rpn_big_conv2"
  type: "Convolution"
  bottom: "rpn_big_conv1"
  top: "rpn_big_conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 512
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      mean: 0
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "rpn_big_conv2_relu"
  type: "ReLU"
  bottom: "rpn_big_conv2"
  top: "rpn_big_conv2"
  relu_param {
    negative_slope: 0
  }
}
layer {
  name: "rpn_big_conv2_drop"
  type: "Dropout"
  bottom: "rpn_big_conv2"
  top: "rpn_big_conv2"
  dropout_param {
    dropout_ratio: 0.3
  }
}
layer {
  name: "rpn_big_output_cls"
  type: "Convolution"
  bottom: "rpn_big_conv2"
  top: "rpn_big_output_cls"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      mean: 0
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "rpn_big_output_cls_relu"
  type: "ReLU"
  bottom: "rpn_big_output_cls"
  top: "rpn_big_output_cls"
  relu_param {
    negative_slope: 0
  }
}
layer {
  name: "rpn_big_cls_score"
  type: "Convolution"
  bottom: "rpn_big_output_cls"
  top: "rpn_big_cls_score"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 52
    pad: 0
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      mean: 0
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "rpn_big_cls_score_reshape"
  type: "Reshape"
  bottom: "rpn_big_cls_score"
  top: "rpn_big_cls_score_reshape"
  reshape_param {
    shape {
      dim: 0
      dim: 2
      dim: -1
      dim: 0
    }
  }
}
layer {
  name: "rpn_big_cls_prob"
  type: "Softmax"
  bottom: "rpn_big_cls_score_reshape"
  top: "rpn_big_cls_prob"
}
layer {
  name: "rpn_big_cls_prob_reshape"
  type: "Reshape"
  bottom: "rpn_big_cls_prob"
  top: "rpn_big_cls_prob_reshape"
  reshape_param {
    shape {
      dim: 0
      dim: 52
      dim: -1
      dim: 0
    }
  }
}
layer {
  name: "rpn_big_anchor_target"
  type: "Python"
  bottom: "rpn_big_cls_score"
  bottom: "input_gt_boxes"
  bottom: "input_ignored_boxes"
  bottom: "input_im_info"
  bottom: "input_data"
  bottom: "rpn_big_cls_prob_reshape"
  top: "rpn_big_anchor_target_labels"
  top: "rpn_big_anchor_target_bbox_targets"
  top: "rpn_big_anchor_target_inside_weights"
  top: "rpn_big_anchor_target_outside_weights"
  python_param {
    module: "rpn.anchor_target_layer"
    layer: "AnchorTargetLayer"
    param_str: "{\"anchor_scales\": [2.5, 3.025, 3.66025, 4.4289024999999995, 5.358972025, 6.484356150249999, 7.846070941802498, 9.493745839581022, 11.487432465893036, 13.899793283730574, 16.818749873313998, 20.350687346709933, 24.62433168951902, 29.795441344318014], \"num_classes\": 1, \"square_targets\": true, \"batchsize\": 256, \"anchor_shift_num_xy\": [[2, 2], [2, 2], [2, 2], [2, 2], [1, 1], [1, 1], [1, 1], [1, 1], [1, 1], [1, 1], [1, 1], [1, 1], [1, 1], [1, 1]], \"feat_stride\": 16, \"name\": \"rpn_big\", \"negative_overlap\": 0.4, \"tn_fraction\": 0.2, \"positive_overlap\": 0.6, \"fg_fraction\": 0.5, \"anchor_ratios\": [1.0]}"
  }
}
layer {
  name: "rpn_big_loss_cls"
  type: "SoftmaxWithLoss"
  bottom: "rpn_big_cls_score_reshape"
  bottom: "rpn_big_anchor_target_labels"
  top: "rpn_big_loss_cls"
  loss_weight: 1
  propagate_down: true
  propagate_down: false
  loss_param {
    ignore_label: -1
    normalization: VALID
  }
}
layer {
  name: "rpn_big_output_box"
  type: "Convolution"
  bottom: "rpn_big_conv2"
  top: "rpn_big_output_box"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      mean: 0
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "rpn_big_output_box_relu"
  type: "ReLU"
  bottom: "rpn_big_output_box"
  top: "rpn_big_output_box"
  relu_param {
    negative_slope: 0
  }
}
layer {
  name: "rpn_big_bbox_pred"
  type: "Convolution"
  bottom: "rpn_big_output_box"
  top: "rpn_big_bbox_pred"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 104
    pad: 0
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      mean: 0
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "rpn_big_loss_bbox"
  type: "Python"
  bottom: "rpn_big_bbox_pred"
  bottom: "rpn_big_anchor_target_bbox_targets"
  bottom: "rpn_big_anchor_target_inside_weights"
  bottom: "rpn_big_anchor_target_outside_weights"
  top: "rpn_big_loss_bbox"
  loss_weight: 1
  python_param {
    module: "layers.smooth_l1_loss"
    layer: "SmoothL1LossLayer"
    param_str: "{\"sigma\": 3}"
  }
}
layer {
  name: "proposal"
  type: "Python"
  bottom: "rpn_big_cls_prob_reshape"
  bottom: "rpn_big_bbox_pred"
  bottom: "input_im_info"
  top: "rpn_rois"
  python_param {
    module: "rpn.proposal_layer"
    layer: "ProposalLayer"
    param_str: "{\"anchor_shift_num_xy\": [[2, 2], [2, 2], [2, 2], [2, 2], [1, 1], [1, 1], [1, 1], [1, 1], [1, 1], [1, 1], [1, 1], [1, 1], [1, 1], [1, 1]], \"feat_stride\": 16, \"num_classes\": 1, \"anchor_scales\": [2.5, 3.025, 3.66025, 4.4289024999999995, 5.358972025, 6.484356150249999, 7.846070941802498, 9.493745839581022, 11.487432465893036, 13.899793283730574, 16.818749873313998, 20.350687346709933, 24.62433168951902, 29.795441344318014], \"square_targets\": true, \"name\": \"rpn_big\", \"batchsize\": 256, \"anchor_ratios\": [1.0]}"
  }
}
layer {
  name: "roi-data"
  type: "Python"
  bottom: "rpn_rois"
  bottom: "input_gt_boxes"
  top: "rois"
  top: "labels"
  top: "bbox_targets"
  top: "bbox_inside_weights"
  top: "bbox_outside_weights"
  python_param {
    module: "rpn.proposal_target_layer"
    layer: "ProposalTargetLayer"
    param_str: "{\"num_classes\": 2, \"bg_aux_label\": 5532}"
  }
}
layer {
  name: "roi-pool"
  type: "ROIPooling"
  bottom: "conv5_3"
  bottom: "rois"
  top: "roi-pool"
  roi_pooling_param {
    pooled_h: 7
    pooled_w: 7
    spatial_scale: 0.0625
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "roi-pool"
  top: "fc6"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 4096
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 4096
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "bbox_pred"
  type: "InnerProduct"
  bottom: "fc7"
  top: "bbox_pred"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 8
    weight_filler {
      type: "gaussian"
      std: 0.001
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "loss_bbox"
  type: "SmoothL1Loss"
  bottom: "bbox_pred"
  bottom: "bbox_targets"
  bottom: "bbox_inside_weights"
  bottom: "bbox_outside_weights"
  top: "loss_bbox"
  loss_weight: 1
}
layer {
  name: "feat"
  type: "InnerProduct"
  bottom: "fc7"
  top: "feat"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 256
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu8"
  type: "ReLU"
  bottom: "feat"
  top: "feat"
}
layer {
  name: "drop8"
  type: "Dropout"
  bottom: "feat"
  top: "feat"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "pid_score"
  type: "InnerProduct"
  bottom: "feat"
  top: "pid_score"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 5533
    weight_filler {
      type: "gaussian"
      std: 0.001
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "pid_loss"
  type: "SoftmaxWithLoss"
  bottom: "pid_score"
  bottom: "labels"
  top: "pid_loss"
  loss_weight: 1
  propagate_down: true
  propagate_down: false
  loss_param {
    ignore_label: -1
    normalize: true
  }
}
I0413 18:57:18.038702 14690 layer_factory.hpp:77] Creating layer input
I0413 18:57:18.040004 14690 net.cpp:100] Creating Layer input
I0413 18:57:18.040017 14690 net.cpp:408] input -> input_data
I0413 18:57:18.040030 14690 net.cpp:408] input -> input_im_info
I0413 18:57:18.040036 14690 net.cpp:408] input -> input_gt_boxes
I0413 18:57:18.040042 14690 net.cpp:408] input -> input_ignored_boxes
RoiDataLayer: name_to_top: {'data': 0, 'ignored_boxes': 3, 'im_info': 1, 'gt_boxes': 2}
I0413 18:57:18.384307 14690 net.cpp:150] Setting up input
I0413 18:57:18.384335 14690 net.cpp:157] Top shape: 1 3 1000 1500 (4500000)
I0413 18:57:18.384343 14690 net.cpp:157] Top shape: 1 3 (3)
I0413 18:57:18.384351 14690 net.cpp:157] Top shape: 1 5 (5)
I0413 18:57:18.384356 14690 net.cpp:157] Top shape: 1 5 (5)
I0413 18:57:18.384361 14690 net.cpp:165] Memory required for data: 18000052
I0413 18:57:18.384369 14690 layer_factory.hpp:77] Creating layer input_data_input_0_split
I0413 18:57:18.384395 14690 net.cpp:100] Creating Layer input_data_input_0_split
I0413 18:57:18.384403 14690 net.cpp:434] input_data_input_0_split <- input_data
I0413 18:57:18.384414 14690 net.cpp:408] input_data_input_0_split -> input_data_input_0_split_0
I0413 18:57:18.384431 14690 net.cpp:408] input_data_input_0_split -> input_data_input_0_split_1
I0413 18:57:18.384485 14690 net.cpp:150] Setting up input_data_input_0_split
I0413 18:57:18.384495 14690 net.cpp:157] Top shape: 1 3 1000 1500 (4500000)
I0413 18:57:18.384510 14690 net.cpp:157] Top shape: 1 3 1000 1500 (4500000)
I0413 18:57:18.384515 14690 net.cpp:165] Memory required for data: 54000052
I0413 18:57:18.384521 14690 layer_factory.hpp:77] Creating layer input_im_info_input_1_split
I0413 18:57:18.384528 14690 net.cpp:100] Creating Layer input_im_info_input_1_split
I0413 18:57:18.384534 14690 net.cpp:434] input_im_info_input_1_split <- input_im_info
I0413 18:57:18.384544 14690 net.cpp:408] input_im_info_input_1_split -> input_im_info_input_1_split_0
I0413 18:57:18.384557 14690 net.cpp:408] input_im_info_input_1_split -> input_im_info_input_1_split_1
I0413 18:57:18.384594 14690 net.cpp:150] Setting up input_im_info_input_1_split
I0413 18:57:18.384604 14690 net.cpp:157] Top shape: 1 3 (3)
I0413 18:57:18.384610 14690 net.cpp:157] Top shape: 1 3 (3)
I0413 18:57:18.384618 14690 net.cpp:165] Memory required for data: 54000076
I0413 18:57:18.384623 14690 layer_factory.hpp:77] Creating layer input_gt_boxes_input_2_split
I0413 18:57:18.384631 14690 net.cpp:100] Creating Layer input_gt_boxes_input_2_split
I0413 18:57:18.384637 14690 net.cpp:434] input_gt_boxes_input_2_split <- input_gt_boxes
I0413 18:57:18.384647 14690 net.cpp:408] input_gt_boxes_input_2_split -> input_gt_boxes_input_2_split_0
I0413 18:57:18.384660 14690 net.cpp:408] input_gt_boxes_input_2_split -> input_gt_boxes_input_2_split_1
I0413 18:57:18.384703 14690 net.cpp:150] Setting up input_gt_boxes_input_2_split
I0413 18:57:18.384713 14690 net.cpp:157] Top shape: 1 5 (5)
I0413 18:57:18.384721 14690 net.cpp:157] Top shape: 1 5 (5)
I0413 18:57:18.384727 14690 net.cpp:165] Memory required for data: 54000116
I0413 18:57:18.384732 14690 layer_factory.hpp:77] Creating layer conv1_1
I0413 18:57:18.384747 14690 net.cpp:100] Creating Layer conv1_1
I0413 18:57:18.384754 14690 net.cpp:434] conv1_1 <- input_data_input_0_split_0
I0413 18:57:18.384768 14690 net.cpp:408] conv1_1 -> conv1_1
I0413 18:57:18.388056 14690 net.cpp:150] Setting up conv1_1
I0413 18:57:18.388092 14690 net.cpp:157] Top shape: 1 64 1000 1500 (96000000)
I0413 18:57:18.388094 14690 net.cpp:165] Memory required for data: 438000116
I0413 18:57:18.388120 14690 layer_factory.hpp:77] Creating layer relu1_1
I0413 18:57:18.388136 14690 net.cpp:100] Creating Layer relu1_1
I0413 18:57:18.388144 14690 net.cpp:434] relu1_1 <- conv1_1
I0413 18:57:18.388155 14690 net.cpp:395] relu1_1 -> conv1_1 (in-place)
I0413 18:57:18.388170 14690 net.cpp:150] Setting up relu1_1
I0413 18:57:18.388180 14690 net.cpp:157] Top shape: 1 64 1000 1500 (96000000)
I0413 18:57:18.388185 14690 net.cpp:165] Memory required for data: 822000116
I0413 18:57:18.388190 14690 layer_factory.hpp:77] Creating layer conv1_2
I0413 18:57:18.388206 14690 net.cpp:100] Creating Layer conv1_2
I0413 18:57:18.388213 14690 net.cpp:434] conv1_2 <- conv1_1
I0413 18:57:18.388223 14690 net.cpp:408] conv1_2 -> conv1_2
I0413 18:57:18.391516 14690 net.cpp:150] Setting up conv1_2
I0413 18:57:18.391556 14690 net.cpp:157] Top shape: 1 64 1000 1500 (96000000)
I0413 18:57:18.391559 14690 net.cpp:165] Memory required for data: 1206000116
I0413 18:57:18.391580 14690 layer_factory.hpp:77] Creating layer relu1_2
I0413 18:57:18.391595 14690 net.cpp:100] Creating Layer relu1_2
I0413 18:57:18.391604 14690 net.cpp:434] relu1_2 <- conv1_2
I0413 18:57:18.391618 14690 net.cpp:395] relu1_2 -> conv1_2 (in-place)
I0413 18:57:18.391634 14690 net.cpp:150] Setting up relu1_2
I0413 18:57:18.391643 14690 net.cpp:157] Top shape: 1 64 1000 1500 (96000000)
I0413 18:57:18.391647 14690 net.cpp:165] Memory required for data: 1590000116
I0413 18:57:18.391654 14690 layer_factory.hpp:77] Creating layer pool1
I0413 18:57:18.391680 14690 net.cpp:100] Creating Layer pool1
I0413 18:57:18.391686 14690 net.cpp:434] pool1 <- conv1_2
I0413 18:57:18.391698 14690 net.cpp:408] pool1 -> pool1
I0413 18:57:18.391784 14690 net.cpp:150] Setting up pool1
I0413 18:57:18.391796 14690 net.cpp:157] Top shape: 1 64 500 750 (24000000)
I0413 18:57:18.391801 14690 net.cpp:165] Memory required for data: 1686000116
I0413 18:57:18.391806 14690 layer_factory.hpp:77] Creating layer conv2_1
I0413 18:57:18.391824 14690 net.cpp:100] Creating Layer conv2_1
I0413 18:57:18.391830 14690 net.cpp:434] conv2_1 <- pool1
I0413 18:57:18.391842 14690 net.cpp:408] conv2_1 -> conv2_1
I0413 18:57:18.393213 14690 net.cpp:150] Setting up conv2_1
I0413 18:57:18.393234 14690 net.cpp:157] Top shape: 1 128 500 750 (48000000)
I0413 18:57:18.393237 14690 net.cpp:165] Memory required for data: 1878000116
I0413 18:57:18.393255 14690 layer_factory.hpp:77] Creating layer relu2_1
I0413 18:57:18.393267 14690 net.cpp:100] Creating Layer relu2_1
I0413 18:57:18.393275 14690 net.cpp:434] relu2_1 <- conv2_1
I0413 18:57:18.393282 14690 net.cpp:395] relu2_1 -> conv2_1 (in-place)
I0413 18:57:18.393295 14690 net.cpp:150] Setting up relu2_1
I0413 18:57:18.393303 14690 net.cpp:157] Top shape: 1 128 500 750 (48000000)
I0413 18:57:18.393308 14690 net.cpp:165] Memory required for data: 2070000116
I0413 18:57:18.393313 14690 layer_factory.hpp:77] Creating layer conv2_2
I0413 18:57:18.393326 14690 net.cpp:100] Creating Layer conv2_2
I0413 18:57:18.393333 14690 net.cpp:434] conv2_2 <- conv2_1
I0413 18:57:18.393343 14690 net.cpp:408] conv2_2 -> conv2_2
I0413 18:57:18.394387 14690 net.cpp:150] Setting up conv2_2
I0413 18:57:18.394410 14690 net.cpp:157] Top shape: 1 128 500 750 (48000000)
I0413 18:57:18.394413 14690 net.cpp:165] Memory required for data: 2262000116
I0413 18:57:18.394426 14690 layer_factory.hpp:77] Creating layer relu2_2
I0413 18:57:18.394440 14690 net.cpp:100] Creating Layer relu2_2
I0413 18:57:18.394446 14690 net.cpp:434] relu2_2 <- conv2_2
I0413 18:57:18.394455 14690 net.cpp:395] relu2_2 -> conv2_2 (in-place)
I0413 18:57:18.394467 14690 net.cpp:150] Setting up relu2_2
I0413 18:57:18.394475 14690 net.cpp:157] Top shape: 1 128 500 750 (48000000)
I0413 18:57:18.394480 14690 net.cpp:165] Memory required for data: 2454000116
I0413 18:57:18.394486 14690 layer_factory.hpp:77] Creating layer pool2
I0413 18:57:18.394498 14690 net.cpp:100] Creating Layer pool2
I0413 18:57:18.394505 14690 net.cpp:434] pool2 <- conv2_2
I0413 18:57:18.394513 14690 net.cpp:408] pool2 -> pool2
I0413 18:57:18.394570 14690 net.cpp:150] Setting up pool2
I0413 18:57:18.394582 14690 net.cpp:157] Top shape: 1 128 250 375 (12000000)
I0413 18:57:18.394587 14690 net.cpp:165] Memory required for data: 2502000116
I0413 18:57:18.394593 14690 layer_factory.hpp:77] Creating layer conv3_1
I0413 18:57:18.394608 14690 net.cpp:100] Creating Layer conv3_1
I0413 18:57:18.394614 14690 net.cpp:434] conv3_1 <- pool2
I0413 18:57:18.394623 14690 net.cpp:408] conv3_1 -> conv3_1
I0413 18:57:18.395870 14690 net.cpp:150] Setting up conv3_1
I0413 18:57:18.395897 14690 net.cpp:157] Top shape: 1 256 250 375 (24000000)
I0413 18:57:18.395900 14690 net.cpp:165] Memory required for data: 2598000116
I0413 18:57:18.395920 14690 layer_factory.hpp:77] Creating layer relu3_1
I0413 18:57:18.395933 14690 net.cpp:100] Creating Layer relu3_1
I0413 18:57:18.395941 14690 net.cpp:434] relu3_1 <- conv3_1
I0413 18:57:18.395951 14690 net.cpp:395] relu3_1 -> conv3_1 (in-place)
I0413 18:57:18.395963 14690 net.cpp:150] Setting up relu3_1
I0413 18:57:18.395972 14690 net.cpp:157] Top shape: 1 256 250 375 (24000000)
I0413 18:57:18.395977 14690 net.cpp:165] Memory required for data: 2694000116
I0413 18:57:18.395982 14690 layer_factory.hpp:77] Creating layer conv3_2
I0413 18:57:18.395998 14690 net.cpp:100] Creating Layer conv3_2
I0413 18:57:18.396005 14690 net.cpp:434] conv3_2 <- conv3_1
I0413 18:57:18.396015 14690 net.cpp:408] conv3_2 -> conv3_2
I0413 18:57:18.397353 14690 net.cpp:150] Setting up conv3_2
I0413 18:57:18.397377 14690 net.cpp:157] Top shape: 1 256 250 375 (24000000)
I0413 18:57:18.397382 14690 net.cpp:165] Memory required for data: 2790000116
I0413 18:57:18.397394 14690 layer_factory.hpp:77] Creating layer relu3_2
I0413 18:57:18.397408 14690 net.cpp:100] Creating Layer relu3_2
I0413 18:57:18.397415 14690 net.cpp:434] relu3_2 <- conv3_2
I0413 18:57:18.397426 14690 net.cpp:395] relu3_2 -> conv3_2 (in-place)
I0413 18:57:18.397439 14690 net.cpp:150] Setting up relu3_2
I0413 18:57:18.397447 14690 net.cpp:157] Top shape: 1 256 250 375 (24000000)
I0413 18:57:18.397454 14690 net.cpp:165] Memory required for data: 2886000116
I0413 18:57:18.397459 14690 layer_factory.hpp:77] Creating layer conv3_3
I0413 18:57:18.397472 14690 net.cpp:100] Creating Layer conv3_3
I0413 18:57:18.397480 14690 net.cpp:434] conv3_3 <- conv3_2
I0413 18:57:18.397490 14690 net.cpp:408] conv3_3 -> conv3_3
I0413 18:57:18.399221 14690 net.cpp:150] Setting up conv3_3
I0413 18:57:18.399247 14690 net.cpp:157] Top shape: 1 256 250 375 (24000000)
I0413 18:57:18.399250 14690 net.cpp:165] Memory required for data: 2982000116
I0413 18:57:18.399262 14690 layer_factory.hpp:77] Creating layer relu3_3
I0413 18:57:18.399276 14690 net.cpp:100] Creating Layer relu3_3
I0413 18:57:18.399283 14690 net.cpp:434] relu3_3 <- conv3_3
I0413 18:57:18.399294 14690 net.cpp:395] relu3_3 -> conv3_3 (in-place)
I0413 18:57:18.399308 14690 net.cpp:150] Setting up relu3_3
I0413 18:57:18.399317 14690 net.cpp:157] Top shape: 1 256 250 375 (24000000)
I0413 18:57:18.399320 14690 net.cpp:165] Memory required for data: 3078000116
I0413 18:57:18.399327 14690 layer_factory.hpp:77] Creating layer pool3
I0413 18:57:18.399336 14690 net.cpp:100] Creating Layer pool3
I0413 18:57:18.399343 14690 net.cpp:434] pool3 <- conv3_3
I0413 18:57:18.399350 14690 net.cpp:408] pool3 -> pool3
I0413 18:57:18.399410 14690 net.cpp:150] Setting up pool3
I0413 18:57:18.399421 14690 net.cpp:157] Top shape: 1 256 125 188 (6016000)
I0413 18:57:18.399426 14690 net.cpp:165] Memory required for data: 3102064116
I0413 18:57:18.399431 14690 layer_factory.hpp:77] Creating layer conv4_1
I0413 18:57:18.399446 14690 net.cpp:100] Creating Layer conv4_1
I0413 18:57:18.399451 14690 net.cpp:434] conv4_1 <- pool3
I0413 18:57:18.399461 14690 net.cpp:408] conv4_1 -> conv4_1
I0413 18:57:18.402353 14690 net.cpp:150] Setting up conv4_1
I0413 18:57:18.402386 14690 net.cpp:157] Top shape: 1 512 125 188 (12032000)
I0413 18:57:18.402390 14690 net.cpp:165] Memory required for data: 3150192116
I0413 18:57:18.402401 14690 layer_factory.hpp:77] Creating layer relu4_1
I0413 18:57:18.402415 14690 net.cpp:100] Creating Layer relu4_1
I0413 18:57:18.402423 14690 net.cpp:434] relu4_1 <- conv4_1
I0413 18:57:18.402436 14690 net.cpp:395] relu4_1 -> conv4_1 (in-place)
I0413 18:57:18.402451 14690 net.cpp:150] Setting up relu4_1
I0413 18:57:18.402457 14690 net.cpp:157] Top shape: 1 512 125 188 (12032000)
I0413 18:57:18.402462 14690 net.cpp:165] Memory required for data: 3198320116
I0413 18:57:18.402467 14690 layer_factory.hpp:77] Creating layer conv4_2
I0413 18:57:18.402479 14690 net.cpp:100] Creating Layer conv4_2
I0413 18:57:18.402485 14690 net.cpp:434] conv4_2 <- conv4_1
I0413 18:57:18.402496 14690 net.cpp:408] conv4_2 -> conv4_2
I0413 18:57:18.407594 14690 net.cpp:150] Setting up conv4_2
I0413 18:57:18.407632 14690 net.cpp:157] Top shape: 1 512 125 188 (12032000)
I0413 18:57:18.407639 14690 net.cpp:165] Memory required for data: 3246448116
I0413 18:57:18.407663 14690 layer_factory.hpp:77] Creating layer relu4_2
I0413 18:57:18.407677 14690 net.cpp:100] Creating Layer relu4_2
I0413 18:57:18.407690 14690 net.cpp:434] relu4_2 <- conv4_2
I0413 18:57:18.407702 14690 net.cpp:395] relu4_2 -> conv4_2 (in-place)
I0413 18:57:18.407717 14690 net.cpp:150] Setting up relu4_2
I0413 18:57:18.407727 14690 net.cpp:157] Top shape: 1 512 125 188 (12032000)
I0413 18:57:18.407733 14690 net.cpp:165] Memory required for data: 3294576116
I0413 18:57:18.407738 14690 layer_factory.hpp:77] Creating layer conv4_3
I0413 18:57:18.407752 14690 net.cpp:100] Creating Layer conv4_3
I0413 18:57:18.407758 14690 net.cpp:434] conv4_3 <- conv4_2
I0413 18:57:18.407768 14690 net.cpp:408] conv4_3 -> conv4_3
I0413 18:57:18.412082 14690 net.cpp:150] Setting up conv4_3
I0413 18:57:18.412127 14690 net.cpp:157] Top shape: 1 512 125 188 (12032000)
I0413 18:57:18.412132 14690 net.cpp:165] Memory required for data: 3342704116
I0413 18:57:18.412145 14690 layer_factory.hpp:77] Creating layer relu4_3
I0413 18:57:18.412156 14690 net.cpp:100] Creating Layer relu4_3
I0413 18:57:18.412163 14690 net.cpp:434] relu4_3 <- conv4_3
I0413 18:57:18.412174 14690 net.cpp:395] relu4_3 -> conv4_3 (in-place)
I0413 18:57:18.412186 14690 net.cpp:150] Setting up relu4_3
I0413 18:57:18.412192 14690 net.cpp:157] Top shape: 1 512 125 188 (12032000)
I0413 18:57:18.412195 14690 net.cpp:165] Memory required for data: 3390832116
I0413 18:57:18.412199 14690 layer_factory.hpp:77] Creating layer pool4
I0413 18:57:18.412206 14690 net.cpp:100] Creating Layer pool4
I0413 18:57:18.412210 14690 net.cpp:434] pool4 <- conv4_3
I0413 18:57:18.412217 14690 net.cpp:408] pool4 -> pool4
I0413 18:57:18.412281 14690 net.cpp:150] Setting up pool4
I0413 18:57:18.412291 14690 net.cpp:157] Top shape: 1 512 63 94 (3032064)
I0413 18:57:18.412294 14690 net.cpp:165] Memory required for data: 3402960372
I0413 18:57:18.412297 14690 layer_factory.hpp:77] Creating layer conv5_1
I0413 18:57:18.412308 14690 net.cpp:100] Creating Layer conv5_1
I0413 18:57:18.412313 14690 net.cpp:434] conv5_1 <- pool4
I0413 18:57:18.412322 14690 net.cpp:408] conv5_1 -> conv5_1
I0413 18:57:18.417033 14690 net.cpp:150] Setting up conv5_1
I0413 18:57:18.417062 14690 net.cpp:157] Top shape: 1 512 63 94 (3032064)
I0413 18:57:18.417074 14690 net.cpp:165] Memory required for data: 3415088628
I0413 18:57:18.417085 14690 layer_factory.hpp:77] Creating layer relu5_1
I0413 18:57:18.417095 14690 net.cpp:100] Creating Layer relu5_1
I0413 18:57:18.417101 14690 net.cpp:434] relu5_1 <- conv5_1
I0413 18:57:18.417109 14690 net.cpp:395] relu5_1 -> conv5_1 (in-place)
I0413 18:57:18.417122 14690 net.cpp:150] Setting up relu5_1
I0413 18:57:18.417129 14690 net.cpp:157] Top shape: 1 512 63 94 (3032064)
I0413 18:57:18.417132 14690 net.cpp:165] Memory required for data: 3427216884
I0413 18:57:18.417136 14690 layer_factory.hpp:77] Creating layer conv5_2
I0413 18:57:18.417146 14690 net.cpp:100] Creating Layer conv5_2
I0413 18:57:18.417150 14690 net.cpp:434] conv5_2 <- conv5_1
I0413 18:57:18.417158 14690 net.cpp:408] conv5_2 -> conv5_2
I0413 18:57:18.421541 14690 net.cpp:150] Setting up conv5_2
I0413 18:57:18.421573 14690 net.cpp:157] Top shape: 1 512 63 94 (3032064)
I0413 18:57:18.421578 14690 net.cpp:165] Memory required for data: 3439345140
I0413 18:57:18.421589 14690 layer_factory.hpp:77] Creating layer relu5_2
I0413 18:57:18.421600 14690 net.cpp:100] Creating Layer relu5_2
I0413 18:57:18.421607 14690 net.cpp:434] relu5_2 <- conv5_2
I0413 18:57:18.421613 14690 net.cpp:395] relu5_2 -> conv5_2 (in-place)
I0413 18:57:18.421624 14690 net.cpp:150] Setting up relu5_2
I0413 18:57:18.421629 14690 net.cpp:157] Top shape: 1 512 63 94 (3032064)
I0413 18:57:18.421633 14690 net.cpp:165] Memory required for data: 3451473396
I0413 18:57:18.421635 14690 layer_factory.hpp:77] Creating layer conv5_3
I0413 18:57:18.421654 14690 net.cpp:100] Creating Layer conv5_3
I0413 18:57:18.421659 14690 net.cpp:434] conv5_3 <- conv5_2
I0413 18:57:18.421665 14690 net.cpp:408] conv5_3 -> conv5_3
I0413 18:57:18.427148 14690 net.cpp:150] Setting up conv5_3
I0413 18:57:18.427198 14690 net.cpp:157] Top shape: 1 512 63 94 (3032064)
I0413 18:57:18.427204 14690 net.cpp:165] Memory required for data: 3463601652
I0413 18:57:18.427220 14690 layer_factory.hpp:77] Creating layer relu5_3
I0413 18:57:18.427234 14690 net.cpp:100] Creating Layer relu5_3
I0413 18:57:18.427242 14690 net.cpp:434] relu5_3 <- conv5_3
I0413 18:57:18.427254 14690 net.cpp:395] relu5_3 -> conv5_3 (in-place)
I0413 18:57:18.427268 14690 net.cpp:150] Setting up relu5_3
I0413 18:57:18.427275 14690 net.cpp:157] Top shape: 1 512 63 94 (3032064)
I0413 18:57:18.427280 14690 net.cpp:165] Memory required for data: 3475729908
I0413 18:57:18.427285 14690 layer_factory.hpp:77] Creating layer conv5_3_relu5_3_0_split
I0413 18:57:18.427294 14690 net.cpp:100] Creating Layer conv5_3_relu5_3_0_split
I0413 18:57:18.427299 14690 net.cpp:434] conv5_3_relu5_3_0_split <- conv5_3
I0413 18:57:18.427309 14690 net.cpp:408] conv5_3_relu5_3_0_split -> conv5_3_relu5_3_0_split_0
I0413 18:57:18.427320 14690 net.cpp:408] conv5_3_relu5_3_0_split -> conv5_3_relu5_3_0_split_1
I0413 18:57:18.427376 14690 net.cpp:150] Setting up conv5_3_relu5_3_0_split
I0413 18:57:18.427390 14690 net.cpp:157] Top shape: 1 512 63 94 (3032064)
I0413 18:57:18.427398 14690 net.cpp:157] Top shape: 1 512 63 94 (3032064)
I0413 18:57:18.427402 14690 net.cpp:165] Memory required for data: 3499986420
I0413 18:57:18.427407 14690 layer_factory.hpp:77] Creating layer rpn_big_conv1
I0413 18:57:18.427428 14690 net.cpp:100] Creating Layer rpn_big_conv1
I0413 18:57:18.427438 14690 net.cpp:434] rpn_big_conv1 <- conv5_3_relu5_3_0_split_0
I0413 18:57:18.427448 14690 net.cpp:408] rpn_big_conv1 -> rpn_big_conv1
I0413 18:57:18.453904 14690 net.cpp:150] Setting up rpn_big_conv1
I0413 18:57:18.453935 14690 net.cpp:157] Top shape: 1 512 63 94 (3032064)
I0413 18:57:18.453948 14690 net.cpp:165] Memory required for data: 3512114676
I0413 18:57:18.453958 14690 layer_factory.hpp:77] Creating layer rpn_big_conv1_relu
I0413 18:57:18.453969 14690 net.cpp:100] Creating Layer rpn_big_conv1_relu
I0413 18:57:18.453974 14690 net.cpp:434] rpn_big_conv1_relu <- rpn_big_conv1
I0413 18:57:18.453982 14690 net.cpp:395] rpn_big_conv1_relu -> rpn_big_conv1 (in-place)
I0413 18:57:18.453992 14690 net.cpp:150] Setting up rpn_big_conv1_relu
I0413 18:57:18.453997 14690 net.cpp:157] Top shape: 1 512 63 94 (3032064)
I0413 18:57:18.454000 14690 net.cpp:165] Memory required for data: 3524242932
I0413 18:57:18.454004 14690 layer_factory.hpp:77] Creating layer rpn_big_conv2
I0413 18:57:18.454013 14690 net.cpp:100] Creating Layer rpn_big_conv2
I0413 18:57:18.454017 14690 net.cpp:434] rpn_big_conv2 <- rpn_big_conv1
I0413 18:57:18.454023 14690 net.cpp:408] rpn_big_conv2 -> rpn_big_conv2
I0413 18:57:18.479491 14690 net.cpp:150] Setting up rpn_big_conv2
I0413 18:57:18.479523 14690 net.cpp:157] Top shape: 1 512 63 94 (3032064)
I0413 18:57:18.479537 14690 net.cpp:165] Memory required for data: 3536371188
I0413 18:57:18.479547 14690 layer_factory.hpp:77] Creating layer rpn_big_conv2_relu
I0413 18:57:18.479558 14690 net.cpp:100] Creating Layer rpn_big_conv2_relu
I0413 18:57:18.479563 14690 net.cpp:434] rpn_big_conv2_relu <- rpn_big_conv2
I0413 18:57:18.479571 14690 net.cpp:395] rpn_big_conv2_relu -> rpn_big_conv2 (in-place)
I0413 18:57:18.479581 14690 net.cpp:150] Setting up rpn_big_conv2_relu
I0413 18:57:18.479585 14690 net.cpp:157] Top shape: 1 512 63 94 (3032064)
I0413 18:57:18.479588 14690 net.cpp:165] Memory required for data: 3548499444
I0413 18:57:18.479591 14690 layer_factory.hpp:77] Creating layer rpn_big_conv2_drop
I0413 18:57:18.479599 14690 net.cpp:100] Creating Layer rpn_big_conv2_drop
I0413 18:57:18.479611 14690 net.cpp:434] rpn_big_conv2_drop <- rpn_big_conv2
I0413 18:57:18.479615 14690 net.cpp:395] rpn_big_conv2_drop -> rpn_big_conv2 (in-place)
I0413 18:57:18.479640 14690 net.cpp:150] Setting up rpn_big_conv2_drop
I0413 18:57:18.479647 14690 net.cpp:157] Top shape: 1 512 63 94 (3032064)
I0413 18:57:18.479650 14690 net.cpp:165] Memory required for data: 3560627700
I0413 18:57:18.479653 14690 layer_factory.hpp:77] Creating layer rpn_big_conv2_rpn_big_conv2_drop_0_split
I0413 18:57:18.479660 14690 net.cpp:100] Creating Layer rpn_big_conv2_rpn_big_conv2_drop_0_split
I0413 18:57:18.479665 14690 net.cpp:434] rpn_big_conv2_rpn_big_conv2_drop_0_split <- rpn_big_conv2
I0413 18:57:18.479671 14690 net.cpp:408] rpn_big_conv2_rpn_big_conv2_drop_0_split -> rpn_big_conv2_rpn_big_conv2_drop_0_split_0
I0413 18:57:18.479681 14690 net.cpp:408] rpn_big_conv2_rpn_big_conv2_drop_0_split -> rpn_big_conv2_rpn_big_conv2_drop_0_split_1
I0413 18:57:18.479720 14690 net.cpp:150] Setting up rpn_big_conv2_rpn_big_conv2_drop_0_split
I0413 18:57:18.479728 14690 net.cpp:157] Top shape: 1 512 63 94 (3032064)
I0413 18:57:18.479733 14690 net.cpp:157] Top shape: 1 512 63 94 (3032064)
I0413 18:57:18.479737 14690 net.cpp:165] Memory required for data: 3584884212
I0413 18:57:18.479740 14690 layer_factory.hpp:77] Creating layer rpn_big_output_cls
I0413 18:57:18.479750 14690 net.cpp:100] Creating Layer rpn_big_output_cls
I0413 18:57:18.479755 14690 net.cpp:434] rpn_big_output_cls <- rpn_big_conv2_rpn_big_conv2_drop_0_split_0
I0413 18:57:18.479770 14690 net.cpp:408] rpn_big_output_cls -> rpn_big_output_cls
I0413 18:57:18.498917 14690 net.cpp:150] Setting up rpn_big_output_cls
I0413 18:57:18.498950 14690 net.cpp:157] Top shape: 1 384 63 94 (2274048)
I0413 18:57:18.498962 14690 net.cpp:165] Memory required for data: 3593980404
I0413 18:57:18.498972 14690 layer_factory.hpp:77] Creating layer rpn_big_output_cls_relu
I0413 18:57:18.498983 14690 net.cpp:100] Creating Layer rpn_big_output_cls_relu
I0413 18:57:18.498988 14690 net.cpp:434] rpn_big_output_cls_relu <- rpn_big_output_cls
I0413 18:57:18.498997 14690 net.cpp:395] rpn_big_output_cls_relu -> rpn_big_output_cls (in-place)
I0413 18:57:18.499008 14690 net.cpp:150] Setting up rpn_big_output_cls_relu
I0413 18:57:18.499013 14690 net.cpp:157] Top shape: 1 384 63 94 (2274048)
I0413 18:57:18.499017 14690 net.cpp:165] Memory required for data: 3603076596
I0413 18:57:18.499019 14690 layer_factory.hpp:77] Creating layer rpn_big_cls_score
I0413 18:57:18.499030 14690 net.cpp:100] Creating Layer rpn_big_cls_score
I0413 18:57:18.499034 14690 net.cpp:434] rpn_big_cls_score <- rpn_big_output_cls
I0413 18:57:18.499042 14690 net.cpp:408] rpn_big_cls_score -> rpn_big_cls_score
I0413 18:57:18.501436 14690 net.cpp:150] Setting up rpn_big_cls_score
I0413 18:57:18.501459 14690 net.cpp:157] Top shape: 1 52 61 92 (291824)
I0413 18:57:18.501462 14690 net.cpp:165] Memory required for data: 3604243892
I0413 18:57:18.501479 14690 layer_factory.hpp:77] Creating layer rpn_big_cls_score_rpn_big_cls_score_0_split
I0413 18:57:18.501489 14690 net.cpp:100] Creating Layer rpn_big_cls_score_rpn_big_cls_score_0_split
I0413 18:57:18.501493 14690 net.cpp:434] rpn_big_cls_score_rpn_big_cls_score_0_split <- rpn_big_cls_score
I0413 18:57:18.501499 14690 net.cpp:408] rpn_big_cls_score_rpn_big_cls_score_0_split -> rpn_big_cls_score_rpn_big_cls_score_0_split_0
I0413 18:57:18.501509 14690 net.cpp:408] rpn_big_cls_score_rpn_big_cls_score_0_split -> rpn_big_cls_score_rpn_big_cls_score_0_split_1
I0413 18:57:18.501554 14690 net.cpp:150] Setting up rpn_big_cls_score_rpn_big_cls_score_0_split
I0413 18:57:18.501562 14690 net.cpp:157] Top shape: 1 52 61 92 (291824)
I0413 18:57:18.501569 14690 net.cpp:157] Top shape: 1 52 61 92 (291824)
I0413 18:57:18.501572 14690 net.cpp:165] Memory required for data: 3606578484
I0413 18:57:18.501577 14690 layer_factory.hpp:77] Creating layer rpn_big_cls_score_reshape
I0413 18:57:18.501590 14690 net.cpp:100] Creating Layer rpn_big_cls_score_reshape
I0413 18:57:18.501595 14690 net.cpp:434] rpn_big_cls_score_reshape <- rpn_big_cls_score_rpn_big_cls_score_0_split_0
I0413 18:57:18.501605 14690 net.cpp:408] rpn_big_cls_score_reshape -> rpn_big_cls_score_reshape
I0413 18:57:18.501641 14690 net.cpp:150] Setting up rpn_big_cls_score_reshape
I0413 18:57:18.501652 14690 net.cpp:157] Top shape: 1 2 1586 92 (291824)
I0413 18:57:18.501657 14690 net.cpp:165] Memory required for data: 3607745780
I0413 18:57:18.501663 14690 layer_factory.hpp:77] Creating layer rpn_big_cls_score_reshape_rpn_big_cls_score_reshape_0_split
I0413 18:57:18.501674 14690 net.cpp:100] Creating Layer rpn_big_cls_score_reshape_rpn_big_cls_score_reshape_0_split
I0413 18:57:18.501682 14690 net.cpp:434] rpn_big_cls_score_reshape_rpn_big_cls_score_reshape_0_split <- rpn_big_cls_score_reshape
I0413 18:57:18.501690 14690 net.cpp:408] rpn_big_cls_score_reshape_rpn_big_cls_score_reshape_0_split -> rpn_big_cls_score_reshape_rpn_big_cls_score_reshape_0_split_0
I0413 18:57:18.501703 14690 net.cpp:408] rpn_big_cls_score_reshape_rpn_big_cls_score_reshape_0_split -> rpn_big_cls_score_reshape_rpn_big_cls_score_reshape_0_split_1
I0413 18:57:18.501750 14690 net.cpp:150] Setting up rpn_big_cls_score_reshape_rpn_big_cls_score_reshape_0_split
I0413 18:57:18.501763 14690 net.cpp:157] Top shape: 1 2 1586 92 (291824)
I0413 18:57:18.501771 14690 net.cpp:157] Top shape: 1 2 1586 92 (291824)
I0413 18:57:18.501777 14690 net.cpp:165] Memory required for data: 3610080372
I0413 18:57:18.501783 14690 layer_factory.hpp:77] Creating layer rpn_big_cls_prob
I0413 18:57:18.501792 14690 net.cpp:100] Creating Layer rpn_big_cls_prob
I0413 18:57:18.501799 14690 net.cpp:434] rpn_big_cls_prob <- rpn_big_cls_score_reshape_rpn_big_cls_score_reshape_0_split_0
I0413 18:57:18.501809 14690 net.cpp:408] rpn_big_cls_prob -> rpn_big_cls_prob
I0413 18:57:18.501879 14690 net.cpp:150] Setting up rpn_big_cls_prob
I0413 18:57:18.501889 14690 net.cpp:157] Top shape: 1 2 1586 92 (291824)
I0413 18:57:18.501895 14690 net.cpp:165] Memory required for data: 3611247668
I0413 18:57:18.501902 14690 layer_factory.hpp:77] Creating layer rpn_big_cls_prob_reshape
I0413 18:57:18.501912 14690 net.cpp:100] Creating Layer rpn_big_cls_prob_reshape
I0413 18:57:18.501919 14690 net.cpp:434] rpn_big_cls_prob_reshape <- rpn_big_cls_prob
I0413 18:57:18.501931 14690 net.cpp:408] rpn_big_cls_prob_reshape -> rpn_big_cls_prob_reshape
I0413 18:57:18.501962 14690 net.cpp:150] Setting up rpn_big_cls_prob_reshape
I0413 18:57:18.501972 14690 net.cpp:157] Top shape: 1 52 61 92 (291824)
I0413 18:57:18.501978 14690 net.cpp:165] Memory required for data: 3612414964
I0413 18:57:18.501984 14690 layer_factory.hpp:77] Creating layer rpn_big_cls_prob_reshape_rpn_big_cls_prob_reshape_0_split
I0413 18:57:18.501993 14690 net.cpp:100] Creating Layer rpn_big_cls_prob_reshape_rpn_big_cls_prob_reshape_0_split
I0413 18:57:18.502001 14690 net.cpp:434] rpn_big_cls_prob_reshape_rpn_big_cls_prob_reshape_0_split <- rpn_big_cls_prob_reshape
I0413 18:57:18.502012 14690 net.cpp:408] rpn_big_cls_prob_reshape_rpn_big_cls_prob_reshape_0_split -> rpn_big_cls_prob_reshape_rpn_big_cls_prob_reshape_0_split_0
I0413 18:57:18.502024 14690 net.cpp:408] rpn_big_cls_prob_reshape_rpn_big_cls_prob_reshape_0_split -> rpn_big_cls_prob_reshape_rpn_big_cls_prob_reshape_0_split_1
I0413 18:57:18.502065 14690 net.cpp:150] Setting up rpn_big_cls_prob_reshape_rpn_big_cls_prob_reshape_0_split
I0413 18:57:18.502076 14690 net.cpp:157] Top shape: 1 52 61 92 (291824)
I0413 18:57:18.502084 14690 net.cpp:157] Top shape: 1 52 61 92 (291824)
I0413 18:57:18.502089 14690 net.cpp:165] Memory required for data: 3614749556
I0413 18:57:18.502095 14690 layer_factory.hpp:77] Creating layer rpn_big_anchor_target
I0413 18:57:18.503417 14690 net.cpp:100] Creating Layer rpn_big_anchor_target
I0413 18:57:18.503437 14690 net.cpp:434] rpn_big_anchor_target <- rpn_big_cls_score_rpn_big_cls_score_0_split_1
I0413 18:57:18.503446 14690 net.cpp:434] rpn_big_anchor_target <- input_gt_boxes_input_2_split_0
I0413 18:57:18.503454 14690 net.cpp:434] rpn_big_anchor_target <- input_ignored_boxes
I0413 18:57:18.503463 14690 net.cpp:434] rpn_big_anchor_target <- input_im_info_input_1_split_0
I0413 18:57:18.503470 14690 net.cpp:434] rpn_big_anchor_target <- input_data_input_0_split_1
I0413 18:57:18.503479 14690 net.cpp:434] rpn_big_anchor_target <- rpn_big_cls_prob_reshape_rpn_big_cls_prob_reshape_0_split_0
I0413 18:57:18.503490 14690 net.cpp:408] rpn_big_anchor_target -> rpn_big_anchor_target_labels
I0413 18:57:18.503506 14690 net.cpp:408] rpn_big_anchor_target -> rpn_big_anchor_target_bbox_targets
I0413 18:57:18.503521 14690 net.cpp:408] rpn_big_anchor_target -> rpn_big_anchor_target_inside_weights
I0413 18:57:18.503538 14690 net.cpp:408] rpn_big_anchor_target -> rpn_big_anchor_target_outside_weights
I0413 18:57:18.510741 14690 net.cpp:150] Setting up rpn_big_anchor_target
I0413 18:57:18.510784 14690 net.cpp:157] Top shape: 1 1 1586 92 (145912)
I0413 18:57:18.510794 14690 net.cpp:157] Top shape: 1 104 61 92 (583648)
I0413 18:57:18.510802 14690 net.cpp:157] Top shape: 1 104 61 92 (583648)
I0413 18:57:18.510807 14690 net.cpp:157] Top shape: 1 104 61 92 (583648)
I0413 18:57:18.510812 14690 net.cpp:165] Memory required for data: 3622336980
I0413 18:57:18.510819 14690 layer_factory.hpp:77] Creating layer rpn_big_loss_cls
I0413 18:57:18.510833 14690 net.cpp:100] Creating Layer rpn_big_loss_cls
I0413 18:57:18.510854 14690 net.cpp:434] rpn_big_loss_cls <- rpn_big_cls_score_reshape_rpn_big_cls_score_reshape_0_split_1
I0413 18:57:18.510866 14690 net.cpp:434] rpn_big_loss_cls <- rpn_big_anchor_target_labels
I0413 18:57:18.510879 14690 net.cpp:408] rpn_big_loss_cls -> rpn_big_loss_cls
I0413 18:57:18.510898 14690 layer_factory.hpp:77] Creating layer rpn_big_loss_cls
I0413 18:57:18.511591 14690 net.cpp:150] Setting up rpn_big_loss_cls
I0413 18:57:18.511610 14690 net.cpp:157] Top shape: (1)
I0413 18:57:18.511623 14690 net.cpp:160]     with loss weight 1
I0413 18:57:18.511634 14690 net.cpp:165] Memory required for data: 3622336984
I0413 18:57:18.511639 14690 layer_factory.hpp:77] Creating layer rpn_big_output_box
I0413 18:57:18.511653 14690 net.cpp:100] Creating Layer rpn_big_output_box
I0413 18:57:18.511659 14690 net.cpp:434] rpn_big_output_box <- rpn_big_conv2_rpn_big_conv2_drop_0_split_1
I0413 18:57:18.511667 14690 net.cpp:408] rpn_big_output_box -> rpn_big_output_box
I0413 18:57:18.532613 14690 net.cpp:150] Setting up rpn_big_output_box
I0413 18:57:18.532641 14690 net.cpp:157] Top shape: 1 384 63 94 (2274048)
I0413 18:57:18.532655 14690 net.cpp:165] Memory required for data: 3631433176
I0413 18:57:18.532665 14690 layer_factory.hpp:77] Creating layer rpn_big_output_box_relu
I0413 18:57:18.532675 14690 net.cpp:100] Creating Layer rpn_big_output_box_relu
I0413 18:57:18.532681 14690 net.cpp:434] rpn_big_output_box_relu <- rpn_big_output_box
I0413 18:57:18.532688 14690 net.cpp:395] rpn_big_output_box_relu -> rpn_big_output_box (in-place)
I0413 18:57:18.532698 14690 net.cpp:150] Setting up rpn_big_output_box_relu
I0413 18:57:18.532703 14690 net.cpp:157] Top shape: 1 384 63 94 (2274048)
I0413 18:57:18.532707 14690 net.cpp:165] Memory required for data: 3640529368
I0413 18:57:18.532712 14690 layer_factory.hpp:77] Creating layer rpn_big_bbox_pred
I0413 18:57:18.532730 14690 net.cpp:100] Creating Layer rpn_big_bbox_pred
I0413 18:57:18.532738 14690 net.cpp:434] rpn_big_bbox_pred <- rpn_big_output_box
I0413 18:57:18.532748 14690 net.cpp:408] rpn_big_bbox_pred -> rpn_big_bbox_pred
I0413 18:57:18.536921 14690 net.cpp:150] Setting up rpn_big_bbox_pred
I0413 18:57:18.536944 14690 net.cpp:157] Top shape: 1 104 61 92 (583648)
I0413 18:57:18.536948 14690 net.cpp:165] Memory required for data: 3642863960
I0413 18:57:18.536957 14690 layer_factory.hpp:77] Creating layer rpn_big_bbox_pred_rpn_big_bbox_pred_0_split
I0413 18:57:18.536967 14690 net.cpp:100] Creating Layer rpn_big_bbox_pred_rpn_big_bbox_pred_0_split
I0413 18:57:18.536972 14690 net.cpp:434] rpn_big_bbox_pred_rpn_big_bbox_pred_0_split <- rpn_big_bbox_pred
I0413 18:57:18.536980 14690 net.cpp:408] rpn_big_bbox_pred_rpn_big_bbox_pred_0_split -> rpn_big_bbox_pred_rpn_big_bbox_pred_0_split_0
I0413 18:57:18.536989 14690 net.cpp:408] rpn_big_bbox_pred_rpn_big_bbox_pred_0_split -> rpn_big_bbox_pred_rpn_big_bbox_pred_0_split_1
I0413 18:57:18.537024 14690 net.cpp:150] Setting up rpn_big_bbox_pred_rpn_big_bbox_pred_0_split
I0413 18:57:18.537034 14690 net.cpp:157] Top shape: 1 104 61 92 (583648)
I0413 18:57:18.537039 14690 net.cpp:157] Top shape: 1 104 61 92 (583648)
I0413 18:57:18.537041 14690 net.cpp:165] Memory required for data: 3647533144
I0413 18:57:18.537045 14690 layer_factory.hpp:77] Creating layer rpn_big_loss_bbox
I0413 18:57:18.537439 14690 net.cpp:100] Creating Layer rpn_big_loss_bbox
I0413 18:57:18.537452 14690 net.cpp:434] rpn_big_loss_bbox <- rpn_big_bbox_pred_rpn_big_bbox_pred_0_split_0
I0413 18:57:18.537458 14690 net.cpp:434] rpn_big_loss_bbox <- rpn_big_anchor_target_bbox_targets
I0413 18:57:18.537463 14690 net.cpp:434] rpn_big_loss_bbox <- rpn_big_anchor_target_inside_weights
I0413 18:57:18.537467 14690 net.cpp:434] rpn_big_loss_bbox <- rpn_big_anchor_target_outside_weights
I0413 18:57:18.537474 14690 net.cpp:408] rpn_big_loss_bbox -> rpn_big_loss_bbox
I0413 18:57:18.539243 14690 net.cpp:150] Setting up rpn_big_loss_bbox
I0413 18:57:18.539276 14690 net.cpp:157] Top shape: 1 (1)
I0413 18:57:18.539279 14690 net.cpp:160]     with loss weight 1
I0413 18:57:18.539291 14690 net.cpp:165] Memory required for data: 3647533148
I0413 18:57:18.539297 14690 layer_factory.hpp:77] Creating layer proposal
I0413 18:57:18.539707 14690 net.cpp:100] Creating Layer proposal
I0413 18:57:18.539719 14690 net.cpp:434] proposal <- rpn_big_cls_prob_reshape_rpn_big_cls_prob_reshape_0_split_1
I0413 18:57:18.539736 14690 net.cpp:434] proposal <- rpn_big_bbox_pred_rpn_big_bbox_pred_0_split_1
I0413 18:57:18.539741 14690 net.cpp:434] proposal <- input_im_info_input_1_split_1
I0413 18:57:18.539748 14690 net.cpp:408] proposal -> rpn_rois
I0413 18:57:18.543998 14690 net.cpp:150] Setting up proposal
I0413 18:57:18.544019 14690 net.cpp:157] Top shape: 1 5 (5)
I0413 18:57:18.544023 14690 net.cpp:165] Memory required for data: 3647533168
I0413 18:57:18.544028 14690 layer_factory.hpp:77] Creating layer roi-data
I0413 18:57:18.544404 14690 net.cpp:100] Creating Layer roi-data
I0413 18:57:18.544414 14690 net.cpp:434] roi-data <- rpn_rois
I0413 18:57:18.544421 14690 net.cpp:434] roi-data <- input_gt_boxes_input_2_split_1
I0413 18:57:18.544428 14690 net.cpp:408] roi-data -> rois
I0413 18:57:18.544437 14690 net.cpp:408] roi-data -> labels
I0413 18:57:18.544445 14690 net.cpp:408] roi-data -> bbox_targets
I0413 18:57:18.544451 14690 net.cpp:408] roi-data -> bbox_inside_weights
I0413 18:57:18.544457 14690 net.cpp:408] roi-data -> bbox_outside_weights
I0413 18:57:18.545392 14690 net.cpp:150] Setting up roi-data
I0413 18:57:18.545404 14690 net.cpp:157] Top shape: 1 5 (5)
I0413 18:57:18.545408 14690 net.cpp:157] Top shape: 1 1 (1)
I0413 18:57:18.545413 14690 net.cpp:157] Top shape: 1 8 (8)
I0413 18:57:18.545415 14690 net.cpp:157] Top shape: 1 8 (8)
I0413 18:57:18.545418 14690 net.cpp:157] Top shape: 1 8 (8)
I0413 18:57:18.545421 14690 net.cpp:165] Memory required for data: 3647533288
I0413 18:57:18.545425 14690 layer_factory.hpp:77] Creating layer roi-pool
I0413 18:57:18.545435 14690 net.cpp:100] Creating Layer roi-pool
I0413 18:57:18.545439 14690 net.cpp:434] roi-pool <- conv5_3_relu5_3_0_split_1
I0413 18:57:18.545445 14690 net.cpp:434] roi-pool <- rois
I0413 18:57:18.545450 14690 net.cpp:408] roi-pool -> roi-pool
I0413 18:57:18.545459 14690 roi_pooling_layer.cpp:33] Spatial scale: 0.0625
I0413 18:57:18.545470 14690 roi_pooling_layer.cpp:34] Box scale: 1
I0413 18:57:18.545508 14690 net.cpp:150] Setting up roi-pool
I0413 18:57:18.545517 14690 net.cpp:157] Top shape: 1 512 7 7 (25088)
I0413 18:57:18.545521 14690 net.cpp:165] Memory required for data: 3647633640
I0413 18:57:18.545523 14690 layer_factory.hpp:77] Creating layer fc6
I0413 18:57:18.545531 14690 net.cpp:100] Creating Layer fc6
I0413 18:57:18.545533 14690 net.cpp:434] fc6 <- roi-pool
I0413 18:57:18.545539 14690 net.cpp:408] fc6 -> fc6
I0413 18:57:18.710559 14690 net.cpp:150] Setting up fc6
I0413 18:57:18.710670 14690 net.cpp:157] Top shape: 1 4096 (4096)
I0413 18:57:18.710685 14690 net.cpp:165] Memory required for data: 3647650024
I0413 18:57:18.710705 14690 layer_factory.hpp:77] Creating layer relu6
I0413 18:57:18.710719 14690 net.cpp:100] Creating Layer relu6
I0413 18:57:18.710727 14690 net.cpp:434] relu6 <- fc6
I0413 18:57:18.710739 14690 net.cpp:395] relu6 -> fc6 (in-place)
I0413 18:57:18.710753 14690 net.cpp:150] Setting up relu6
I0413 18:57:18.710760 14690 net.cpp:157] Top shape: 1 4096 (4096)
I0413 18:57:18.710764 14690 net.cpp:165] Memory required for data: 3647666408
I0413 18:57:18.710770 14690 layer_factory.hpp:77] Creating layer drop6
I0413 18:57:18.710780 14690 net.cpp:100] Creating Layer drop6
I0413 18:57:18.710784 14690 net.cpp:434] drop6 <- fc6
I0413 18:57:18.710789 14690 net.cpp:395] drop6 -> fc6 (in-place)
I0413 18:57:18.710814 14690 net.cpp:150] Setting up drop6
I0413 18:57:18.710821 14690 net.cpp:157] Top shape: 1 4096 (4096)
I0413 18:57:18.710824 14690 net.cpp:165] Memory required for data: 3647682792
I0413 18:57:18.710827 14690 layer_factory.hpp:77] Creating layer fc7
I0413 18:57:18.710834 14690 net.cpp:100] Creating Layer fc7
I0413 18:57:18.710837 14690 net.cpp:434] fc7 <- fc6
I0413 18:57:18.710842 14690 net.cpp:408] fc7 -> fc7
I0413 18:57:18.738687 14690 net.cpp:150] Setting up fc7
I0413 18:57:18.738723 14690 net.cpp:157] Top shape: 1 4096 (4096)
I0413 18:57:18.738736 14690 net.cpp:165] Memory required for data: 3647699176
I0413 18:57:18.738747 14690 layer_factory.hpp:77] Creating layer relu7
I0413 18:57:18.738770 14690 net.cpp:100] Creating Layer relu7
I0413 18:57:18.738775 14690 net.cpp:434] relu7 <- fc7
I0413 18:57:18.738785 14690 net.cpp:395] relu7 -> fc7 (in-place)
I0413 18:57:18.738795 14690 net.cpp:150] Setting up relu7
I0413 18:57:18.738798 14690 net.cpp:157] Top shape: 1 4096 (4096)
I0413 18:57:18.738801 14690 net.cpp:165] Memory required for data: 3647715560
I0413 18:57:18.738804 14690 layer_factory.hpp:77] Creating layer drop7
I0413 18:57:18.738811 14690 net.cpp:100] Creating Layer drop7
I0413 18:57:18.738813 14690 net.cpp:434] drop7 <- fc7
I0413 18:57:18.738818 14690 net.cpp:395] drop7 -> fc7 (in-place)
I0413 18:57:18.738839 14690 net.cpp:150] Setting up drop7
I0413 18:57:18.738844 14690 net.cpp:157] Top shape: 1 4096 (4096)
I0413 18:57:18.738847 14690 net.cpp:165] Memory required for data: 3647731944
I0413 18:57:18.738850 14690 layer_factory.hpp:77] Creating layer fc7_drop7_0_split
I0413 18:57:18.738857 14690 net.cpp:100] Creating Layer fc7_drop7_0_split
I0413 18:57:18.738859 14690 net.cpp:434] fc7_drop7_0_split <- fc7
I0413 18:57:18.738867 14690 net.cpp:408] fc7_drop7_0_split -> fc7_drop7_0_split_0
I0413 18:57:18.738873 14690 net.cpp:408] fc7_drop7_0_split -> fc7_drop7_0_split_1
I0413 18:57:18.738912 14690 net.cpp:150] Setting up fc7_drop7_0_split
I0413 18:57:18.738919 14690 net.cpp:157] Top shape: 1 4096 (4096)
I0413 18:57:18.738924 14690 net.cpp:157] Top shape: 1 4096 (4096)
I0413 18:57:18.738927 14690 net.cpp:165] Memory required for data: 3647764712
I0413 18:57:18.738931 14690 layer_factory.hpp:77] Creating layer bbox_pred
I0413 18:57:18.738939 14690 net.cpp:100] Creating Layer bbox_pred
I0413 18:57:18.738941 14690 net.cpp:434] bbox_pred <- fc7_drop7_0_split_0
I0413 18:57:18.738947 14690 net.cpp:408] bbox_pred -> bbox_pred
I0413 18:57:18.739367 14690 net.cpp:150] Setting up bbox_pred
I0413 18:57:18.739374 14690 net.cpp:157] Top shape: 1 8 (8)
I0413 18:57:18.739377 14690 net.cpp:165] Memory required for data: 3647764744
I0413 18:57:18.739383 14690 layer_factory.hpp:77] Creating layer loss_bbox
I0413 18:57:18.739390 14690 net.cpp:100] Creating Layer loss_bbox
I0413 18:57:18.739393 14690 net.cpp:434] loss_bbox <- bbox_pred
I0413 18:57:18.739398 14690 net.cpp:434] loss_bbox <- bbox_targets
I0413 18:57:18.739403 14690 net.cpp:434] loss_bbox <- bbox_inside_weights
I0413 18:57:18.739406 14690 net.cpp:434] loss_bbox <- bbox_outside_weights
I0413 18:57:18.739413 14690 net.cpp:408] loss_bbox -> loss_bbox
I0413 18:57:18.739475 14690 net.cpp:150] Setting up loss_bbox
I0413 18:57:18.739482 14690 net.cpp:157] Top shape: (1)
I0413 18:57:18.739485 14690 net.cpp:160]     with loss weight 1
I0413 18:57:18.739496 14690 net.cpp:165] Memory required for data: 3647764748
I0413 18:57:18.739500 14690 layer_factory.hpp:77] Creating layer feat
I0413 18:57:18.739506 14690 net.cpp:100] Creating Layer feat
I0413 18:57:18.739511 14690 net.cpp:434] feat <- fc7_drop7_0_split_1
I0413 18:57:18.739516 14690 net.cpp:408] feat -> feat
I0413 18:57:18.752584 14690 net.cpp:150] Setting up feat
I0413 18:57:18.752612 14690 net.cpp:157] Top shape: 1 256 (256)
I0413 18:57:18.752624 14690 net.cpp:165] Memory required for data: 3647765772
I0413 18:57:18.752635 14690 layer_factory.hpp:77] Creating layer relu8
I0413 18:57:18.752646 14690 net.cpp:100] Creating Layer relu8
I0413 18:57:18.752652 14690 net.cpp:434] relu8 <- feat
I0413 18:57:18.752660 14690 net.cpp:395] relu8 -> feat (in-place)
I0413 18:57:18.752670 14690 net.cpp:150] Setting up relu8
I0413 18:57:18.752673 14690 net.cpp:157] Top shape: 1 256 (256)
I0413 18:57:18.752676 14690 net.cpp:165] Memory required for data: 3647766796
I0413 18:57:18.752679 14690 layer_factory.hpp:77] Creating layer drop8
I0413 18:57:18.752686 14690 net.cpp:100] Creating Layer drop8
I0413 18:57:18.752688 14690 net.cpp:434] drop8 <- feat
I0413 18:57:18.752693 14690 net.cpp:395] drop8 -> feat (in-place)
I0413 18:57:18.752712 14690 net.cpp:150] Setting up drop8
I0413 18:57:18.752718 14690 net.cpp:157] Top shape: 1 256 (256)
I0413 18:57:18.752720 14690 net.cpp:165] Memory required for data: 3647767820
I0413 18:57:18.752723 14690 layer_factory.hpp:77] Creating layer pid_score
I0413 18:57:18.752732 14690 net.cpp:100] Creating Layer pid_score
I0413 18:57:18.752734 14690 net.cpp:434] pid_score <- feat
I0413 18:57:18.752740 14690 net.cpp:408] pid_score -> pid_score
I0413 18:57:18.770342 14690 net.cpp:150] Setting up pid_score
I0413 18:57:18.770371 14690 net.cpp:157] Top shape: 1 5533 (5533)
I0413 18:57:18.770383 14690 net.cpp:165] Memory required for data: 3647789952
I0413 18:57:18.770395 14690 layer_factory.hpp:77] Creating layer pid_loss
I0413 18:57:18.770407 14690 net.cpp:100] Creating Layer pid_loss
I0413 18:57:18.770411 14690 net.cpp:434] pid_loss <- pid_score
I0413 18:57:18.770418 14690 net.cpp:434] pid_loss <- labels
I0413 18:57:18.770424 14690 net.cpp:408] pid_loss -> pid_loss
I0413 18:57:18.770437 14690 layer_factory.hpp:77] Creating layer pid_loss
I0413 18:57:18.770531 14690 net.cpp:150] Setting up pid_loss
I0413 18:57:18.770539 14690 net.cpp:157] Top shape: (1)
I0413 18:57:18.770541 14690 net.cpp:160]     with loss weight 1
I0413 18:57:18.770552 14690 net.cpp:165] Memory required for data: 3647789956
I0413 18:57:18.770555 14690 net.cpp:226] pid_loss needs backward computation.
I0413 18:57:18.770560 14690 net.cpp:226] pid_score needs backward computation.
I0413 18:57:18.770563 14690 net.cpp:226] drop8 needs backward computation.
I0413 18:57:18.770567 14690 net.cpp:226] relu8 needs backward computation.
I0413 18:57:18.770570 14690 net.cpp:226] feat needs backward computation.
I0413 18:57:18.770573 14690 net.cpp:226] loss_bbox needs backward computation.
I0413 18:57:18.770577 14690 net.cpp:226] bbox_pred needs backward computation.
I0413 18:57:18.770581 14690 net.cpp:226] fc7_drop7_0_split needs backward computation.
I0413 18:57:18.770584 14690 net.cpp:226] drop7 needs backward computation.
I0413 18:57:18.770587 14690 net.cpp:226] relu7 needs backward computation.
I0413 18:57:18.770591 14690 net.cpp:226] fc7 needs backward computation.
I0413 18:57:18.770593 14690 net.cpp:226] drop6 needs backward computation.
I0413 18:57:18.770596 14690 net.cpp:226] relu6 needs backward computation.
I0413 18:57:18.770599 14690 net.cpp:226] fc6 needs backward computation.
I0413 18:57:18.770602 14690 net.cpp:226] roi-pool needs backward computation.
I0413 18:57:18.770606 14690 net.cpp:226] roi-data needs backward computation.
I0413 18:57:18.770611 14690 net.cpp:226] proposal needs backward computation.
I0413 18:57:18.770615 14690 net.cpp:226] rpn_big_loss_bbox needs backward computation.
I0413 18:57:18.770620 14690 net.cpp:226] rpn_big_bbox_pred_rpn_big_bbox_pred_0_split needs backward computation.
I0413 18:57:18.770623 14690 net.cpp:226] rpn_big_bbox_pred needs backward computation.
I0413 18:57:18.770627 14690 net.cpp:226] rpn_big_output_box_relu needs backward computation.
I0413 18:57:18.770629 14690 net.cpp:226] rpn_big_output_box needs backward computation.
I0413 18:57:18.770633 14690 net.cpp:226] rpn_big_loss_cls needs backward computation.
I0413 18:57:18.770639 14690 net.cpp:226] rpn_big_anchor_target needs backward computation.
I0413 18:57:18.770647 14690 net.cpp:226] rpn_big_cls_prob_reshape_rpn_big_cls_prob_reshape_0_split needs backward computation.
I0413 18:57:18.770650 14690 net.cpp:226] rpn_big_cls_prob_reshape needs backward computation.
I0413 18:57:18.770653 14690 net.cpp:226] rpn_big_cls_prob needs backward computation.
I0413 18:57:18.770658 14690 net.cpp:226] rpn_big_cls_score_reshape_rpn_big_cls_score_reshape_0_split needs backward computation.
I0413 18:57:18.770663 14690 net.cpp:226] rpn_big_cls_score_reshape needs backward computation.
I0413 18:57:18.770666 14690 net.cpp:226] rpn_big_cls_score_rpn_big_cls_score_0_split needs backward computation.
I0413 18:57:18.770669 14690 net.cpp:226] rpn_big_cls_score needs backward computation.
I0413 18:57:18.770673 14690 net.cpp:226] rpn_big_output_cls_relu needs backward computation.
I0413 18:57:18.770676 14690 net.cpp:226] rpn_big_output_cls needs backward computation.
I0413 18:57:18.770680 14690 net.cpp:226] rpn_big_conv2_rpn_big_conv2_drop_0_split needs backward computation.
I0413 18:57:18.770684 14690 net.cpp:226] rpn_big_conv2_drop needs backward computation.
I0413 18:57:18.770686 14690 net.cpp:226] rpn_big_conv2_relu needs backward computation.
I0413 18:57:18.770689 14690 net.cpp:226] rpn_big_conv2 needs backward computation.
I0413 18:57:18.770694 14690 net.cpp:226] rpn_big_conv1_relu needs backward computation.
I0413 18:57:18.770696 14690 net.cpp:226] rpn_big_conv1 needs backward computation.
I0413 18:57:18.770699 14690 net.cpp:226] conv5_3_relu5_3_0_split needs backward computation.
I0413 18:57:18.770702 14690 net.cpp:226] relu5_3 needs backward computation.
I0413 18:57:18.770705 14690 net.cpp:226] conv5_3 needs backward computation.
I0413 18:57:18.770709 14690 net.cpp:226] relu5_2 needs backward computation.
I0413 18:57:18.770712 14690 net.cpp:226] conv5_2 needs backward computation.
I0413 18:57:18.770715 14690 net.cpp:226] relu5_1 needs backward computation.
I0413 18:57:18.770719 14690 net.cpp:226] conv5_1 needs backward computation.
I0413 18:57:18.770721 14690 net.cpp:226] pool4 needs backward computation.
I0413 18:57:18.770725 14690 net.cpp:226] relu4_3 needs backward computation.
I0413 18:57:18.770727 14690 net.cpp:226] conv4_3 needs backward computation.
I0413 18:57:18.770730 14690 net.cpp:226] relu4_2 needs backward computation.
I0413 18:57:18.770733 14690 net.cpp:226] conv4_2 needs backward computation.
I0413 18:57:18.770736 14690 net.cpp:226] relu4_1 needs backward computation.
I0413 18:57:18.770740 14690 net.cpp:226] conv4_1 needs backward computation.
I0413 18:57:18.770742 14690 net.cpp:226] pool3 needs backward computation.
I0413 18:57:18.770745 14690 net.cpp:226] relu3_3 needs backward computation.
I0413 18:57:18.770748 14690 net.cpp:226] conv3_3 needs backward computation.
I0413 18:57:18.770751 14690 net.cpp:226] relu3_2 needs backward computation.
I0413 18:57:18.770754 14690 net.cpp:226] conv3_2 needs backward computation.
I0413 18:57:18.770758 14690 net.cpp:226] relu3_1 needs backward computation.
I0413 18:57:18.770761 14690 net.cpp:226] conv3_1 needs backward computation.
I0413 18:57:18.770764 14690 net.cpp:228] pool2 does not need backward computation.
I0413 18:57:18.770769 14690 net.cpp:228] relu2_2 does not need backward computation.
I0413 18:57:18.770772 14690 net.cpp:228] conv2_2 does not need backward computation.
I0413 18:57:18.770776 14690 net.cpp:228] relu2_1 does not need backward computation.
I0413 18:57:18.770779 14690 net.cpp:228] conv2_1 does not need backward computation.
I0413 18:57:18.770782 14690 net.cpp:228] pool1 does not need backward computation.
I0413 18:57:18.770786 14690 net.cpp:228] relu1_2 does not need backward computation.
I0413 18:57:18.770789 14690 net.cpp:228] conv1_2 does not need backward computation.
I0413 18:57:18.770792 14690 net.cpp:228] relu1_1 does not need backward computation.
I0413 18:57:18.770795 14690 net.cpp:228] conv1_1 does not need backward computation.
I0413 18:57:18.770799 14690 net.cpp:228] input_gt_boxes_input_2_split does not need backward computation.
I0413 18:57:18.770803 14690 net.cpp:228] input_im_info_input_1_split does not need backward computation.
I0413 18:57:18.770807 14690 net.cpp:228] input_data_input_0_split does not need backward computation.
I0413 18:57:18.770812 14690 net.cpp:228] input does not need backward computation.
I0413 18:57:18.770814 14690 net.cpp:270] This network produces output loss_bbox
I0413 18:57:18.770818 14690 net.cpp:270] This network produces output pid_loss
I0413 18:57:18.770822 14690 net.cpp:270] This network produces output rpn_big_loss_bbox
I0413 18:57:18.770825 14690 net.cpp:270] This network produces output rpn_big_loss_cls
I0413 18:57:18.770874 14690 net.cpp:283] Network initialization done.
I0413 18:57:18.771019 14690 solver.cpp:60] Solver scaffolding done.
Loading pretrained model weights from /home/mopkobka/CourseWork/DockerShared/gml-nn/data/NET/VGG16.v2.caffemodel
[libprotobuf WARNING google/protobuf/io/coded_stream.cc:537] Reading dangerously large protocol message.  If the message turns out to be larger than 2147483647 bytes, parsing will be halted for security reasons.  To increase the limit (or to disable these warnings), see CodedInputStream::SetTotalBytesLimit() in google/protobuf/io/coded_stream.h.
[libprotobuf WARNING google/protobuf/io/coded_stream.cc:78] The total number of bytes read was 553432430
I0413 18:57:19.058104 14690 upgrade_proto.cpp:67] Attempting to upgrade input file specified using deprecated input fields: /home/mopkobka/CourseWork/DockerShared/gml-nn/data/NET/VGG16.v2.caffemodel
I0413 18:57:19.058122 14690 upgrade_proto.cpp:70] Successfully upgraded file specified using deprecated input fields.
W0413 18:57:19.058126 14690 upgrade_proto.cpp:72] Note that future Caffe releases will only support input layers and not input fields.
I0413 18:57:19.069808 14690 net.cpp:761] Ignoring source layer pool5
I0413 18:57:19.154513 14690 net.cpp:761] Ignoring source layer fc8
I0413 18:57:19.154546 14690 net.cpp:761] Ignoring source layer prob
Solving...
F0413 18:57:19.424536 14690 syncedmem.cpp:56] Check failed: error == cudaSuccess (2 vs. 0)  out of memory
*** Check failure stack trace: ***
exps/train_model.sh: line 26: 14690 Aborted                 (core dumped) python3 ./tools/train_net.py --gpu 0 --cfg exps/${EXP_NAME}/config.yml --exp_dir ${EXP_NAME} ${EXTRA_ARGS}
