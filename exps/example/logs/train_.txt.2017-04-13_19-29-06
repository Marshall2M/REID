+ echo Logging output to exps/example/logs/train_.txt.2017-04-13_19-29-06
Logging output to exps/example/logs/train_.txt.2017-04-13_19-29-06
+ python3 ./tools/train_net.py --gpu 0 --cfg exps/example/config.yml --exp_dir example
/home/mopkobka/CourseWork/venv-gml-nn/lib/python3.5/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.
  "This module will be removed in 0.20.", DeprecationWarning)
Called with args:
Namespace(cfg_file='exps/example/config.yml', exp_dir='example', gpu_id=0)
Using config:
{'DATA_DIR': '/home/mopkobka/CourseWork/gml-nn-detector/data',
 'DEDUP_BOXES': 0.0625,
 'DRAW_NET': False,
 'EPS': 1e-14,
 'EXP_DIR': 'example',
 'GPU_ID': 0,
 'MATLAB': 'matlab',
 'MODEL': {'ATTACHED_NETS': [{'ANCHOR_RATIOS': [1.0],
                              'ANCHOR_SCALES': [3, 6, 9, 12],
                              'ANCHOR_SHIFT_NUM_XY': [[1, 1],
                                                      [1, 1],
                                                      [1, 1],
                                                      [1, 1]],
                              'ARCHITECTURE': "m.add_layer(ConvWithActivation(p('conv1'), "
                                              '128, 3))\n'
                                              "m.add_layer(ConvWithActivation(p('conv2'), "
                                              '128, 3, dropout=0.3),\n'
                                              '            parent_layer=-1)\n'
                                              '\n'
                                              "m.add_layer(ConvWithActivation(p('output_cls'), "
                                              '80, 3),\n'
                                              '            '
                                              "parent_layer=p('conv2'))\n"
                                              '\n'
                                              "m.add_layer(ConvWithActivation(p('output_box'), "
                                              '80, 3),\n'
                                              '            '
                                              "parent_layer=p('conv2'))",
                              'BATCHSIZE': 256,
                              'DISABLED': True,
                              'FG_FRACTION': 0.5,
                              'NAME': 'rpn_small',
                              'NEGATIVE_OVERLAP': 0.4,
                              'NMS_THRESH': 0.7,
                              'PARENT_LAYER': 'relu2_2',
                              'POSITIVE_OVERLAP': 0.6,
                              'POST_NMS_TOP_N': 500,
                              'PRE_NMS_TOP_N': 12000,
                              'SQUARE_TARGETS': True,
                              'TOP_NEGATIVE_FRACTION': 0.2,
                              'TYPE': 'RPN'},
                             {'ANCHOR_RATIOS': [1.0],
                              'ANCHOR_SCALES': [2.5,
                                                3.025,
                                                3.66025,
                                                4.4289024999999995,
                                                5.358972025,
                                                6.484356150249999,
                                                7.846070941802498,
                                                9.493745839581022,
                                                11.487432465893036,
                                                13.899793283730574,
                                                16.818749873313998,
                                                20.350687346709933,
                                                24.62433168951902,
                                                29.795441344318014],
                              'ANCHOR_SHIFT_NUM_XY': [[2, 2],
                                                      [2, 2],
                                                      [2, 2],
                                                      [2, 2],
                                                      [1, 1],
                                                      [1, 1],
                                                      [1, 1],
                                                      [1, 1],
                                                      [1, 1],
                                                      [1, 1],
                                                      [1, 1],
                                                      [1, 1],
                                                      [1, 1],
                                                      [1, 1]],
                              'ARCHITECTURE': "m.add_layer(ConvWithActivation(p('conv1'), "
                                              '512, 3))\n'
                                              "m.add_layer(ConvWithActivation(p('conv2'), "
                                              '512, 3, dropout=0.3),\n'
                                              '            parent_layer=-1)\n'
                                              '\n'
                                              "m.add_layer(ConvWithActivation(p('output_cls'), "
                                              '384, 3),\n'
                                              '            '
                                              "parent_layer=p('conv2'))\n"
                                              '\n'
                                              "m.add_layer(ConvWithActivation(p('output_box'), "
                                              '384, 3),\n'
                                              '            '
                                              "parent_layer=p('conv2'))",
                              'BATCHSIZE': 256,
                              'FG_FRACTION': 0.5,
                              'NAME': 'rpn_big',
                              'NEGATIVE_OVERLAP': 0.4,
                              'NMS_THRESH': 0.7,
                              'PARENT_LAYER': 'relu5_3',
                              'POSITIVE_OVERLAP': 0.6,
                              'POST_NMS_TOP_N': 500,
                              'PRE_NMS_TOP_N': 12000,
                              'SQUARE_TARGETS': True,
                              'TOP_NEGATIVE_FRACTION': 0.2,
                              'TYPE': 'RPN'}],
           'NAME': 'faces_example',
           'PRETRAINED_MODEL_CONFIG': '/home/mopkobka/CourseWork/gml-nn-detector/models/vgg16/model.prototxt',
           'WEIGHTS_PATH': '/home/mopkobka/CourseWork/DockerShared/gml-nn/data/NET/VGG16.v2.caffemodel'},
 'MODELS_DIR': '/home/mopkobka/CourseWork/gml-nn-detector/models',
 'PIXEL_MEANS': array([[[ 102.9801,  115.9465,  122.7717]]]),
 'RNG_SEED': 777,
 'ROOT_DIR': '/home/mopkobka/CourseWork/gml-nn-detector',
 'RPN': {'ANCHOR_BASE_SIZE': 16,
         'ANCHOR_RATIOS': [0.5, 1, 2],
         'ANCHOR_SCALES': [8, 16, 32],
         'ANCHOR_SHIFT_NUM_XY': [(1, 1)]},
 'TEST': {'BBOX_REG': True,
          'DATASETS': [{'MAX_SIZE': 1400,
                        'OUTPUT_FILE': 'videoset.json',
                        'PATH': '/home/local/work/data/faces/BigSampleFNTest_p3lr11/imgs',
                        'SCALES': [350, 1000],
                        'TYPE': 'IMAGES_DIR'},
                       {'MAX_SIZE': 700,
                        'OUTPUT_FILE': 'fddb.json',
                        'PATH': '/home/local/work/data/faces/FDDB/FDDB_p3lr11/imgs',
                        'SCALES': [500],
                        'TYPE': 'IMAGES_DIR'}],
          'DENSE_SCAN': False,
          'FINAL_NMS': 0.5,
          'HAS_RPN': False,
          'MAX_PER_IMAGE': 200,
          'NMS': 0.3,
          'PROPOSAL_METHOD': 'selective_search',
          'RPN_MIN_SIZE': 16,
          'RPN_NMS_THRESH': 0.7,
          'RPN_ONLY': False,
          'RPN_POST_NMS_TOP_N': 300,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SVM': False,
          'VIZUALIZATION': {'DRAW_BOXES': True,
                            'DRAW_SCORES': False,
                            'ENABLE': False,
                            'ONLY_WITH_OBJECTS': False,
                            'SCORE_THRESH': 0.5},
          'WITHOUT_UPSAMPLE': True},
 'TRAIN': {'ASPECT_GROUPING': True,
           'BBOX_INSIDE_WEIGHTS': [1.0, 1.0, 1.0, 1.0],
           'BBOX_NORMALIZE_MEANS': [0.0, 0.0, 0.0, 0.0],
           'BBOX_NORMALIZE_STDS': [0.1, 0.1, 0.2, 0.2],
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_REG': True,
           'BBOX_THRESH': 0.5,
           'DATASET': 'default',
           'DATASETS': [{'MAX_SIZE': 700,
                         'PATH': '/home/mopkobka/CourseWork/DockerShared/gml-nn/tiny_dataset',
                         'SCALES': [500],
                         'TYPE': 'BBOX_JSON_MARKING'}],
           'DOUBLE_GENERATE': False,
           'ENABLE_SMART_ORDER': False,
           'GENERATED_FRACTION': 0.0,
           'HAS_RPN': True,
           'PROPOSAL_METHOD': 'selective_search',
           'RATIO_JITTERING': 0.0,
           'REDISTRIBUTE_CLASSES': False,
           'RPN_BATCHSIZE': 256,
           'RPN_BBOX_INSIDE_WEIGHTS': [1.0, 1.0, 1.0, 1.0],
           'RPN_CLOBBER_POSITIVES': False,
           'RPN_FG_FRACTION': 0.5,
           'RPN_LINEAR_START_TNF': 0.1,
           'RPN_LINEAR_TNF_K': -1.0,
           'RPN_MIN_SIZE': 16,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_PERIODIC_TN': 0,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POSITIVE_WEIGHT': -1.0,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 12000,
           'RPN_SQUARE_TARGETS': False,
           'RPN_SQUARE_TARGETS_KY': False,
           'RPN_TOP_NEGATIVE_FRACTION': 0.0,
           'SCALE_JITTERING': 0.0,
           'SNAPSHOT_INFIX': '',
           'SNAPSHOT_ITERS': 5000,
           'SOLVER': {'BASE_LR': 0.001,
                      'DISPLAY': {'AVERAGE_LOSS': 100, 'PERIOD': 20},
                      'IMS_PER_BATCH': 2,
                      'LR_POLICY': {'GAMMA': 0.1,
                                    'STEPS': [70000, 120000],
                                    'TYPE': 'multistep'},
                      'MOMENTUM': 0.9,
                      'TRAIN_ITERS': 90000,
                      'WEIGHT_DECAY': 0.0005},
           'SO_FORCE_BATCHSIZE': 500,
           'SO_FORCE_ROUNDS': 8,
           'SO_GOOD_SKIP_PROB': 0.85,
           'SO_TAIL_LEN': 1000,
           'USE_FLIPPED': True,
           'USE_LMDB': False,
           'USE_PREFETCH': False},
 'USE_GPU_NMS': True,
 'WEIGHTS_PATH': 'default'}
Output will be saved to `/home/mopkobka/CourseWork/gml-nn-detector/exps/example/output/train_13_04_2017_19_29`
Created solver path: /tmp/tmpa1jx4amt
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> /tmp/tmpa1jx4amt
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0413 19:29:07.916602 15949 solver.cpp:48] Initializing solver from parameters: 
base_lr: 0.001
display: 20
lr_policy: "multistep"
gamma: 0.1
momentum: 0.9
weight_decay: 0.0005
snapshot: 0
snapshot_prefix: "faces_example"
net_param {
  name: "GeneratedModel_13_04_2017_19_29"
  layer {
    name: "input"
    type: "Python"
    top: "input_data"
    top: "input_im_info"
    top: "input_gt_boxes"
    top: "input_ignored_boxes"
    python_param {
      module: "layers.roi_data_layer"
      layer: "RoIDataLayer"
      param_str: "{\"num_classes\": 2}"
    }
  }
  layer {
    name: "conv1_1"
    type: "Convolution"
    bottom: "input_data"
    top: "conv1_1"
    param {
      lr_mult: 0
      decay_mult: 0
    }
    param {
      lr_mult: 0
      decay_mult: 0
    }
    convolution_param {
      num_output: 64
      pad: 1
      kernel_size: 3
    }
  }
  layer {
    name: "relu1_1"
    type: "ReLU"
    bottom: "conv1_1"
    top: "conv1_1"
  }
  layer {
    name: "conv1_2"
    type: "Convolution"
    bottom: "conv1_1"
    top: "conv1_2"
    param {
      lr_mult: 0
      decay_mult: 0
    }
    param {
      lr_mult: 0
      decay_mult: 0
    }
    convolution_param {
      num_output: 64
      pad: 1
      kernel_size: 3
    }
  }
  layer {
    name: "relu1_2"
    type: "ReLU"
    bottom: "conv1_2"
    top: "conv1_2"
  }
  layer {
    name: "pool1"
    type: "Pooling"
    bottom: "conv1_2"
    top: "pool1"
    pooling_param {
      pool: MAX
      kernel_size: 2
      stride: 2
    }
  }
  layer {
    name: "conv2_1"
    type: "Convolution"
    bottom: "pool1"
    top: "conv2_1"
    param {
      lr_mult: 0
      decay_mult: 0
    }
    param {
      lr_mult: 0
      decay_mult: 0
    }
    convolution_param {
      num_output: 128
      pad: 1
      kernel_size: 3
    }
  }
  layer {
    name: "relu2_1"
    type: "ReLU"
    bottom: "conv2_1"
    top: "conv2_1"
  }
  layer {
    name: "conv2_2"
    type: "Convolution"
    bottom: "conv2_1"
    top: "conv2_2"
    param {
      lr_mult: 0
      decay_mult: 0
    }
    param {
      lr_mult: 0
      decay_mult: 0
    }
    convolution_param {
      num_output: 128
      pad: 1
      kernel_size: 3
    }
  }
  layer {
    name: "relu2_2"
    type: "ReLU"
    bottom: "conv2_2"
    top: "conv2_2"
  }
  layer {
    name: "pool2"
    type: "Pooling"
    bottom: "conv2_2"
    top: "pool2"
    pooling_param {
      pool: MAX
      kernel_size: 2
      stride: 2
    }
  }
  layer {
    name: "conv3_1"
    type: "Convolution"
    bottom: "pool2"
    top: "conv3_1"
    param {
      lr_mult: 1
    }
    param {
      lr_mult: 2
    }
    convolution_param {
      num_output: 256
      pad: 1
      kernel_size: 3
    }
  }
  layer {
    name: "relu3_1"
    type: "ReLU"
    bottom: "conv3_1"
    top: "conv3_1"
  }
  layer {
    name: "conv3_2"
    type: "Convolution"
    bottom: "conv3_1"
    top: "conv3_2"
    param {
      lr_mult: 1
    }
    param {
      lr_mult: 2
    }
    convolution_param {
      num_output: 256
      pad: 1
      kernel_size: 3
    }
  }
  layer {
    name: "relu3_2"
    type: "ReLU"
    bottom: "conv3_2"
    top: "conv3_2"
  }
  layer {
    name: "conv3_3"
    type: "Convolution"
    bottom: "conv3_2"
    top: "conv3_3"
    param {
      lr_mult: 1
    }
    param {
      lr_mult: 2
    }
    convolution_param {
      num_output: 256
      pad: 1
      kernel_size: 3
    }
  }
  layer {
    name: "relu3_3"
    type: "ReLU"
    bottom: "conv3_3"
    top: "conv3_3"
  }
  layer {
    name: "pool3"
    type: "Pooling"
    bottom: "conv3_3"
    top: "pool3"
    pooling_param {
      pool: MAX
      kernel_size: 2
      stride: 2
    }
  }
  layer {
    name: "conv4_1"
    type: "Convolution"
    bottom: "pool3"
    top: "conv4_1"
    param {
      lr_mult: 1
    }
    param {
      lr_mult: 2
    }
    convolution_param {
      num_output: 512
      pad: 1
      kernel_size: 3
    }
  }
  layer {
    name: "relu4_1"
    type: "ReLU"
    bottom: "conv4_1"
    top: "conv4_1"
  }
  layer {
    name: "conv4_2"
    type: "Convolution"
    bottom: "conv4_1"
    top: "conv4_2"
    param {
      lr_mult: 1
    }
    param {
      lr_mult: 2
    }
    convolution_param {
      num_output: 512
      pad: 1
      kernel_size: 3
    }
  }
  layer {
    name: "relu4_2"
    type: "ReLU"
    bottom: "conv4_2"
    top: "conv4_2"
  }
  layer {
    name: "conv4_3"
    type: "Convolution"
    bottom: "conv4_2"
    top: "conv4_3"
    param {
      lr_mult: 1
    }
    param {
      lr_mult: 2
    }
    convolution_param {
      num_output: 512
      pad: 1
      kernel_size: 3
    }
  }
  layer {
    name: "relu4_3"
    type: "ReLU"
    bottom: "conv4_3"
    top: "conv4_3"
  }
  layer {
    name: "pool4"
    type: "Pooling"
    bottom: "conv4_3"
    top: "pool4"
    pooling_param {
      pool: MAX
      kernel_size: 2
      stride: 2
    }
  }
  layer {
    name: "conv5_1"
    type: "Convolution"
    bottom: "pool4"
    top: "conv5_1"
    param {
      lr_mult: 1
    }
    param {
      lr_mult: 2
    }
    convolution_param {
      num_output: 512
      pad: 1
      kernel_size: 3
    }
  }
  layer {
    name: "relu5_1"
    type: "ReLU"
    bottom: "conv5_1"
    top: "conv5_1"
  }
  layer {
    name: "conv5_2"
    type: "Convolution"
    bottom: "conv5_1"
    top: "conv5_2"
    param {
      lr_mult: 1
    }
    param {
      lr_mult: 2
    }
    convolution_param {
      num_output: 512
      pad: 1
      kernel_size: 3
    }
  }
  layer {
    name: "relu5_2"
    type: "ReLU"
    bottom: "conv5_2"
    top: "conv5_2"
  }
  layer {
    name: "conv5_3"
    type: "Convolution"
    bottom: "conv5_2"
    top: "conv5_3"
    param {
      lr_mult: 1
    }
    param {
      lr_mult: 2
    }
    convolution_param {
      num_output: 512
      pad: 1
      kernel_size: 3
    }
  }
  layer {
    name: "relu5_3"
    type: "ReLU"
    bottom: "conv5_3"
    top: "conv5_3"
  }
  layer {
    name: "rpn_big_conv1"
    type: "Convolution"
    bottom: "conv5_3"
    top: "rpn_big_conv1"
    param {
      lr_mult: 1
    }
    param {
      lr_mult: 2
    }
    convolution_param {
      num_output: 512
      pad: 1
      kernel_size: 3
      stride: 1
      weight_filler {
        type: "gaussian"
        mean: 0
        std: 0.01
      }
      bias_filler {
        type: "constant"
      }
    }
  }
  layer {
    name: "rpn_big_conv1_relu"
    type: "ReLU"
    bottom: "rpn_big_conv1"
    top: "rpn_big_conv1"
    relu_param {
      negative_slope: 0
    }
  }
  layer {
    name: "rpn_big_conv2"
    type: "Convolution"
    bottom: "rpn_big_conv1"
    top: "rpn_big_conv2"
    param {
      lr_mult: 1
    }
    param {
      lr_mult: 2
    }
    convolution_param {
      num_output: 512
      pad: 1
      kernel_size: 3
      stride: 1
      weight_filler {
        type: "gaussian"
        mean: 0
        std: 0.01
      }
      bias_filler {
        type: "constant"
      }
    }
  }
  layer {
    name: "rpn_big_conv2_relu"
    type: "ReLU"
    bottom: "rpn_big_conv2"
    top: "rpn_big_conv2"
    relu_param {
      negative_slope: 0
    }
  }
  layer {
    name: "rpn_big_conv2_drop"
    type: "Dropout"
    bottom: "rpn_big_conv2"
    top: "rpn_big_conv2"
    dropout_param {
      dropout_ratio: 0.3
    }
  }
  layer {
    name: "rpn_big_output_cls"
    type: "Convolution"
    bottom: "rpn_big_conv2"
    top: "rpn_big_output_cls"
    param {
      lr_mult: 1
    }
    param {
      lr_mult: 2
    }
    convolution_param {
      num_output: 384
      pad: 1
      kernel_size: 3
      stride: 1
      weight_filler {
        type: "gaussian"
        mean: 0
        std: 0.01
      }
      bias_filler {
        type: "constant"
      }
    }
  }
  layer {
    name: "rpn_big_output_cls_relu"
    type: "ReLU"
    bottom: "rpn_big_output_cls"
    top: "rpn_big_output_cls"
    relu_param {
      negative_slope: 0
    }
  }
  layer {
    name: "rpn_big_cls_score"
    type: "Convolution"
    bottom: "rpn_big_output_cls"
    top: "rpn_big_cls_score"
    param {
      lr_mult: 1
    }
    param {
      lr_mult: 2
    }
    convolution_param {
      num_output: 52
      pad: 0
      kernel_size: 3
      stride: 1
      weight_filler {
        type: "gaussian"
        mean: 0
        std: 0.01
      }
      bias_filler {
        type: "constant"
      }
    }
  }
  layer {
    name: "rpn_big_cls_score_reshape"
    type: "Reshape"
    bottom: "rpn_big_cls_score"
    top: "rpn_big_cls_score_reshape"
    reshape_param {
      shape {
        dim: 0
        dim: 2
        dim: -1
        dim: 0
      }
    }
  }
  layer {
    name: "rpn_big_cls_prob"
    type: "Softmax"
    bottom: "rpn_big_cls_score_reshape"
    top: "rpn_big_cls_prob"
  }
  layer {
    name: "rpn_big_cls_prob_reshape"
    type: "Reshape"
    bottom: "rpn_big_cls_prob"
    top: "rpn_big_cls_prob_reshape"
    reshape_param {
      shape {
        dim: 0
        dim: 52
        dim: -1
        dim: 0
      }
    }
  }
  layer {
    name: "rpn_big_anchor_target"
    type: "Python"
    bottom: "rpn_big_cls_score"
    bottom: "input_gt_boxes"
    bottom: "input_ignored_boxes"
    bottom: "input_im_info"
    bottom: "input_data"
    bottom: "rpn_big_cls_prob_reshape"
    top: "rpn_big_anchor_target_labels"
    top: "rpn_big_anchor_target_bbox_targets"
    top: "rpn_big_anchor_target_inside_weights"
    top: "rpn_big_anchor_target_outside_weights"
    python_param {
      module: "rpn.anchor_target_layer"
      layer: "AnchorTargetLayer"
      param_str: "{\"num_classes\": 1, \"square_targets\": true, \"anchor_scales\": [2.5, 3.025, 3.66025, 4.4289024999999995, 5.358972025, 6.484356150249999, 7.846070941802498, 9.493745839581022, 11.487432465893036, 13.899793283730574, 16.818749873313998, 20.350687346709933, 24.62433168951902, 29.795441344318014], \"tn_fraction\": 0.2, \"feat_stride\": 16, \"fg_fraction\": 0.5, \"anchor_ratios\": [1.0], \"negative_overlap\": 0.4, \"positive_overlap\": 0.6, \"anchor_shift_num_xy\": [[2, 2], [2, 2], [2, 2], [2, 2], [1, 1], [1, 1], [1, 1], [1, 1], [1, 1], [1, 1], [1, 1], [1, 1], [1, 1], [1, 1]], \"name\": \"rpn_big\", \"batchsize\": 256}"
    }
  }
  layer {
    name: "rpn_big_loss_cls"
    type: "SoftmaxWithLoss"
    bottom: "rpn_big_cls_score_reshape"
    bottom: "rpn_big_anchor_target_labels"
    top: "rpn_big_loss_cls"
    loss_weight: 1
    propagate_down: true
    propagate_down: false
    loss_param {
      ignore_label: -1
      normalization: VALID
    }
  }
  layer {
    name: "rpn_big_output_box"
    type: "Convolution"
    bottom: "rpn_big_conv2"
    top: "rpn_big_output_box"
    param {
      lr_mult: 1
    }
    param {
      lr_mult: 2
    }
    convolution_param {
      num_output: 384
      pad: 1
      kernel_size: 3
      stride: 1
      weight_filler {
        type: "gaussian"
        mean: 0
        std: 0.01
      }
      bias_filler {
        type: "constant"
      }
    }
  }
  layer {
    name: "rpn_big_output_box_relu"
    type: "ReLU"
    bottom: "rpn_big_output_box"
    top: "rpn_big_output_box"
    relu_param {
      negative_slope: 0
    }
  }
  layer {
    name: "rpn_big_bbox_pred"
    type: "Convolution"
    bottom: "rpn_big_output_box"
    top: "rpn_big_bbox_pred"
    param {
      lr_mult: 1
    }
    param {
      lr_mult: 2
    }
    convolution_param {
      num_output: 104
      pad: 0
      kernel_size: 3
      stride: 1
      weight_filler {
        type: "gaussian"
        mean: 0
        std: 0.01
      }
      bias_filler {
        type: "constant"
      }
    }
  }
  layer {
    name: "rpn_big_loss_bbox"
    type: "Python"
    bottom: "rpn_big_bbox_pred"
    bottom: "rpn_big_anchor_target_bbox_targets"
    bottom: "rpn_big_anchor_target_inside_weights"
    bottom: "rpn_big_anchor_target_outside_weights"
    top: "rpn_big_loss_bbox"
    loss_weight: 1
    python_param {
      module: "layers.smooth_l1_loss"
      layer: "SmoothL1LossLayer"
      param_str: "{\"sigma\": 3}"
    }
  }
  layer {
    name: "proposal"
    type: "Python"
    bottom: "rpn_big_cls_prob_reshape"
    bottom: "rpn_big_bbox_pred"
    bottom: "input_im_info"
    top: "rpn_rois"
    python_param {
      module: "rpn.proposal_layer"
      layer: "ProposalLayer"
      param_str: "{\"anchor_shift_num_xy\": [[2, 2], [2, 2], [2, 2], [2, 2], [1, 1], [1, 1], [1, 1], [1, 1], [1, 1], [1, 1], [1, 1], [1, 1], [1, 1], [1, 1]], \"feat_stride\": 16, \"num_classes\": 1, \"anchor_scales\": [2.5, 3.025, 3.66025, 4.4289024999999995, 5.358972025, 6.484356150249999, 7.846070941802498, 9.493745839581022, 11.487432465893036, 13.899793283730574, 16.818749873313998, 20.350687346709933, 24.62433168951902, 29.795441344318014], \"square_targets\": true, \"name\": \"rpn_big\", \"batchsize\": 256, \"anchor_ratios\": [1.0]}"
    }
  }
  layer {
    name: "roi-data"
    type: "Python"
    bottom: "rpn_rois"
    bottom: "input_gt_boxes"
    top: "rois"
    top: "labels"
    top: "bbox_targets"
    top: "bbox_inside_weights"
    top: "bbox_outside_weights"
    top: "pid_label"
    python_param {
      module: "rpn.proposal_target_layer"
      layer: "ProposalTargetLayer"
      param_str: "{\"num_classes\": 2, \"bg_aux_label\": 5532}"
    }
  }
  layer {
    name: "roi-pool"
    type: "ROIPooling"
    bottom: "conv5_3"
    bottom: "rois"
    top: "roi-pool"
    roi_pooling_param {
      pooled_h: 7
      pooled_w: 7
      spatial_scale: 0.0625
    }
  }
  layer {
    name: "fc6"
    type: "InnerProduct"
    bottom: "roi-pool"
    top: "fc6"
    param {
      lr_mult: 1
    }
    param {
      lr_mult: 2
    }
    inner_product_param {
      num_output: 4096
    }
  }
  layer {
    name: "relu6"
    type: "ReLU"
    bottom: "fc6"
    top: "fc6"
  }
  layer {
    name: "drop6"
    type: "Dropout"
    bottom: "fc6"
    top: "fc6"
    dropout_param {
      dropout_ratio: 0.5
    }
  }
  layer {
    name: "fc7"
    type: "InnerProduct"
    bottom: "fc6"
    top: "fc7"
    param {
      lr_mult: 1
    }
    param {
      lr_mult: 2
    }
    inner_product_param {
      num_output: 4096
    }
  }
  layer {
    name: "relu7"
    type: "ReLU"
    bottom: "fc7"
    top: "fc7"
  }
  layer {
    name: "drop7"
    type: "Dropout"
    bottom: "fc7"
    top: "fc7"
    dropout_param {
      dropout_ratio: 0.5
    }
  }
  layer {
    name: "bbox_pred"
    type: "InnerProduct"
    bottom: "fc7"
    top: "bbox_pred"
    param {
      lr_mult: 1
    }
    param {
      lr_mult: 2
    }
    inner_product_param {
      num_output: 8
      weight_filler {
        type: "gaussian"
        std: 0.001
      }
      bias_filler {
        type: "constant"
        value: 0
      }
    }
  }
  layer {
    name: "loss_bbox"
    type: "SmoothL1Loss"
    bottom: "bbox_pred"
    bottom: "bbox_targets"
    bottom: "bbox_inside_weights"
    bottom: "bbox_outside_weights"
    top: "loss_bbox"
    loss_weight: 1
  }
  layer {
    name: "feat"
    type: "InnerProduct"
    bottom: "fc7"
    top: "feat"
    param {
      lr_mult: 1
      decay_mult: 1
    }
    param {
      lr_mult: 2
      decay_mult: 0
    }
    inner_product_param {
      num_output: 256
      weight_filler {
        type: "gaussian"
        std: 0.01
      }
      bias_filler {
        type: "constant"
        value: 0
      }
    }
  }
  layer {
    name: "relu8"
    type: "ReLU"
    bottom: "feat"
    top: "feat"
  }
  layer {
    name: "drop8"
    type: "Dropout"
    bottom: "feat"
    top: "feat"
    dropout_param {
      dropout_ratio: 0.5
    }
  }
  layer {
    name: "pid_score"
    type: "InnerProduct"
    bottom: "feat"
    top: "pid_score"
    param {
      lr_mult: 1
      decay_mult: 1
    }
    param {
      lr_mult: 2
      decay_mult: 0
    }
    inner_product_param {
      num_output: 5533
      weight_filler {
        type: "gaussian"
        std: 0.001
      }
      bias_filler {
        type: "constant"
        value: 0
      }
    }
  }
  layer {
    name: "pid_accuracy"
    type: "Accuracy"
    bottom: "pid_score"
    bottom: "pid_label"
    top: "pid_accuracy"
    accuracy_param {
      ignore_label: -1
    }
  }
}
average_loss: 100
stepvalue: 70000
stepvalue: 120000
iter_size: 2
I0413 19:29:07.916692 15949 solver.cpp:86] Creating training net specified in net_param.
I0413 19:29:07.917184 15949 net.cpp:58] Initializing net from parameters: 
name: "GeneratedModel_13_04_2017_19_29"
state {
  phase: TRAIN
}
layer {
  name: "input"
  type: "Python"
  top: "input_data"
  top: "input_im_info"
  top: "input_gt_boxes"
  top: "input_ignored_boxes"
  python_param {
    module: "layers.roi_data_layer"
    layer: "RoIDataLayer"
    param_str: "{\"num_classes\": 2}"
  }
}
layer {
  name: "conv1_1"
  type: "Convolution"
  bottom: "input_data"
  top: "conv1_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "relu1_1"
  type: "ReLU"
  bottom: "conv1_1"
  top: "conv1_1"
}
layer {
  name: "conv1_2"
  type: "Convolution"
  bottom: "conv1_1"
  top: "conv1_2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "relu1_2"
  type: "ReLU"
  bottom: "conv1_2"
  top: "conv1_2"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1_2"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2_1"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "relu2_1"
  type: "ReLU"
  bottom: "conv2_1"
  top: "conv2_1"
}
layer {
  name: "conv2_2"
  type: "Convolution"
  bottom: "conv2_1"
  top: "conv2_2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "relu2_2"
  type: "ReLU"
  bottom: "conv2_2"
  top: "conv2_2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2_2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv3_1"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3_1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "relu3_1"
  type: "ReLU"
  bottom: "conv3_1"
  top: "conv3_1"
}
layer {
  name: "conv3_2"
  type: "Convolution"
  bottom: "conv3_1"
  top: "conv3_2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "relu3_2"
  type: "ReLU"
  bottom: "conv3_2"
  top: "conv3_2"
}
layer {
  name: "conv3_3"
  type: "Convolution"
  bottom: "conv3_2"
  top: "conv3_3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "relu3_3"
  type: "ReLU"
  bottom: "conv3_3"
  top: "conv3_3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3_3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv4_1"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4_1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 512
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "relu4_1"
  type: "ReLU"
  bottom: "conv4_1"
  top: "conv4_1"
}
layer {
  name: "conv4_2"
  type: "Convolution"
  bottom: "conv4_1"
  top: "conv4_2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 512
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "relu4_2"
  type: "ReLU"
  bottom: "conv4_2"
  top: "conv4_2"
}
layer {
  name: "conv4_3"
  type: "Convolution"
  bottom: "conv4_2"
  top: "conv4_3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 512
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "relu4_3"
  type: "ReLU"
  bottom: "conv4_3"
  top: "conv4_3"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4_3"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv5_1"
  type: "Convolution"
  bottom: "pool4"
  top: "conv5_1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 512
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "relu5_1"
  type: "ReLU"
  bottom: "conv5_1"
  top: "conv5_1"
}
layer {
  name: "conv5_2"
  type: "Convolution"
  bottom: "conv5_1"
  top: "conv5_2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 512
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "relu5_2"
  type: "ReLU"
  bottom: "conv5_2"
  top: "conv5_2"
}
layer {
  name: "conv5_3"
  type: "Convolution"
  bottom: "conv5_2"
  top: "conv5_3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 512
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "relu5_3"
  type: "ReLU"
  bottom: "conv5_3"
  top: "conv5_3"
}
layer {
  name: "rpn_big_conv1"
  type: "Convolution"
  bottom: "conv5_3"
  top: "rpn_big_conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 512
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      mean: 0
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "rpn_big_conv1_relu"
  type: "ReLU"
  bottom: "rpn_big_conv1"
  top: "rpn_big_conv1"
  relu_param {
    negative_slope: 0
  }
}
layer {
  name: "rpn_big_conv2"
  type: "Convolution"
  bottom: "rpn_big_conv1"
  top: "rpn_big_conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 512
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      mean: 0
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "rpn_big_conv2_relu"
  type: "ReLU"
  bottom: "rpn_big_conv2"
  top: "rpn_big_conv2"
  relu_param {
    negative_slope: 0
  }
}
layer {
  name: "rpn_big_conv2_drop"
  type: "Dropout"
  bottom: "rpn_big_conv2"
  top: "rpn_big_conv2"
  dropout_param {
    dropout_ratio: 0.3
  }
}
layer {
  name: "rpn_big_output_cls"
  type: "Convolution"
  bottom: "rpn_big_conv2"
  top: "rpn_big_output_cls"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      mean: 0
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "rpn_big_output_cls_relu"
  type: "ReLU"
  bottom: "rpn_big_output_cls"
  top: "rpn_big_output_cls"
  relu_param {
    negative_slope: 0
  }
}
layer {
  name: "rpn_big_cls_score"
  type: "Convolution"
  bottom: "rpn_big_output_cls"
  top: "rpn_big_cls_score"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 52
    pad: 0
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      mean: 0
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "rpn_big_cls_score_reshape"
  type: "Reshape"
  bottom: "rpn_big_cls_score"
  top: "rpn_big_cls_score_reshape"
  reshape_param {
    shape {
      dim: 0
      dim: 2
      dim: -1
      dim: 0
    }
  }
}
layer {
  name: "rpn_big_cls_prob"
  type: "Softmax"
  bottom: "rpn_big_cls_score_reshape"
  top: "rpn_big_cls_prob"
}
layer {
  name: "rpn_big_cls_prob_reshape"
  type: "Reshape"
  bottom: "rpn_big_cls_prob"
  top: "rpn_big_cls_prob_reshape"
  reshape_param {
    shape {
      dim: 0
      dim: 52
      dim: -1
      dim: 0
    }
  }
}
layer {
  name: "rpn_big_anchor_target"
  type: "Python"
  bottom: "rpn_big_cls_score"
  bottom: "input_gt_boxes"
  bottom: "input_ignored_boxes"
  bottom: "input_im_info"
  bottom: "input_data"
  bottom: "rpn_big_cls_prob_reshape"
  top: "rpn_big_anchor_target_labels"
  top: "rpn_big_anchor_target_bbox_targets"
  top: "rpn_big_anchor_target_inside_weights"
  top: "rpn_big_anchor_target_outside_weights"
  python_param {
    module: "rpn.anchor_target_layer"
    layer: "AnchorTargetLayer"
    param_str: "{\"num_classes\": 1, \"square_targets\": true, \"anchor_scales\": [2.5, 3.025, 3.66025, 4.4289024999999995, 5.358972025, 6.484356150249999, 7.846070941802498, 9.493745839581022, 11.487432465893036, 13.899793283730574, 16.818749873313998, 20.350687346709933, 24.62433168951902, 29.795441344318014], \"tn_fraction\": 0.2, \"feat_stride\": 16, \"fg_fraction\": 0.5, \"anchor_ratios\": [1.0], \"negative_overlap\": 0.4, \"positive_overlap\": 0.6, \"anchor_shift_num_xy\": [[2, 2], [2, 2], [2, 2], [2, 2], [1, 1], [1, 1], [1, 1], [1, 1], [1, 1], [1, 1], [1, 1], [1, 1], [1, 1], [1, 1]], \"name\": \"rpn_big\", \"batchsize\": 256}"
  }
}
layer {
  name: "rpn_big_loss_cls"
  type: "SoftmaxWithLoss"
  bottom: "rpn_big_cls_score_reshape"
  bottom: "rpn_big_anchor_target_labels"
  top: "rpn_big_loss_cls"
  loss_weight: 1
  propagate_down: true
  propagate_down: false
  loss_param {
    ignore_label: -1
    normalization: VALID
  }
}
layer {
  name: "rpn_big_output_box"
  type: "Convolution"
  bottom: "rpn_big_conv2"
  top: "rpn_big_output_box"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      mean: 0
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "rpn_big_output_box_relu"
  type: "ReLU"
  bottom: "rpn_big_output_box"
  top: "rpn_big_output_box"
  relu_param {
    negative_slope: 0
  }
}
layer {
  name: "rpn_big_bbox_pred"
  type: "Convolution"
  bottom: "rpn_big_output_box"
  top: "rpn_big_bbox_pred"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 104
    pad: 0
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      mean: 0
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "rpn_big_loss_bbox"
  type: "Python"
  bottom: "rpn_big_bbox_pred"
  bottom: "rpn_big_anchor_target_bbox_targets"
  bottom: "rpn_big_anchor_target_inside_weights"
  bottom: "rpn_big_anchor_target_outside_weights"
  top: "rpn_big_loss_bbox"
  loss_weight: 1
  python_param {
    module: "layers.smooth_l1_loss"
    layer: "SmoothL1LossLayer"
    param_str: "{\"sigma\": 3}"
  }
}
layer {
  name: "proposal"
  type: "Python"
  bottom: "rpn_big_cls_prob_reshape"
  bottom: "rpn_big_bbox_pred"
  bottom: "input_im_info"
  top: "rpn_rois"
  python_param {
    module: "rpn.proposal_layer"
    layer: "ProposalLayer"
    param_str: "{\"anchor_shift_num_xy\": [[2, 2], [2, 2], [2, 2], [2, 2], [1, 1], [1, 1], [1, 1], [1, 1], [1, 1], [1, 1], [1, 1], [1, 1], [1, 1], [1, 1]], \"feat_stride\": 16, \"num_classes\": 1, \"anchor_scales\": [2.5, 3.025, 3.66025, 4.4289024999999995, 5.358972025, 6.484356150249999, 7.846070941802498, 9.493745839581022, 11.487432465893036, 13.899793283730574, 16.818749873313998, 20.350687346709933, 24.62433168951902, 29.795441344318014], \"square_targets\": true, \"name\": \"rpn_big\", \"batchsize\": 256, \"anchor_ratios\": [1.0]}"
  }
}
layer {
  name: "roi-data"
  type: "Python"
  bottom: "rpn_rois"
  bottom: "input_gt_boxes"
  top: "rois"
  top: "labels"
  top: "bbox_targets"
  top: "bbox_inside_weights"
  top: "bbox_outside_weights"
  top: "pid_label"
  python_param {
    module: "rpn.proposal_target_layer"
    layer: "ProposalTargetLayer"
    param_str: "{\"num_classes\": 2, \"bg_aux_label\": 5532}"
  }
}
layer {
  name: "roi-pool"
  type: "ROIPooling"
  bottom: "conv5_3"
  bottom: "rois"
  top: "roi-pool"
  roi_pooling_param {
    pooled_h: 7
    pooled_w: 7
    spatial_scale: 0.0625
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "roi-pool"
  top: "fc6"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 4096
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 4096
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "bbox_pred"
  type: "InnerProduct"
  bottom: "fc7"
  top: "bbox_pred"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 8
    weight_filler {
      type: "gaussian"
      std: 0.001
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "loss_bbox"
  type: "SmoothL1Loss"
  bottom: "bbox_pred"
  bottom: "bbox_targets"
  bottom: "bbox_inside_weights"
  bottom: "bbox_outside_weights"
  top: "loss_bbox"
  loss_weight: 1
}
layer {
  name: "feat"
  type: "InnerProduct"
  bottom: "fc7"
  top: "feat"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 256
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu8"
  type: "ReLU"
  bottom: "feat"
  top: "feat"
}
layer {
  name: "drop8"
  type: "Dropout"
  bottom: "feat"
  top: "feat"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "pid_score"
  type: "InnerProduct"
  bottom: "feat"
  top: "pid_score"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 5533
    weight_filler {
      type: "gaussian"
      std: 0.001
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "pid_accuracy"
  type: "Accuracy"
  bottom: "pid_score"
  bottom: "pid_label"
  top: "pid_accuracy"
  accuracy_param {
    ignore_label: -1
  }
}
I0413 19:29:07.917438 15949 layer_factory.hpp:77] Creating layer input
I0413 19:29:07.918763 15949 net.cpp:100] Creating Layer input
I0413 19:29:07.918781 15949 net.cpp:408] input -> input_data
I0413 19:29:07.918793 15949 net.cpp:408] input -> input_im_info
I0413 19:29:07.918802 15949 net.cpp:408] input -> input_gt_boxes
I0413 19:29:07.918808 15949 net.cpp:408] input -> input_ignored_boxes
RoiDataLayer: name_to_top: {'data': 0, 'ignored_boxes': 3, 'gt_boxes': 2, 'im_info': 1}
I0413 19:29:08.245165 15949 net.cpp:150] Setting up input
I0413 19:29:08.245192 15949 net.cpp:157] Top shape: 1 3 1000 1500 (4500000)
I0413 19:29:08.245201 15949 net.cpp:157] Top shape: 1 3 (3)
I0413 19:29:08.245209 15949 net.cpp:157] Top shape: 1 5 (5)
I0413 19:29:08.245229 15949 net.cpp:157] Top shape: 1 5 (5)
I0413 19:29:08.245240 15949 net.cpp:165] Memory required for data: 18000052
I0413 19:29:08.245249 15949 layer_factory.hpp:77] Creating layer input_data_input_0_split
I0413 19:29:08.245270 15949 net.cpp:100] Creating Layer input_data_input_0_split
I0413 19:29:08.245280 15949 net.cpp:434] input_data_input_0_split <- input_data
I0413 19:29:08.245290 15949 net.cpp:408] input_data_input_0_split -> input_data_input_0_split_0
I0413 19:29:08.245302 15949 net.cpp:408] input_data_input_0_split -> input_data_input_0_split_1
I0413 19:29:08.245338 15949 net.cpp:150] Setting up input_data_input_0_split
I0413 19:29:08.245347 15949 net.cpp:157] Top shape: 1 3 1000 1500 (4500000)
I0413 19:29:08.245350 15949 net.cpp:157] Top shape: 1 3 1000 1500 (4500000)
I0413 19:29:08.245353 15949 net.cpp:165] Memory required for data: 54000052
I0413 19:29:08.245357 15949 layer_factory.hpp:77] Creating layer input_im_info_input_1_split
I0413 19:29:08.245364 15949 net.cpp:100] Creating Layer input_im_info_input_1_split
I0413 19:29:08.245368 15949 net.cpp:434] input_im_info_input_1_split <- input_im_info
I0413 19:29:08.245373 15949 net.cpp:408] input_im_info_input_1_split -> input_im_info_input_1_split_0
I0413 19:29:08.245381 15949 net.cpp:408] input_im_info_input_1_split -> input_im_info_input_1_split_1
I0413 19:29:08.245409 15949 net.cpp:150] Setting up input_im_info_input_1_split
I0413 19:29:08.245414 15949 net.cpp:157] Top shape: 1 3 (3)
I0413 19:29:08.245419 15949 net.cpp:157] Top shape: 1 3 (3)
I0413 19:29:08.245421 15949 net.cpp:165] Memory required for data: 54000076
I0413 19:29:08.245424 15949 layer_factory.hpp:77] Creating layer input_gt_boxes_input_2_split
I0413 19:29:08.245430 15949 net.cpp:100] Creating Layer input_gt_boxes_input_2_split
I0413 19:29:08.245434 15949 net.cpp:434] input_gt_boxes_input_2_split <- input_gt_boxes
I0413 19:29:08.245438 15949 net.cpp:408] input_gt_boxes_input_2_split -> input_gt_boxes_input_2_split_0
I0413 19:29:08.245447 15949 net.cpp:408] input_gt_boxes_input_2_split -> input_gt_boxes_input_2_split_1
I0413 19:29:08.245476 15949 net.cpp:150] Setting up input_gt_boxes_input_2_split
I0413 19:29:08.245481 15949 net.cpp:157] Top shape: 1 5 (5)
I0413 19:29:08.245486 15949 net.cpp:157] Top shape: 1 5 (5)
I0413 19:29:08.245488 15949 net.cpp:165] Memory required for data: 54000116
I0413 19:29:08.245491 15949 layer_factory.hpp:77] Creating layer conv1_1
I0413 19:29:08.245504 15949 net.cpp:100] Creating Layer conv1_1
I0413 19:29:08.245507 15949 net.cpp:434] conv1_1 <- input_data_input_0_split_0
I0413 19:29:08.245513 15949 net.cpp:408] conv1_1 -> conv1_1
I0413 19:29:08.248651 15949 net.cpp:150] Setting up conv1_1
I0413 19:29:08.248682 15949 net.cpp:157] Top shape: 1 64 1000 1500 (96000000)
I0413 19:29:08.248685 15949 net.cpp:165] Memory required for data: 438000116
I0413 19:29:08.248703 15949 layer_factory.hpp:77] Creating layer relu1_1
I0413 19:29:08.248715 15949 net.cpp:100] Creating Layer relu1_1
I0413 19:29:08.248720 15949 net.cpp:434] relu1_1 <- conv1_1
I0413 19:29:08.248728 15949 net.cpp:395] relu1_1 -> conv1_1 (in-place)
I0413 19:29:08.248739 15949 net.cpp:150] Setting up relu1_1
I0413 19:29:08.248744 15949 net.cpp:157] Top shape: 1 64 1000 1500 (96000000)
I0413 19:29:08.248746 15949 net.cpp:165] Memory required for data: 822000116
I0413 19:29:08.248749 15949 layer_factory.hpp:77] Creating layer conv1_2
I0413 19:29:08.248759 15949 net.cpp:100] Creating Layer conv1_2
I0413 19:29:08.248762 15949 net.cpp:434] conv1_2 <- conv1_1
I0413 19:29:08.248769 15949 net.cpp:408] conv1_2 -> conv1_2
I0413 19:29:08.254070 15949 net.cpp:150] Setting up conv1_2
I0413 19:29:08.254103 15949 net.cpp:157] Top shape: 1 64 1000 1500 (96000000)
I0413 19:29:08.254108 15949 net.cpp:165] Memory required for data: 1206000116
I0413 19:29:08.254127 15949 layer_factory.hpp:77] Creating layer relu1_2
I0413 19:29:08.254142 15949 net.cpp:100] Creating Layer relu1_2
I0413 19:29:08.254158 15949 net.cpp:434] relu1_2 <- conv1_2
I0413 19:29:08.254166 15949 net.cpp:395] relu1_2 -> conv1_2 (in-place)
I0413 19:29:08.254179 15949 net.cpp:150] Setting up relu1_2
I0413 19:29:08.254184 15949 net.cpp:157] Top shape: 1 64 1000 1500 (96000000)
I0413 19:29:08.254187 15949 net.cpp:165] Memory required for data: 1590000116
I0413 19:29:08.254191 15949 layer_factory.hpp:77] Creating layer pool1
I0413 19:29:08.254202 15949 net.cpp:100] Creating Layer pool1
I0413 19:29:08.254211 15949 net.cpp:434] pool1 <- conv1_2
I0413 19:29:08.254222 15949 net.cpp:408] pool1 -> pool1
I0413 19:29:08.254302 15949 net.cpp:150] Setting up pool1
I0413 19:29:08.254312 15949 net.cpp:157] Top shape: 1 64 500 750 (24000000)
I0413 19:29:08.254315 15949 net.cpp:165] Memory required for data: 1686000116
I0413 19:29:08.254321 15949 layer_factory.hpp:77] Creating layer conv2_1
I0413 19:29:08.254338 15949 net.cpp:100] Creating Layer conv2_1
I0413 19:29:08.254343 15949 net.cpp:434] conv2_1 <- pool1
I0413 19:29:08.254353 15949 net.cpp:408] conv2_1 -> conv2_1
I0413 19:29:08.255715 15949 net.cpp:150] Setting up conv2_1
I0413 19:29:08.255733 15949 net.cpp:157] Top shape: 1 128 500 750 (48000000)
I0413 19:29:08.255736 15949 net.cpp:165] Memory required for data: 1878000116
I0413 19:29:08.255756 15949 layer_factory.hpp:77] Creating layer relu2_1
I0413 19:29:08.255769 15949 net.cpp:100] Creating Layer relu2_1
I0413 19:29:08.255776 15949 net.cpp:434] relu2_1 <- conv2_1
I0413 19:29:08.255784 15949 net.cpp:395] relu2_1 -> conv2_1 (in-place)
I0413 19:29:08.255797 15949 net.cpp:150] Setting up relu2_1
I0413 19:29:08.255805 15949 net.cpp:157] Top shape: 1 128 500 750 (48000000)
I0413 19:29:08.255810 15949 net.cpp:165] Memory required for data: 2070000116
I0413 19:29:08.255815 15949 layer_factory.hpp:77] Creating layer conv2_2
I0413 19:29:08.255830 15949 net.cpp:100] Creating Layer conv2_2
I0413 19:29:08.255834 15949 net.cpp:434] conv2_2 <- conv2_1
I0413 19:29:08.255841 15949 net.cpp:408] conv2_2 -> conv2_2
I0413 19:29:08.256801 15949 net.cpp:150] Setting up conv2_2
I0413 19:29:08.256842 15949 net.cpp:157] Top shape: 1 128 500 750 (48000000)
I0413 19:29:08.256846 15949 net.cpp:165] Memory required for data: 2262000116
I0413 19:29:08.256858 15949 layer_factory.hpp:77] Creating layer relu2_2
I0413 19:29:08.256868 15949 net.cpp:100] Creating Layer relu2_2
I0413 19:29:08.256875 15949 net.cpp:434] relu2_2 <- conv2_2
I0413 19:29:08.256886 15949 net.cpp:395] relu2_2 -> conv2_2 (in-place)
I0413 19:29:08.256898 15949 net.cpp:150] Setting up relu2_2
I0413 19:29:08.256920 15949 net.cpp:157] Top shape: 1 128 500 750 (48000000)
I0413 19:29:08.256925 15949 net.cpp:165] Memory required for data: 2454000116
I0413 19:29:08.256928 15949 layer_factory.hpp:77] Creating layer pool2
I0413 19:29:08.256937 15949 net.cpp:100] Creating Layer pool2
I0413 19:29:08.256942 15949 net.cpp:434] pool2 <- conv2_2
I0413 19:29:08.256950 15949 net.cpp:408] pool2 -> pool2
I0413 19:29:08.256999 15949 net.cpp:150] Setting up pool2
I0413 19:29:08.257009 15949 net.cpp:157] Top shape: 1 128 250 375 (12000000)
I0413 19:29:08.257014 15949 net.cpp:165] Memory required for data: 2502000116
I0413 19:29:08.257021 15949 layer_factory.hpp:77] Creating layer conv3_1
I0413 19:29:08.257032 15949 net.cpp:100] Creating Layer conv3_1
I0413 19:29:08.257037 15949 net.cpp:434] conv3_1 <- pool2
I0413 19:29:08.257047 15949 net.cpp:408] conv3_1 -> conv3_1
I0413 19:29:08.258157 15949 net.cpp:150] Setting up conv3_1
I0413 19:29:08.258182 15949 net.cpp:157] Top shape: 1 256 250 375 (24000000)
I0413 19:29:08.258186 15949 net.cpp:165] Memory required for data: 2598000116
I0413 19:29:08.258203 15949 layer_factory.hpp:77] Creating layer relu3_1
I0413 19:29:08.258216 15949 net.cpp:100] Creating Layer relu3_1
I0413 19:29:08.258222 15949 net.cpp:434] relu3_1 <- conv3_1
I0413 19:29:08.258231 15949 net.cpp:395] relu3_1 -> conv3_1 (in-place)
I0413 19:29:08.258244 15949 net.cpp:150] Setting up relu3_1
I0413 19:29:08.258249 15949 net.cpp:157] Top shape: 1 256 250 375 (24000000)
I0413 19:29:08.258254 15949 net.cpp:165] Memory required for data: 2694000116
I0413 19:29:08.258260 15949 layer_factory.hpp:77] Creating layer conv3_2
I0413 19:29:08.258276 15949 net.cpp:100] Creating Layer conv3_2
I0413 19:29:08.258282 15949 net.cpp:434] conv3_2 <- conv3_1
I0413 19:29:08.258292 15949 net.cpp:408] conv3_2 -> conv3_2
I0413 19:29:08.259485 15949 net.cpp:150] Setting up conv3_2
I0413 19:29:08.259512 15949 net.cpp:157] Top shape: 1 256 250 375 (24000000)
I0413 19:29:08.259516 15949 net.cpp:165] Memory required for data: 2790000116
I0413 19:29:08.259529 15949 layer_factory.hpp:77] Creating layer relu3_2
I0413 19:29:08.259543 15949 net.cpp:100] Creating Layer relu3_2
I0413 19:29:08.259549 15949 net.cpp:434] relu3_2 <- conv3_2
I0413 19:29:08.259560 15949 net.cpp:395] relu3_2 -> conv3_2 (in-place)
I0413 19:29:08.259572 15949 net.cpp:150] Setting up relu3_2
I0413 19:29:08.259578 15949 net.cpp:157] Top shape: 1 256 250 375 (24000000)
I0413 19:29:08.259584 15949 net.cpp:165] Memory required for data: 2886000116
I0413 19:29:08.259590 15949 layer_factory.hpp:77] Creating layer conv3_3
I0413 19:29:08.259604 15949 net.cpp:100] Creating Layer conv3_3
I0413 19:29:08.259609 15949 net.cpp:434] conv3_3 <- conv3_2
I0413 19:29:08.259616 15949 net.cpp:408] conv3_3 -> conv3_3
I0413 19:29:08.261240 15949 net.cpp:150] Setting up conv3_3
I0413 19:29:08.261270 15949 net.cpp:157] Top shape: 1 256 250 375 (24000000)
I0413 19:29:08.261274 15949 net.cpp:165] Memory required for data: 2982000116
I0413 19:29:08.261289 15949 layer_factory.hpp:77] Creating layer relu3_3
I0413 19:29:08.261304 15949 net.cpp:100] Creating Layer relu3_3
I0413 19:29:08.261312 15949 net.cpp:434] relu3_3 <- conv3_3
I0413 19:29:08.261322 15949 net.cpp:395] relu3_3 -> conv3_3 (in-place)
I0413 19:29:08.261333 15949 net.cpp:150] Setting up relu3_3
I0413 19:29:08.261339 15949 net.cpp:157] Top shape: 1 256 250 375 (24000000)
I0413 19:29:08.261345 15949 net.cpp:165] Memory required for data: 3078000116
I0413 19:29:08.261351 15949 layer_factory.hpp:77] Creating layer pool3
I0413 19:29:08.261363 15949 net.cpp:100] Creating Layer pool3
I0413 19:29:08.261368 15949 net.cpp:434] pool3 <- conv3_3
I0413 19:29:08.261375 15949 net.cpp:408] pool3 -> pool3
I0413 19:29:08.261422 15949 net.cpp:150] Setting up pool3
I0413 19:29:08.261430 15949 net.cpp:157] Top shape: 1 256 125 188 (6016000)
I0413 19:29:08.261435 15949 net.cpp:165] Memory required for data: 3102064116
I0413 19:29:08.261441 15949 layer_factory.hpp:77] Creating layer conv4_1
I0413 19:29:08.261456 15949 net.cpp:100] Creating Layer conv4_1
I0413 19:29:08.261461 15949 net.cpp:434] conv4_1 <- pool3
I0413 19:29:08.261468 15949 net.cpp:408] conv4_1 -> conv4_1
I0413 19:29:08.264093 15949 net.cpp:150] Setting up conv4_1
I0413 19:29:08.264127 15949 net.cpp:157] Top shape: 1 512 125 188 (12032000)
I0413 19:29:08.264132 15949 net.cpp:165] Memory required for data: 3150192116
I0413 19:29:08.264147 15949 layer_factory.hpp:77] Creating layer relu4_1
I0413 19:29:08.264160 15949 net.cpp:100] Creating Layer relu4_1
I0413 19:29:08.264168 15949 net.cpp:434] relu4_1 <- conv4_1
I0413 19:29:08.264178 15949 net.cpp:395] relu4_1 -> conv4_1 (in-place)
I0413 19:29:08.264191 15949 net.cpp:150] Setting up relu4_1
I0413 19:29:08.264197 15949 net.cpp:157] Top shape: 1 512 125 188 (12032000)
I0413 19:29:08.264201 15949 net.cpp:165] Memory required for data: 3198320116
I0413 19:29:08.264209 15949 layer_factory.hpp:77] Creating layer conv4_2
I0413 19:29:08.264222 15949 net.cpp:100] Creating Layer conv4_2
I0413 19:29:08.264227 15949 net.cpp:434] conv4_2 <- conv4_1
I0413 19:29:08.264237 15949 net.cpp:408] conv4_2 -> conv4_2
I0413 19:29:08.268553 15949 net.cpp:150] Setting up conv4_2
I0413 19:29:08.268587 15949 net.cpp:157] Top shape: 1 512 125 188 (12032000)
I0413 19:29:08.268592 15949 net.cpp:165] Memory required for data: 3246448116
I0413 19:29:08.268613 15949 layer_factory.hpp:77] Creating layer relu4_2
I0413 19:29:08.268628 15949 net.cpp:100] Creating Layer relu4_2
I0413 19:29:08.268638 15949 net.cpp:434] relu4_2 <- conv4_2
I0413 19:29:08.268651 15949 net.cpp:395] relu4_2 -> conv4_2 (in-place)
I0413 19:29:08.268666 15949 net.cpp:150] Setting up relu4_2
I0413 19:29:08.268676 15949 net.cpp:157] Top shape: 1 512 125 188 (12032000)
I0413 19:29:08.268681 15949 net.cpp:165] Memory required for data: 3294576116
I0413 19:29:08.268688 15949 layer_factory.hpp:77] Creating layer conv4_3
I0413 19:29:08.268702 15949 net.cpp:100] Creating Layer conv4_3
I0413 19:29:08.268708 15949 net.cpp:434] conv4_3 <- conv4_2
I0413 19:29:08.268715 15949 net.cpp:408] conv4_3 -> conv4_3
I0413 19:29:08.273227 15949 net.cpp:150] Setting up conv4_3
I0413 19:29:08.273264 15949 net.cpp:157] Top shape: 1 512 125 188 (12032000)
I0413 19:29:08.273268 15949 net.cpp:165] Memory required for data: 3342704116
I0413 19:29:08.273283 15949 layer_factory.hpp:77] Creating layer relu4_3
I0413 19:29:08.273298 15949 net.cpp:100] Creating Layer relu4_3
I0413 19:29:08.273317 15949 net.cpp:434] relu4_3 <- conv4_3
I0413 19:29:08.273332 15949 net.cpp:395] relu4_3 -> conv4_3 (in-place)
I0413 19:29:08.273346 15949 net.cpp:150] Setting up relu4_3
I0413 19:29:08.273355 15949 net.cpp:157] Top shape: 1 512 125 188 (12032000)
I0413 19:29:08.273360 15949 net.cpp:165] Memory required for data: 3390832116
I0413 19:29:08.273365 15949 layer_factory.hpp:77] Creating layer pool4
I0413 19:29:08.273377 15949 net.cpp:100] Creating Layer pool4
I0413 19:29:08.273385 15949 net.cpp:434] pool4 <- conv4_3
I0413 19:29:08.273393 15949 net.cpp:408] pool4 -> pool4
I0413 19:29:08.273448 15949 net.cpp:150] Setting up pool4
I0413 19:29:08.273460 15949 net.cpp:157] Top shape: 1 512 63 94 (3032064)
I0413 19:29:08.273466 15949 net.cpp:165] Memory required for data: 3402960372
I0413 19:29:08.273473 15949 layer_factory.hpp:77] Creating layer conv5_1
I0413 19:29:08.273488 15949 net.cpp:100] Creating Layer conv5_1
I0413 19:29:08.273495 15949 net.cpp:434] conv5_1 <- pool4
I0413 19:29:08.273504 15949 net.cpp:408] conv5_1 -> conv5_1
I0413 19:29:08.278479 15949 net.cpp:150] Setting up conv5_1
I0413 19:29:08.278513 15949 net.cpp:157] Top shape: 1 512 63 94 (3032064)
I0413 19:29:08.278527 15949 net.cpp:165] Memory required for data: 3415088628
I0413 19:29:08.278537 15949 layer_factory.hpp:77] Creating layer relu5_1
I0413 19:29:08.278548 15949 net.cpp:100] Creating Layer relu5_1
I0413 19:29:08.278553 15949 net.cpp:434] relu5_1 <- conv5_1
I0413 19:29:08.278563 15949 net.cpp:395] relu5_1 -> conv5_1 (in-place)
I0413 19:29:08.278578 15949 net.cpp:150] Setting up relu5_1
I0413 19:29:08.278599 15949 net.cpp:157] Top shape: 1 512 63 94 (3032064)
I0413 19:29:08.278605 15949 net.cpp:165] Memory required for data: 3427216884
I0413 19:29:08.278614 15949 layer_factory.hpp:77] Creating layer conv5_2
I0413 19:29:08.278628 15949 net.cpp:100] Creating Layer conv5_2
I0413 19:29:08.278635 15949 net.cpp:434] conv5_2 <- conv5_1
I0413 19:29:08.278645 15949 net.cpp:408] conv5_2 -> conv5_2
I0413 19:29:08.283164 15949 net.cpp:150] Setting up conv5_2
I0413 19:29:08.283206 15949 net.cpp:157] Top shape: 1 512 63 94 (3032064)
I0413 19:29:08.283211 15949 net.cpp:165] Memory required for data: 3439345140
I0413 19:29:08.283221 15949 layer_factory.hpp:77] Creating layer relu5_2
I0413 19:29:08.283231 15949 net.cpp:100] Creating Layer relu5_2
I0413 19:29:08.283237 15949 net.cpp:434] relu5_2 <- conv5_2
I0413 19:29:08.283251 15949 net.cpp:395] relu5_2 -> conv5_2 (in-place)
I0413 19:29:08.283264 15949 net.cpp:150] Setting up relu5_2
I0413 19:29:08.283283 15949 net.cpp:157] Top shape: 1 512 63 94 (3032064)
I0413 19:29:08.283290 15949 net.cpp:165] Memory required for data: 3451473396
I0413 19:29:08.283296 15949 layer_factory.hpp:77] Creating layer conv5_3
I0413 19:29:08.283320 15949 net.cpp:100] Creating Layer conv5_3
I0413 19:29:08.283335 15949 net.cpp:434] conv5_3 <- conv5_2
I0413 19:29:08.283344 15949 net.cpp:408] conv5_3 -> conv5_3
I0413 19:29:08.288000 15949 net.cpp:150] Setting up conv5_3
I0413 19:29:08.288046 15949 net.cpp:157] Top shape: 1 512 63 94 (3032064)
I0413 19:29:08.288050 15949 net.cpp:165] Memory required for data: 3463601652
I0413 19:29:08.288063 15949 layer_factory.hpp:77] Creating layer relu5_3
I0413 19:29:08.288079 15949 net.cpp:100] Creating Layer relu5_3
I0413 19:29:08.288089 15949 net.cpp:434] relu5_3 <- conv5_3
I0413 19:29:08.288100 15949 net.cpp:395] relu5_3 -> conv5_3 (in-place)
I0413 19:29:08.288125 15949 net.cpp:150] Setting up relu5_3
I0413 19:29:08.288133 15949 net.cpp:157] Top shape: 1 512 63 94 (3032064)
I0413 19:29:08.288138 15949 net.cpp:165] Memory required for data: 3475729908
I0413 19:29:08.288144 15949 layer_factory.hpp:77] Creating layer conv5_3_relu5_3_0_split
I0413 19:29:08.288156 15949 net.cpp:100] Creating Layer conv5_3_relu5_3_0_split
I0413 19:29:08.288161 15949 net.cpp:434] conv5_3_relu5_3_0_split <- conv5_3
I0413 19:29:08.288172 15949 net.cpp:408] conv5_3_relu5_3_0_split -> conv5_3_relu5_3_0_split_0
I0413 19:29:08.288184 15949 net.cpp:408] conv5_3_relu5_3_0_split -> conv5_3_relu5_3_0_split_1
I0413 19:29:08.288234 15949 net.cpp:150] Setting up conv5_3_relu5_3_0_split
I0413 19:29:08.288245 15949 net.cpp:157] Top shape: 1 512 63 94 (3032064)
I0413 19:29:08.288252 15949 net.cpp:157] Top shape: 1 512 63 94 (3032064)
I0413 19:29:08.288259 15949 net.cpp:165] Memory required for data: 3499986420
I0413 19:29:08.288265 15949 layer_factory.hpp:77] Creating layer rpn_big_conv1
I0413 19:29:08.288282 15949 net.cpp:100] Creating Layer rpn_big_conv1
I0413 19:29:08.288288 15949 net.cpp:434] rpn_big_conv1 <- conv5_3_relu5_3_0_split_0
I0413 19:29:08.288300 15949 net.cpp:408] rpn_big_conv1 -> rpn_big_conv1
I0413 19:29:08.313772 15949 net.cpp:150] Setting up rpn_big_conv1
I0413 19:29:08.313802 15949 net.cpp:157] Top shape: 1 512 63 94 (3032064)
I0413 19:29:08.313814 15949 net.cpp:165] Memory required for data: 3512114676
I0413 19:29:08.313824 15949 layer_factory.hpp:77] Creating layer rpn_big_conv1_relu
I0413 19:29:08.313839 15949 net.cpp:100] Creating Layer rpn_big_conv1_relu
I0413 19:29:08.313848 15949 net.cpp:434] rpn_big_conv1_relu <- rpn_big_conv1
I0413 19:29:08.313859 15949 net.cpp:395] rpn_big_conv1_relu -> rpn_big_conv1 (in-place)
I0413 19:29:08.313876 15949 net.cpp:150] Setting up rpn_big_conv1_relu
I0413 19:29:08.313885 15949 net.cpp:157] Top shape: 1 512 63 94 (3032064)
I0413 19:29:08.313890 15949 net.cpp:165] Memory required for data: 3524242932
I0413 19:29:08.313894 15949 layer_factory.hpp:77] Creating layer rpn_big_conv2
I0413 19:29:08.313910 15949 net.cpp:100] Creating Layer rpn_big_conv2
I0413 19:29:08.313917 15949 net.cpp:434] rpn_big_conv2 <- rpn_big_conv1
I0413 19:29:08.313928 15949 net.cpp:408] rpn_big_conv2 -> rpn_big_conv2
I0413 19:29:08.339823 15949 net.cpp:150] Setting up rpn_big_conv2
I0413 19:29:08.339851 15949 net.cpp:157] Top shape: 1 512 63 94 (3032064)
I0413 19:29:08.339864 15949 net.cpp:165] Memory required for data: 3536371188
I0413 19:29:08.339874 15949 layer_factory.hpp:77] Creating layer rpn_big_conv2_relu
I0413 19:29:08.339885 15949 net.cpp:100] Creating Layer rpn_big_conv2_relu
I0413 19:29:08.339893 15949 net.cpp:434] rpn_big_conv2_relu <- rpn_big_conv2
I0413 19:29:08.339906 15949 net.cpp:395] rpn_big_conv2_relu -> rpn_big_conv2 (in-place)
I0413 19:29:08.339920 15949 net.cpp:150] Setting up rpn_big_conv2_relu
I0413 19:29:08.339927 15949 net.cpp:157] Top shape: 1 512 63 94 (3032064)
I0413 19:29:08.339932 15949 net.cpp:165] Memory required for data: 3548499444
I0413 19:29:08.339938 15949 layer_factory.hpp:77] Creating layer rpn_big_conv2_drop
I0413 19:29:08.339948 15949 net.cpp:100] Creating Layer rpn_big_conv2_drop
I0413 19:29:08.339956 15949 net.cpp:434] rpn_big_conv2_drop <- rpn_big_conv2
I0413 19:29:08.339967 15949 net.cpp:395] rpn_big_conv2_drop -> rpn_big_conv2 (in-place)
I0413 19:29:08.340000 15949 net.cpp:150] Setting up rpn_big_conv2_drop
I0413 19:29:08.340019 15949 net.cpp:157] Top shape: 1 512 63 94 (3032064)
I0413 19:29:08.340024 15949 net.cpp:165] Memory required for data: 3560627700
I0413 19:29:08.340037 15949 layer_factory.hpp:77] Creating layer rpn_big_conv2_rpn_big_conv2_drop_0_split
I0413 19:29:08.340047 15949 net.cpp:100] Creating Layer rpn_big_conv2_rpn_big_conv2_drop_0_split
I0413 19:29:08.340052 15949 net.cpp:434] rpn_big_conv2_rpn_big_conv2_drop_0_split <- rpn_big_conv2
I0413 19:29:08.340061 15949 net.cpp:408] rpn_big_conv2_rpn_big_conv2_drop_0_split -> rpn_big_conv2_rpn_big_conv2_drop_0_split_0
I0413 19:29:08.340075 15949 net.cpp:408] rpn_big_conv2_rpn_big_conv2_drop_0_split -> rpn_big_conv2_rpn_big_conv2_drop_0_split_1
I0413 19:29:08.340139 15949 net.cpp:150] Setting up rpn_big_conv2_rpn_big_conv2_drop_0_split
I0413 19:29:08.340149 15949 net.cpp:157] Top shape: 1 512 63 94 (3032064)
I0413 19:29:08.340167 15949 net.cpp:157] Top shape: 1 512 63 94 (3032064)
I0413 19:29:08.340173 15949 net.cpp:165] Memory required for data: 3584884212
I0413 19:29:08.340178 15949 layer_factory.hpp:77] Creating layer rpn_big_output_cls
I0413 19:29:08.340193 15949 net.cpp:100] Creating Layer rpn_big_output_cls
I0413 19:29:08.340199 15949 net.cpp:434] rpn_big_output_cls <- rpn_big_conv2_rpn_big_conv2_drop_0_split_0
I0413 19:29:08.340211 15949 net.cpp:408] rpn_big_output_cls -> rpn_big_output_cls
I0413 19:29:08.361492 15949 net.cpp:150] Setting up rpn_big_output_cls
I0413 19:29:08.361521 15949 net.cpp:157] Top shape: 1 384 63 94 (2274048)
I0413 19:29:08.361533 15949 net.cpp:165] Memory required for data: 3593980404
I0413 19:29:08.361543 15949 layer_factory.hpp:77] Creating layer rpn_big_output_cls_relu
I0413 19:29:08.361557 15949 net.cpp:100] Creating Layer rpn_big_output_cls_relu
I0413 19:29:08.361564 15949 net.cpp:434] rpn_big_output_cls_relu <- rpn_big_output_cls
I0413 19:29:08.361577 15949 net.cpp:395] rpn_big_output_cls_relu -> rpn_big_output_cls (in-place)
I0413 19:29:08.361590 15949 net.cpp:150] Setting up rpn_big_output_cls_relu
I0413 19:29:08.361600 15949 net.cpp:157] Top shape: 1 384 63 94 (2274048)
I0413 19:29:08.361608 15949 net.cpp:165] Memory required for data: 3603076596
I0413 19:29:08.361616 15949 layer_factory.hpp:77] Creating layer rpn_big_cls_score
I0413 19:29:08.361634 15949 net.cpp:100] Creating Layer rpn_big_cls_score
I0413 19:29:08.361649 15949 net.cpp:434] rpn_big_cls_score <- rpn_big_output_cls
I0413 19:29:08.361670 15949 net.cpp:408] rpn_big_cls_score -> rpn_big_cls_score
I0413 19:29:08.365615 15949 net.cpp:150] Setting up rpn_big_cls_score
I0413 19:29:08.365630 15949 net.cpp:157] Top shape: 1 52 61 92 (291824)
I0413 19:29:08.365634 15949 net.cpp:165] Memory required for data: 3604243892
I0413 19:29:08.365656 15949 layer_factory.hpp:77] Creating layer rpn_big_cls_score_rpn_big_cls_score_0_split
I0413 19:29:08.365665 15949 net.cpp:100] Creating Layer rpn_big_cls_score_rpn_big_cls_score_0_split
I0413 19:29:08.365672 15949 net.cpp:434] rpn_big_cls_score_rpn_big_cls_score_0_split <- rpn_big_cls_score
I0413 19:29:08.365685 15949 net.cpp:408] rpn_big_cls_score_rpn_big_cls_score_0_split -> rpn_big_cls_score_rpn_big_cls_score_0_split_0
I0413 19:29:08.365701 15949 net.cpp:408] rpn_big_cls_score_rpn_big_cls_score_0_split -> rpn_big_cls_score_rpn_big_cls_score_0_split_1
I0413 19:29:08.365757 15949 net.cpp:150] Setting up rpn_big_cls_score_rpn_big_cls_score_0_split
I0413 19:29:08.365768 15949 net.cpp:157] Top shape: 1 52 61 92 (291824)
I0413 19:29:08.365774 15949 net.cpp:157] Top shape: 1 52 61 92 (291824)
I0413 19:29:08.365787 15949 net.cpp:165] Memory required for data: 3606578484
I0413 19:29:08.365793 15949 layer_factory.hpp:77] Creating layer rpn_big_cls_score_reshape
I0413 19:29:08.365803 15949 net.cpp:100] Creating Layer rpn_big_cls_score_reshape
I0413 19:29:08.365809 15949 net.cpp:434] rpn_big_cls_score_reshape <- rpn_big_cls_score_rpn_big_cls_score_0_split_0
I0413 19:29:08.365825 15949 net.cpp:408] rpn_big_cls_score_reshape -> rpn_big_cls_score_reshape
I0413 19:29:08.365869 15949 net.cpp:150] Setting up rpn_big_cls_score_reshape
I0413 19:29:08.365880 15949 net.cpp:157] Top shape: 1 2 1586 92 (291824)
I0413 19:29:08.365885 15949 net.cpp:165] Memory required for data: 3607745780
I0413 19:29:08.365897 15949 layer_factory.hpp:77] Creating layer rpn_big_cls_score_reshape_rpn_big_cls_score_reshape_0_split
I0413 19:29:08.365906 15949 net.cpp:100] Creating Layer rpn_big_cls_score_reshape_rpn_big_cls_score_reshape_0_split
I0413 19:29:08.365913 15949 net.cpp:434] rpn_big_cls_score_reshape_rpn_big_cls_score_reshape_0_split <- rpn_big_cls_score_reshape
I0413 19:29:08.365924 15949 net.cpp:408] rpn_big_cls_score_reshape_rpn_big_cls_score_reshape_0_split -> rpn_big_cls_score_reshape_rpn_big_cls_score_reshape_0_split_0
I0413 19:29:08.365937 15949 net.cpp:408] rpn_big_cls_score_reshape_rpn_big_cls_score_reshape_0_split -> rpn_big_cls_score_reshape_rpn_big_cls_score_reshape_0_split_1
I0413 19:29:08.365984 15949 net.cpp:150] Setting up rpn_big_cls_score_reshape_rpn_big_cls_score_reshape_0_split
I0413 19:29:08.365995 15949 net.cpp:157] Top shape: 1 2 1586 92 (291824)
I0413 19:29:08.366003 15949 net.cpp:157] Top shape: 1 2 1586 92 (291824)
I0413 19:29:08.366009 15949 net.cpp:165] Memory required for data: 3610080372
I0413 19:29:08.366015 15949 layer_factory.hpp:77] Creating layer rpn_big_cls_prob
I0413 19:29:08.366025 15949 net.cpp:100] Creating Layer rpn_big_cls_prob
I0413 19:29:08.366032 15949 net.cpp:434] rpn_big_cls_prob <- rpn_big_cls_score_reshape_rpn_big_cls_score_reshape_0_split_0
I0413 19:29:08.366044 15949 net.cpp:408] rpn_big_cls_prob -> rpn_big_cls_prob
I0413 19:29:08.366122 15949 net.cpp:150] Setting up rpn_big_cls_prob
I0413 19:29:08.366132 15949 net.cpp:157] Top shape: 1 2 1586 92 (291824)
I0413 19:29:08.366138 15949 net.cpp:165] Memory required for data: 3611247668
I0413 19:29:08.366144 15949 layer_factory.hpp:77] Creating layer rpn_big_cls_prob_reshape
I0413 19:29:08.366156 15949 net.cpp:100] Creating Layer rpn_big_cls_prob_reshape
I0413 19:29:08.366163 15949 net.cpp:434] rpn_big_cls_prob_reshape <- rpn_big_cls_prob
I0413 19:29:08.366173 15949 net.cpp:408] rpn_big_cls_prob_reshape -> rpn_big_cls_prob_reshape
I0413 19:29:08.366204 15949 net.cpp:150] Setting up rpn_big_cls_prob_reshape
I0413 19:29:08.366215 15949 net.cpp:157] Top shape: 1 52 61 92 (291824)
I0413 19:29:08.366221 15949 net.cpp:165] Memory required for data: 3612414964
I0413 19:29:08.366228 15949 layer_factory.hpp:77] Creating layer rpn_big_cls_prob_reshape_rpn_big_cls_prob_reshape_0_split
I0413 19:29:08.366240 15949 net.cpp:100] Creating Layer rpn_big_cls_prob_reshape_rpn_big_cls_prob_reshape_0_split
I0413 19:29:08.366246 15949 net.cpp:434] rpn_big_cls_prob_reshape_rpn_big_cls_prob_reshape_0_split <- rpn_big_cls_prob_reshape
I0413 19:29:08.366256 15949 net.cpp:408] rpn_big_cls_prob_reshape_rpn_big_cls_prob_reshape_0_split -> rpn_big_cls_prob_reshape_rpn_big_cls_prob_reshape_0_split_0
I0413 19:29:08.366268 15949 net.cpp:408] rpn_big_cls_prob_reshape_rpn_big_cls_prob_reshape_0_split -> rpn_big_cls_prob_reshape_rpn_big_cls_prob_reshape_0_split_1
I0413 19:29:08.366312 15949 net.cpp:150] Setting up rpn_big_cls_prob_reshape_rpn_big_cls_prob_reshape_0_split
I0413 19:29:08.366322 15949 net.cpp:157] Top shape: 1 52 61 92 (291824)
I0413 19:29:08.366329 15949 net.cpp:157] Top shape: 1 52 61 92 (291824)
I0413 19:29:08.366335 15949 net.cpp:165] Memory required for data: 3614749556
I0413 19:29:08.366341 15949 layer_factory.hpp:77] Creating layer rpn_big_anchor_target
I0413 19:29:08.368106 15949 net.cpp:100] Creating Layer rpn_big_anchor_target
I0413 19:29:08.368125 15949 net.cpp:434] rpn_big_anchor_target <- rpn_big_cls_score_rpn_big_cls_score_0_split_1
I0413 19:29:08.368134 15949 net.cpp:434] rpn_big_anchor_target <- input_gt_boxes_input_2_split_0
I0413 19:29:08.368140 15949 net.cpp:434] rpn_big_anchor_target <- input_ignored_boxes
I0413 19:29:08.368150 15949 net.cpp:434] rpn_big_anchor_target <- input_im_info_input_1_split_0
I0413 19:29:08.368158 15949 net.cpp:434] rpn_big_anchor_target <- input_data_input_0_split_1
I0413 19:29:08.368166 15949 net.cpp:434] rpn_big_anchor_target <- rpn_big_cls_prob_reshape_rpn_big_cls_prob_reshape_0_split_0
I0413 19:29:08.368177 15949 net.cpp:408] rpn_big_anchor_target -> rpn_big_anchor_target_labels
I0413 19:29:08.368193 15949 net.cpp:408] rpn_big_anchor_target -> rpn_big_anchor_target_bbox_targets
I0413 19:29:08.368207 15949 net.cpp:408] rpn_big_anchor_target -> rpn_big_anchor_target_inside_weights
I0413 19:29:08.368221 15949 net.cpp:408] rpn_big_anchor_target -> rpn_big_anchor_target_outside_weights
I0413 19:29:08.375113 15949 net.cpp:150] Setting up rpn_big_anchor_target
I0413 19:29:08.375144 15949 net.cpp:157] Top shape: 1 1 1586 92 (145912)
I0413 19:29:08.375159 15949 net.cpp:157] Top shape: 1 104 61 92 (583648)
I0413 19:29:08.375164 15949 net.cpp:157] Top shape: 1 104 61 92 (583648)
I0413 19:29:08.375166 15949 net.cpp:157] Top shape: 1 104 61 92 (583648)
I0413 19:29:08.375169 15949 net.cpp:165] Memory required for data: 3622336980
I0413 19:29:08.375176 15949 layer_factory.hpp:77] Creating layer rpn_big_loss_cls
I0413 19:29:08.375190 15949 net.cpp:100] Creating Layer rpn_big_loss_cls
I0413 19:29:08.375200 15949 net.cpp:434] rpn_big_loss_cls <- rpn_big_cls_score_reshape_rpn_big_cls_score_reshape_0_split_1
I0413 19:29:08.375211 15949 net.cpp:434] rpn_big_loss_cls <- rpn_big_anchor_target_labels
I0413 19:29:08.375219 15949 net.cpp:408] rpn_big_loss_cls -> rpn_big_loss_cls
I0413 19:29:08.375238 15949 layer_factory.hpp:77] Creating layer rpn_big_loss_cls
I0413 19:29:08.376022 15949 net.cpp:150] Setting up rpn_big_loss_cls
I0413 19:29:08.376039 15949 net.cpp:157] Top shape: (1)
I0413 19:29:08.376041 15949 net.cpp:160]     with loss weight 1
I0413 19:29:08.376056 15949 net.cpp:165] Memory required for data: 3622336984
I0413 19:29:08.376065 15949 layer_factory.hpp:77] Creating layer rpn_big_output_box
I0413 19:29:08.376085 15949 net.cpp:100] Creating Layer rpn_big_output_box
I0413 19:29:08.376093 15949 net.cpp:434] rpn_big_output_box <- rpn_big_conv2_rpn_big_conv2_drop_0_split_1
I0413 19:29:08.376106 15949 net.cpp:408] rpn_big_output_box -> rpn_big_output_box
I0413 19:29:08.396456 15949 net.cpp:150] Setting up rpn_big_output_box
I0413 19:29:08.396483 15949 net.cpp:157] Top shape: 1 384 63 94 (2274048)
I0413 19:29:08.396497 15949 net.cpp:165] Memory required for data: 3631433176
I0413 19:29:08.396507 15949 layer_factory.hpp:77] Creating layer rpn_big_output_box_relu
I0413 19:29:08.396517 15949 net.cpp:100] Creating Layer rpn_big_output_box_relu
I0413 19:29:08.396523 15949 net.cpp:434] rpn_big_output_box_relu <- rpn_big_output_box
I0413 19:29:08.396534 15949 net.cpp:395] rpn_big_output_box_relu -> rpn_big_output_box (in-place)
I0413 19:29:08.396548 15949 net.cpp:150] Setting up rpn_big_output_box_relu
I0413 19:29:08.396558 15949 net.cpp:157] Top shape: 1 384 63 94 (2274048)
I0413 19:29:08.396562 15949 net.cpp:165] Memory required for data: 3640529368
I0413 19:29:08.396567 15949 layer_factory.hpp:77] Creating layer rpn_big_bbox_pred
I0413 19:29:08.396584 15949 net.cpp:100] Creating Layer rpn_big_bbox_pred
I0413 19:29:08.396589 15949 net.cpp:434] rpn_big_bbox_pred <- rpn_big_output_box
I0413 19:29:08.396603 15949 net.cpp:408] rpn_big_bbox_pred -> rpn_big_bbox_pred
I0413 19:29:08.400914 15949 net.cpp:150] Setting up rpn_big_bbox_pred
I0413 19:29:08.400934 15949 net.cpp:157] Top shape: 1 104 61 92 (583648)
I0413 19:29:08.400938 15949 net.cpp:165] Memory required for data: 3642863960
I0413 19:29:08.400948 15949 layer_factory.hpp:77] Creating layer rpn_big_bbox_pred_rpn_big_bbox_pred_0_split
I0413 19:29:08.400961 15949 net.cpp:100] Creating Layer rpn_big_bbox_pred_rpn_big_bbox_pred_0_split
I0413 19:29:08.400970 15949 net.cpp:434] rpn_big_bbox_pred_rpn_big_bbox_pred_0_split <- rpn_big_bbox_pred
I0413 19:29:08.400979 15949 net.cpp:408] rpn_big_bbox_pred_rpn_big_bbox_pred_0_split -> rpn_big_bbox_pred_rpn_big_bbox_pred_0_split_0
I0413 19:29:08.400993 15949 net.cpp:408] rpn_big_bbox_pred_rpn_big_bbox_pred_0_split -> rpn_big_bbox_pred_rpn_big_bbox_pred_0_split_1
I0413 19:29:08.401051 15949 net.cpp:150] Setting up rpn_big_bbox_pred_rpn_big_bbox_pred_0_split
I0413 19:29:08.401062 15949 net.cpp:157] Top shape: 1 104 61 92 (583648)
I0413 19:29:08.401077 15949 net.cpp:157] Top shape: 1 104 61 92 (583648)
I0413 19:29:08.401082 15949 net.cpp:165] Memory required for data: 3647533144
I0413 19:29:08.401087 15949 layer_factory.hpp:77] Creating layer rpn_big_loss_bbox
I0413 19:29:08.401546 15949 net.cpp:100] Creating Layer rpn_big_loss_bbox
I0413 19:29:08.401556 15949 net.cpp:434] rpn_big_loss_bbox <- rpn_big_bbox_pred_rpn_big_bbox_pred_0_split_0
I0413 19:29:08.401562 15949 net.cpp:434] rpn_big_loss_bbox <- rpn_big_anchor_target_bbox_targets
I0413 19:29:08.401572 15949 net.cpp:434] rpn_big_loss_bbox <- rpn_big_anchor_target_inside_weights
I0413 19:29:08.401581 15949 net.cpp:434] rpn_big_loss_bbox <- rpn_big_anchor_target_outside_weights
I0413 19:29:08.401592 15949 net.cpp:408] rpn_big_loss_bbox -> rpn_big_loss_bbox
I0413 19:29:08.403630 15949 net.cpp:150] Setting up rpn_big_loss_bbox
I0413 19:29:08.403666 15949 net.cpp:157] Top shape: 1 (1)
I0413 19:29:08.403668 15949 net.cpp:160]     with loss weight 1
I0413 19:29:08.403682 15949 net.cpp:165] Memory required for data: 3647533148
I0413 19:29:08.403692 15949 layer_factory.hpp:77] Creating layer proposal
I0413 19:29:08.404122 15949 net.cpp:100] Creating Layer proposal
I0413 19:29:08.404134 15949 net.cpp:434] proposal <- rpn_big_cls_prob_reshape_rpn_big_cls_prob_reshape_0_split_1
I0413 19:29:08.404144 15949 net.cpp:434] proposal <- rpn_big_bbox_pred_rpn_big_bbox_pred_0_split_1
I0413 19:29:08.404153 15949 net.cpp:434] proposal <- input_im_info_input_1_split_1
I0413 19:29:08.404166 15949 net.cpp:408] proposal -> rpn_rois
I0413 19:29:08.408627 15949 net.cpp:150] Setting up proposal
I0413 19:29:08.408639 15949 net.cpp:157] Top shape: 1 5 (5)
I0413 19:29:08.408643 15949 net.cpp:165] Memory required for data: 3647533168
I0413 19:29:08.408648 15949 layer_factory.hpp:77] Creating layer roi-data
I0413 19:29:08.411483 15949 net.cpp:100] Creating Layer roi-data
I0413 19:29:08.411510 15949 net.cpp:434] roi-data <- rpn_rois
I0413 19:29:08.411523 15949 net.cpp:434] roi-data <- input_gt_boxes_input_2_split_1
I0413 19:29:08.411535 15949 net.cpp:408] roi-data -> rois
I0413 19:29:08.411550 15949 net.cpp:408] roi-data -> labels
I0413 19:29:08.411562 15949 net.cpp:408] roi-data -> bbox_targets
I0413 19:29:08.411581 15949 net.cpp:408] roi-data -> bbox_inside_weights
I0413 19:29:08.411594 15949 net.cpp:408] roi-data -> bbox_outside_weights
I0413 19:29:08.411610 15949 net.cpp:408] roi-data -> pid_label
anchors:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
<caffe._caffe.Blob object at 0x7fc5c5f63e40>
I0413 19:29:08.413338 15949 net.cpp:150] Setting up roi-data
I0413 19:29:08.413357 15949 net.cpp:157] Top shape: 1 5 (5)
I0413 19:29:08.413373 15949 net.cpp:157] Top shape: 1 1 (1)
I0413 19:29:08.413381 15949 net.cpp:157] Top shape: 1 8 (8)
I0413 19:29:08.413388 15949 net.cpp:157] Top shape: 1 8 (8)
I0413 19:29:08.413395 15949 net.cpp:157] Top shape: 1 8 (8)
I0413 19:29:08.413403 15949 net.cpp:157] Top shape: 1 1 (1)
I0413 19:29:08.413408 15949 net.cpp:165] Memory required for data: 3647533292
I0413 19:29:08.413415 15949 layer_factory.hpp:77] Creating layer roi-pool
I0413 19:29:08.413426 15949 net.cpp:100] Creating Layer roi-pool
I0413 19:29:08.413434 15949 net.cpp:434] roi-pool <- conv5_3_relu5_3_0_split_1
I0413 19:29:08.413444 15949 net.cpp:434] roi-pool <- rois
I0413 19:29:08.413452 15949 net.cpp:408] roi-pool -> roi-pool
I0413 19:29:08.413465 15949 roi_pooling_layer.cpp:33] Spatial scale: 0.0625
I0413 19:29:08.413481 15949 roi_pooling_layer.cpp:34] Box scale: 1
I0413 19:29:08.413533 15949 net.cpp:150] Setting up roi-pool
I0413 19:29:08.413544 15949 net.cpp:157] Top shape: 1 512 7 7 (25088)
I0413 19:29:08.413550 15949 net.cpp:165] Memory required for data: 3647633644
I0413 19:29:08.413556 15949 layer_factory.hpp:77] Creating layer fc6
I0413 19:29:08.413568 15949 net.cpp:100] Creating Layer fc6
I0413 19:29:08.413574 15949 net.cpp:434] fc6 <- roi-pool
I0413 19:29:08.413583 15949 net.cpp:408] fc6 -> fc6
I0413 19:29:08.582340 15949 net.cpp:150] Setting up fc6
I0413 19:29:08.582376 15949 net.cpp:157] Top shape: 1 4096 (4096)
I0413 19:29:08.582388 15949 net.cpp:165] Memory required for data: 3647650028
I0413 19:29:08.582401 15949 layer_factory.hpp:77] Creating layer relu6
I0413 19:29:08.582412 15949 net.cpp:100] Creating Layer relu6
I0413 19:29:08.582417 15949 net.cpp:434] relu6 <- fc6
I0413 19:29:08.582424 15949 net.cpp:395] relu6 -> fc6 (in-place)
I0413 19:29:08.582435 15949 net.cpp:150] Setting up relu6
I0413 19:29:08.582439 15949 net.cpp:157] Top shape: 1 4096 (4096)
I0413 19:29:08.582442 15949 net.cpp:165] Memory required for data: 3647666412
I0413 19:29:08.582445 15949 layer_factory.hpp:77] Creating layer drop6
I0413 19:29:08.582451 15949 net.cpp:100] Creating Layer drop6
I0413 19:29:08.582455 15949 net.cpp:434] drop6 <- fc6
I0413 19:29:08.582459 15949 net.cpp:395] drop6 -> fc6 (in-place)
I0413 19:29:08.582481 15949 net.cpp:150] Setting up drop6
I0413 19:29:08.582486 15949 net.cpp:157] Top shape: 1 4096 (4096)
I0413 19:29:08.582489 15949 net.cpp:165] Memory required for data: 3647682796
I0413 19:29:08.582492 15949 layer_factory.hpp:77] Creating layer fc7
I0413 19:29:08.582499 15949 net.cpp:100] Creating Layer fc7
I0413 19:29:08.582502 15949 net.cpp:434] fc7 <- fc6
I0413 19:29:08.582507 15949 net.cpp:408] fc7 -> fc7
I0413 19:29:08.609081 15949 net.cpp:150] Setting up fc7
I0413 19:29:08.609113 15949 net.cpp:157] Top shape: 1 4096 (4096)
I0413 19:29:08.609125 15949 net.cpp:165] Memory required for data: 3647699180
I0413 19:29:08.609138 15949 layer_factory.hpp:77] Creating layer relu7
I0413 19:29:08.609158 15949 net.cpp:100] Creating Layer relu7
I0413 19:29:08.609163 15949 net.cpp:434] relu7 <- fc7
I0413 19:29:08.609170 15949 net.cpp:395] relu7 -> fc7 (in-place)
I0413 19:29:08.609180 15949 net.cpp:150] Setting up relu7
I0413 19:29:08.609185 15949 net.cpp:157] Top shape: 1 4096 (4096)
I0413 19:29:08.609187 15949 net.cpp:165] Memory required for data: 3647715564
I0413 19:29:08.609191 15949 layer_factory.hpp:77] Creating layer drop7
I0413 19:29:08.609197 15949 net.cpp:100] Creating Layer drop7
I0413 19:29:08.609200 15949 net.cpp:434] drop7 <- fc7
I0413 19:29:08.609205 15949 net.cpp:395] drop7 -> fc7 (in-place)
I0413 19:29:08.609226 15949 net.cpp:150] Setting up drop7
I0413 19:29:08.609231 15949 net.cpp:157] Top shape: 1 4096 (4096)
I0413 19:29:08.609235 15949 net.cpp:165] Memory required for data: 3647731948
I0413 19:29:08.609237 15949 layer_factory.hpp:77] Creating layer fc7_drop7_0_split
I0413 19:29:08.609243 15949 net.cpp:100] Creating Layer fc7_drop7_0_split
I0413 19:29:08.609247 15949 net.cpp:434] fc7_drop7_0_split <- fc7
I0413 19:29:08.609252 15949 net.cpp:408] fc7_drop7_0_split -> fc7_drop7_0_split_0
I0413 19:29:08.609267 15949 net.cpp:408] fc7_drop7_0_split -> fc7_drop7_0_split_1
I0413 19:29:08.609297 15949 net.cpp:150] Setting up fc7_drop7_0_split
I0413 19:29:08.609304 15949 net.cpp:157] Top shape: 1 4096 (4096)
I0413 19:29:08.609308 15949 net.cpp:157] Top shape: 1 4096 (4096)
I0413 19:29:08.609311 15949 net.cpp:165] Memory required for data: 3647764716
I0413 19:29:08.609314 15949 layer_factory.hpp:77] Creating layer bbox_pred
I0413 19:29:08.609321 15949 net.cpp:100] Creating Layer bbox_pred
I0413 19:29:08.609325 15949 net.cpp:434] bbox_pred <- fc7_drop7_0_split_0
I0413 19:29:08.609330 15949 net.cpp:408] bbox_pred -> bbox_pred
I0413 19:29:08.609745 15949 net.cpp:150] Setting up bbox_pred
I0413 19:29:08.609752 15949 net.cpp:157] Top shape: 1 8 (8)
I0413 19:29:08.609755 15949 net.cpp:165] Memory required for data: 3647764748
I0413 19:29:08.609761 15949 layer_factory.hpp:77] Creating layer loss_bbox
I0413 19:29:08.609767 15949 net.cpp:100] Creating Layer loss_bbox
I0413 19:29:08.609771 15949 net.cpp:434] loss_bbox <- bbox_pred
I0413 19:29:08.609776 15949 net.cpp:434] loss_bbox <- bbox_targets
I0413 19:29:08.609781 15949 net.cpp:434] loss_bbox <- bbox_inside_weights
I0413 19:29:08.609784 15949 net.cpp:434] loss_bbox <- bbox_outside_weights
I0413 19:29:08.609789 15949 net.cpp:408] loss_bbox -> loss_bbox
I0413 19:29:08.609858 15949 net.cpp:150] Setting up loss_bbox
I0413 19:29:08.609868 15949 net.cpp:157] Top shape: (1)
I0413 19:29:08.609870 15949 net.cpp:160]     with loss weight 1
I0413 19:29:08.609881 15949 net.cpp:165] Memory required for data: 3647764752
I0413 19:29:08.609885 15949 layer_factory.hpp:77] Creating layer feat
I0413 19:29:08.609891 15949 net.cpp:100] Creating Layer feat
I0413 19:29:08.609894 15949 net.cpp:434] feat <- fc7_drop7_0_split_1
I0413 19:29:08.609900 15949 net.cpp:408] feat -> feat
I0413 19:29:08.621095 15949 net.cpp:150] Setting up feat
I0413 19:29:08.621125 15949 net.cpp:157] Top shape: 1 256 (256)
I0413 19:29:08.621137 15949 net.cpp:165] Memory required for data: 3647765776
I0413 19:29:08.621148 15949 layer_factory.hpp:77] Creating layer relu8
I0413 19:29:08.621158 15949 net.cpp:100] Creating Layer relu8
I0413 19:29:08.621162 15949 net.cpp:434] relu8 <- feat
I0413 19:29:08.621170 15949 net.cpp:395] relu8 -> feat (in-place)
I0413 19:29:08.621181 15949 net.cpp:150] Setting up relu8
I0413 19:29:08.621184 15949 net.cpp:157] Top shape: 1 256 (256)
I0413 19:29:08.621187 15949 net.cpp:165] Memory required for data: 3647766800
I0413 19:29:08.621191 15949 layer_factory.hpp:77] Creating layer drop8
I0413 19:29:08.621196 15949 net.cpp:100] Creating Layer drop8
I0413 19:29:08.621199 15949 net.cpp:434] drop8 <- feat
I0413 19:29:08.621204 15949 net.cpp:395] drop8 -> feat (in-place)
I0413 19:29:08.621224 15949 net.cpp:150] Setting up drop8
I0413 19:29:08.621229 15949 net.cpp:157] Top shape: 1 256 (256)
I0413 19:29:08.621232 15949 net.cpp:165] Memory required for data: 3647767824
I0413 19:29:08.621237 15949 layer_factory.hpp:77] Creating layer pid_score
I0413 19:29:08.621243 15949 net.cpp:100] Creating Layer pid_score
I0413 19:29:08.621246 15949 net.cpp:434] pid_score <- feat
I0413 19:29:08.621253 15949 net.cpp:408] pid_score -> pid_score
I0413 19:29:08.636122 15949 net.cpp:150] Setting up pid_score
I0413 19:29:08.636150 15949 net.cpp:157] Top shape: 1 5533 (5533)
I0413 19:29:08.636162 15949 net.cpp:165] Memory required for data: 3647789956
I0413 19:29:08.636173 15949 layer_factory.hpp:77] Creating layer pid_accuracy
I0413 19:29:08.636191 15949 net.cpp:100] Creating Layer pid_accuracy
I0413 19:29:08.636198 15949 net.cpp:434] pid_accuracy <- pid_score
I0413 19:29:08.636204 15949 net.cpp:434] pid_accuracy <- pid_label
I0413 19:29:08.636210 15949 net.cpp:408] pid_accuracy -> pid_accuracy
I0413 19:29:08.636224 15949 net.cpp:150] Setting up pid_accuracy
I0413 19:29:08.636227 15949 net.cpp:157] Top shape: (1)
I0413 19:29:08.636230 15949 net.cpp:165] Memory required for data: 3647789960
I0413 19:29:08.636234 15949 net.cpp:228] pid_accuracy does not need backward computation.
I0413 19:29:08.636237 15949 net.cpp:228] pid_score does not need backward computation.
I0413 19:29:08.636241 15949 net.cpp:228] drop8 does not need backward computation.
I0413 19:29:08.636245 15949 net.cpp:228] relu8 does not need backward computation.
I0413 19:29:08.636247 15949 net.cpp:228] feat does not need backward computation.
I0413 19:29:08.636251 15949 net.cpp:226] loss_bbox needs backward computation.
I0413 19:29:08.636255 15949 net.cpp:226] bbox_pred needs backward computation.
I0413 19:29:08.636260 15949 net.cpp:226] fc7_drop7_0_split needs backward computation.
I0413 19:29:08.636262 15949 net.cpp:226] drop7 needs backward computation.
I0413 19:29:08.636265 15949 net.cpp:226] relu7 needs backward computation.
I0413 19:29:08.636268 15949 net.cpp:226] fc7 needs backward computation.
I0413 19:29:08.636271 15949 net.cpp:226] drop6 needs backward computation.
I0413 19:29:08.636276 15949 net.cpp:226] relu6 needs backward computation.
I0413 19:29:08.636279 15949 net.cpp:226] fc6 needs backward computation.
I0413 19:29:08.636282 15949 net.cpp:226] roi-pool needs backward computation.
I0413 19:29:08.636286 15949 net.cpp:226] roi-data needs backward computation.
I0413 19:29:08.636291 15949 net.cpp:226] proposal needs backward computation.
I0413 19:29:08.636296 15949 net.cpp:226] rpn_big_loss_bbox needs backward computation.
I0413 19:29:08.636301 15949 net.cpp:226] rpn_big_bbox_pred_rpn_big_bbox_pred_0_split needs backward computation.
I0413 19:29:08.636303 15949 net.cpp:226] rpn_big_bbox_pred needs backward computation.
I0413 19:29:08.636307 15949 net.cpp:226] rpn_big_output_box_relu needs backward computation.
I0413 19:29:08.636310 15949 net.cpp:226] rpn_big_output_box needs backward computation.
I0413 19:29:08.636313 15949 net.cpp:226] rpn_big_loss_cls needs backward computation.
I0413 19:29:08.636320 15949 net.cpp:226] rpn_big_anchor_target needs backward computation.
I0413 19:29:08.636327 15949 net.cpp:226] rpn_big_cls_prob_reshape_rpn_big_cls_prob_reshape_0_split needs backward computation.
I0413 19:29:08.636330 15949 net.cpp:226] rpn_big_cls_prob_reshape needs backward computation.
I0413 19:29:08.636334 15949 net.cpp:226] rpn_big_cls_prob needs backward computation.
I0413 19:29:08.636339 15949 net.cpp:226] rpn_big_cls_score_reshape_rpn_big_cls_score_reshape_0_split needs backward computation.
I0413 19:29:08.636343 15949 net.cpp:226] rpn_big_cls_score_reshape needs backward computation.
I0413 19:29:08.636346 15949 net.cpp:226] rpn_big_cls_score_rpn_big_cls_score_0_split needs backward computation.
I0413 19:29:08.636350 15949 net.cpp:226] rpn_big_cls_score needs backward computation.
I0413 19:29:08.636353 15949 net.cpp:226] rpn_big_output_cls_relu needs backward computation.
I0413 19:29:08.636358 15949 net.cpp:226] rpn_big_output_cls needs backward computation.
I0413 19:29:08.636360 15949 net.cpp:226] rpn_big_conv2_rpn_big_conv2_drop_0_split needs backward computation.
I0413 19:29:08.636364 15949 net.cpp:226] rpn_big_conv2_drop needs backward computation.
I0413 19:29:08.636368 15949 net.cpp:226] rpn_big_conv2_relu needs backward computation.
I0413 19:29:08.636371 15949 net.cpp:226] rpn_big_conv2 needs backward computation.
I0413 19:29:08.636374 15949 net.cpp:226] rpn_big_conv1_relu needs backward computation.
I0413 19:29:08.636378 15949 net.cpp:226] rpn_big_conv1 needs backward computation.
I0413 19:29:08.636381 15949 net.cpp:226] conv5_3_relu5_3_0_split needs backward computation.
I0413 19:29:08.636384 15949 net.cpp:226] relu5_3 needs backward computation.
I0413 19:29:08.636387 15949 net.cpp:226] conv5_3 needs backward computation.
I0413 19:29:08.636390 15949 net.cpp:226] relu5_2 needs backward computation.
I0413 19:29:08.636394 15949 net.cpp:226] conv5_2 needs backward computation.
I0413 19:29:08.636397 15949 net.cpp:226] relu5_1 needs backward computation.
I0413 19:29:08.636400 15949 net.cpp:226] conv5_1 needs backward computation.
I0413 19:29:08.636404 15949 net.cpp:226] pool4 needs backward computation.
I0413 19:29:08.636407 15949 net.cpp:226] relu4_3 needs backward computation.
I0413 19:29:08.636410 15949 net.cpp:226] conv4_3 needs backward computation.
I0413 19:29:08.636414 15949 net.cpp:226] relu4_2 needs backward computation.
I0413 19:29:08.636416 15949 net.cpp:226] conv4_2 needs backward computation.
I0413 19:29:08.636420 15949 net.cpp:226] relu4_1 needs backward computation.
I0413 19:29:08.636422 15949 net.cpp:226] conv4_1 needs backward computation.
I0413 19:29:08.636426 15949 net.cpp:226] pool3 needs backward computation.
I0413 19:29:08.636430 15949 net.cpp:226] relu3_3 needs backward computation.
I0413 19:29:08.636432 15949 net.cpp:226] conv3_3 needs backward computation.
I0413 19:29:08.636435 15949 net.cpp:226] relu3_2 needs backward computation.
I0413 19:29:08.636438 15949 net.cpp:226] conv3_2 needs backward computation.
I0413 19:29:08.636441 15949 net.cpp:226] relu3_1 needs backward computation.
I0413 19:29:08.636445 15949 net.cpp:226] conv3_1 needs backward computation.
I0413 19:29:08.636448 15949 net.cpp:228] pool2 does not need backward computation.
I0413 19:29:08.636451 15949 net.cpp:228] relu2_2 does not need backward computation.
I0413 19:29:08.636456 15949 net.cpp:228] conv2_2 does not need backward computation.
I0413 19:29:08.636459 15949 net.cpp:228] relu2_1 does not need backward computation.
I0413 19:29:08.636463 15949 net.cpp:228] conv2_1 does not need backward computation.
I0413 19:29:08.636467 15949 net.cpp:228] pool1 does not need backward computation.
I0413 19:29:08.636471 15949 net.cpp:228] relu1_2 does not need backward computation.
I0413 19:29:08.636473 15949 net.cpp:228] conv1_2 does not need backward computation.
I0413 19:29:08.636477 15949 net.cpp:228] relu1_1 does not need backward computation.
I0413 19:29:08.636481 15949 net.cpp:228] conv1_1 does not need backward computation.
I0413 19:29:08.636484 15949 net.cpp:228] input_gt_boxes_input_2_split does not need backward computation.
I0413 19:29:08.636488 15949 net.cpp:228] input_im_info_input_1_split does not need backward computation.
I0413 19:29:08.636492 15949 net.cpp:228] input_data_input_0_split does not need backward computation.
I0413 19:29:08.636497 15949 net.cpp:228] input does not need backward computation.
I0413 19:29:08.636500 15949 net.cpp:270] This network produces output labels
I0413 19:29:08.636504 15949 net.cpp:270] This network produces output loss_bbox
I0413 19:29:08.636507 15949 net.cpp:270] This network produces output pid_accuracy
I0413 19:29:08.636510 15949 net.cpp:270] This network produces output rpn_big_loss_bbox
I0413 19:29:08.636514 15949 net.cpp:270] This network produces output rpn_big_loss_cls
I0413 19:29:08.636564 15949 net.cpp:283] Network initialization done.
I0413 19:29:08.636711 15949 solver.cpp:60] Solver scaffolding done.
Loading pretrained model weights from /home/mopkobka/CourseWork/DockerShared/gml-nn/data/NET/VGG16.v2.caffemodel
[libprotobuf WARNING google/protobuf/io/coded_stream.cc:537] Reading dangerously large protocol message.  If the message turns out to be larger than 2147483647 bytes, parsing will be halted for security reasons.  To increase the limit (or to disable these warnings), see CodedInputStream::SetTotalBytesLimit() in google/protobuf/io/coded_stream.h.
[libprotobuf WARNING google/protobuf/io/coded_stream.cc:78] The total number of bytes read was 553432430
I0413 19:29:09.030619 15949 upgrade_proto.cpp:67] Attempting to upgrade input file specified using deprecated input fields: /home/mopkobka/CourseWork/DockerShared/gml-nn/data/NET/VGG16.v2.caffemodel
I0413 19:29:09.030642 15949 upgrade_proto.cpp:70] Successfully upgraded file specified using deprecated input fields.
W0413 19:29:09.030647 15949 upgrade_proto.cpp:72] Note that future Caffe releases will only support input layers and not input fields.
I0413 19:29:09.041249 15949 net.cpp:761] Ignoring source layer pool5
I0413 19:29:09.122797 15949 net.cpp:761] Ignoring source layer fc8
I0413 19:29:09.122834 15949 net.cpp:761] Ignoring source layer prob
Solving...
F0413 19:29:09.386982 15949 syncedmem.cpp:56] Check failed: error == cudaSuccess (2 vs. 0)  out of memory
*** Check failure stack trace: ***
exps/train_model.sh: line 26: 15949 Aborted                 (core dumped) python3 ./tools/train_net.py --gpu 0 --cfg exps/${EXP_NAME}/config.yml --exp_dir ${EXP_NAME} ${EXTRA_ARGS}
