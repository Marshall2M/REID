+ echo Logging output to exps/example/logs/train_.txt.2017-04-13_19-07-02
Logging output to exps/example/logs/train_.txt.2017-04-13_19-07-02
+ python3 ./tools/train_net.py --gpu 0 --cfg exps/example/config.yml --exp_dir example
/home/mopkobka/CourseWork/venv-gml-nn/lib/python3.5/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.
  "This module will be removed in 0.20.", DeprecationWarning)
Called with args:
Namespace(cfg_file='exps/example/config.yml', exp_dir='example', gpu_id=0)
Using config:
{'DATA_DIR': '/home/mopkobka/CourseWork/gml-nn-detector/data',
 'DEDUP_BOXES': 0.0625,
 'DRAW_NET': False,
 'EPS': 1e-14,
 'EXP_DIR': 'example',
 'GPU_ID': 0,
 'MATLAB': 'matlab',
 'MODEL': {'ATTACHED_NETS': [{'ANCHOR_RATIOS': [1.0],
                              'ANCHOR_SCALES': [3, 6, 9, 12],
                              'ANCHOR_SHIFT_NUM_XY': [[1, 1],
                                                      [1, 1],
                                                      [1, 1],
                                                      [1, 1]],
                              'ARCHITECTURE': "m.add_layer(ConvWithActivation(p('conv1'), "
                                              '128, 3))\n'
                                              "m.add_layer(ConvWithActivation(p('conv2'), "
                                              '128, 3, dropout=0.3),\n'
                                              '            parent_layer=-1)\n'
                                              '\n'
                                              "m.add_layer(ConvWithActivation(p('output_cls'), "
                                              '80, 3),\n'
                                              '            '
                                              "parent_layer=p('conv2'))\n"
                                              '\n'
                                              "m.add_layer(ConvWithActivation(p('output_box'), "
                                              '80, 3),\n'
                                              '            '
                                              "parent_layer=p('conv2'))",
                              'BATCHSIZE': 256,
                              'DISABLED': True,
                              'FG_FRACTION': 0.5,
                              'NAME': 'rpn_small',
                              'NEGATIVE_OVERLAP': 0.4,
                              'NMS_THRESH': 0.7,
                              'PARENT_LAYER': 'relu2_2',
                              'POSITIVE_OVERLAP': 0.6,
                              'POST_NMS_TOP_N': 500,
                              'PRE_NMS_TOP_N': 12000,
                              'SQUARE_TARGETS': True,
                              'TOP_NEGATIVE_FRACTION': 0.2,
                              'TYPE': 'RPN'},
                             {'ANCHOR_RATIOS': [1.0],
                              'ANCHOR_SCALES': [2.5,
                                                3.025,
                                                3.66025,
                                                4.4289024999999995,
                                                5.358972025,
                                                6.484356150249999,
                                                7.846070941802498,
                                                9.493745839581022,
                                                11.487432465893036,
                                                13.899793283730574,
                                                16.818749873313998,
                                                20.350687346709933,
                                                24.62433168951902,
                                                29.795441344318014],
                              'ANCHOR_SHIFT_NUM_XY': [[2, 2],
                                                      [2, 2],
                                                      [2, 2],
                                                      [2, 2],
                                                      [1, 1],
                                                      [1, 1],
                                                      [1, 1],
                                                      [1, 1],
                                                      [1, 1],
                                                      [1, 1],
                                                      [1, 1],
                                                      [1, 1],
                                                      [1, 1],
                                                      [1, 1]],
                              'ARCHITECTURE': "m.add_layer(ConvWithActivation(p('conv1'), "
                                              '512, 3))\n'
                                              "m.add_layer(ConvWithActivation(p('conv2'), "
                                              '512, 3, dropout=0.3),\n'
                                              '            parent_layer=-1)\n'
                                              '\n'
                                              "m.add_layer(ConvWithActivation(p('output_cls'), "
                                              '384, 3),\n'
                                              '            '
                                              "parent_layer=p('conv2'))\n"
                                              '\n'
                                              "m.add_layer(ConvWithActivation(p('output_box'), "
                                              '384, 3),\n'
                                              '            '
                                              "parent_layer=p('conv2'))",
                              'BATCHSIZE': 256,
                              'FG_FRACTION': 0.5,
                              'NAME': 'rpn_big',
                              'NEGATIVE_OVERLAP': 0.4,
                              'NMS_THRESH': 0.7,
                              'PARENT_LAYER': 'relu5_3',
                              'POSITIVE_OVERLAP': 0.6,
                              'POST_NMS_TOP_N': 500,
                              'PRE_NMS_TOP_N': 12000,
                              'SQUARE_TARGETS': True,
                              'TOP_NEGATIVE_FRACTION': 0.2,
                              'TYPE': 'RPN'}],
           'NAME': 'faces_example',
           'PRETRAINED_MODEL_CONFIG': '/home/mopkobka/CourseWork/gml-nn-detector/models/vgg16/model.prototxt',
           'WEIGHTS_PATH': '/home/mopkobka/CourseWork/DockerShared/gml-nn/data/NET/VGG16.v2.caffemodel'},
 'MODELS_DIR': '/home/mopkobka/CourseWork/gml-nn-detector/models',
 'PIXEL_MEANS': array([[[ 102.9801,  115.9465,  122.7717]]]),
 'RNG_SEED': 777,
 'ROOT_DIR': '/home/mopkobka/CourseWork/gml-nn-detector',
 'RPN': {'ANCHOR_BASE_SIZE': 16,
         'ANCHOR_RATIOS': [0.5, 1, 2],
         'ANCHOR_SCALES': [8, 16, 32],
         'ANCHOR_SHIFT_NUM_XY': [(1, 1)]},
 'TEST': {'BBOX_REG': True,
          'DATASETS': [{'MAX_SIZE': 1400,
                        'OUTPUT_FILE': 'videoset.json',
                        'PATH': '/home/local/work/data/faces/BigSampleFNTest_p3lr11/imgs',
                        'SCALES': [350, 1000],
                        'TYPE': 'IMAGES_DIR'},
                       {'MAX_SIZE': 700,
                        'OUTPUT_FILE': 'fddb.json',
                        'PATH': '/home/local/work/data/faces/FDDB/FDDB_p3lr11/imgs',
                        'SCALES': [500],
                        'TYPE': 'IMAGES_DIR'}],
          'DENSE_SCAN': False,
          'FINAL_NMS': 0.5,
          'HAS_RPN': False,
          'MAX_PER_IMAGE': 200,
          'NMS': 0.3,
          'PROPOSAL_METHOD': 'selective_search',
          'RPN_MIN_SIZE': 16,
          'RPN_NMS_THRESH': 0.7,
          'RPN_ONLY': False,
          'RPN_POST_NMS_TOP_N': 300,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SVM': False,
          'VIZUALIZATION': {'DRAW_BOXES': True,
                            'DRAW_SCORES': False,
                            'ENABLE': False,
                            'ONLY_WITH_OBJECTS': False,
                            'SCORE_THRESH': 0.5},
          'WITHOUT_UPSAMPLE': True},
 'TRAIN': {'ASPECT_GROUPING': True,
           'BBOX_INSIDE_WEIGHTS': [1.0, 1.0, 1.0, 1.0],
           'BBOX_NORMALIZE_MEANS': [0.0, 0.0, 0.0, 0.0],
           'BBOX_NORMALIZE_STDS': [0.1, 0.1, 0.2, 0.2],
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_REG': True,
           'BBOX_THRESH': 0.5,
           'DATASET': 'default',
           'DATASETS': [{'MAX_SIZE': 700,
                         'PATH': '/home/mopkobka/CourseWork/DockerShared/gml-nn/tiny_dataset',
                         'SCALES': [500],
                         'TYPE': 'BBOX_JSON_MARKING'}],
           'DOUBLE_GENERATE': False,
           'ENABLE_SMART_ORDER': False,
           'GENERATED_FRACTION': 0.0,
           'HAS_RPN': True,
           'PROPOSAL_METHOD': 'selective_search',
           'RATIO_JITTERING': 0.0,
           'REDISTRIBUTE_CLASSES': False,
           'RPN_BATCHSIZE': 256,
           'RPN_BBOX_INSIDE_WEIGHTS': [1.0, 1.0, 1.0, 1.0],
           'RPN_CLOBBER_POSITIVES': False,
           'RPN_FG_FRACTION': 0.5,
           'RPN_LINEAR_START_TNF': 0.1,
           'RPN_LINEAR_TNF_K': -1.0,
           'RPN_MIN_SIZE': 16,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_PERIODIC_TN': 0,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POSITIVE_WEIGHT': -1.0,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 12000,
           'RPN_SQUARE_TARGETS': False,
           'RPN_SQUARE_TARGETS_KY': False,
           'RPN_TOP_NEGATIVE_FRACTION': 0.0,
           'SCALE_JITTERING': 0.0,
           'SNAPSHOT_INFIX': '',
           'SNAPSHOT_ITERS': 5000,
           'SOLVER': {'BASE_LR': 0.001,
                      'DISPLAY': {'AVERAGE_LOSS': 100, 'PERIOD': 20},
                      'IMS_PER_BATCH': 2,
                      'LR_POLICY': {'GAMMA': 0.1,
                                    'STEPS': [70000, 120000],
                                    'TYPE': 'multistep'},
                      'MOMENTUM': 0.9,
                      'TRAIN_ITERS': 90000,
                      'WEIGHT_DECAY': 0.0005},
           'SO_FORCE_BATCHSIZE': 500,
           'SO_FORCE_ROUNDS': 8,
           'SO_GOOD_SKIP_PROB': 0.85,
           'SO_TAIL_LEN': 1000,
           'USE_FLIPPED': True,
           'USE_LMDB': False,
           'USE_PREFETCH': False},
 'USE_GPU_NMS': True,
 'WEIGHTS_PATH': 'default'}
Output will be saved to `/home/mopkobka/CourseWork/gml-nn-detector/exps/example/output/train_13_04_2017_19_07`
Created solver path: /tmp/tmp51jnn1yz
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> /tmp/tmp51jnn1yz
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0413 19:07:03.761078 15276 solver.cpp:48] Initializing solver from parameters: 
base_lr: 0.001
display: 20
lr_policy: "multistep"
gamma: 0.1
momentum: 0.9
weight_decay: 0.0005
snapshot: 0
snapshot_prefix: "faces_example"
net_param {
  name: "GeneratedModel_13_04_2017_19_07"
  layer {
    name: "input"
    type: "Python"
    top: "input_data"
    top: "input_im_info"
    top: "input_gt_boxes"
    top: "input_ignored_boxes"
    python_param {
      module: "layers.roi_data_layer"
      layer: "RoIDataLayer"
      param_str: "{\"num_classes\": 2}"
    }
  }
  layer {
    name: "conv1_1"
    type: "Convolution"
    bottom: "input_data"
    top: "conv1_1"
    param {
      lr_mult: 0
      decay_mult: 0
    }
    param {
      lr_mult: 0
      decay_mult: 0
    }
    convolution_param {
      num_output: 64
      pad: 1
      kernel_size: 3
    }
  }
  layer {
    name: "relu1_1"
    type: "ReLU"
    bottom: "conv1_1"
    top: "conv1_1"
  }
  layer {
    name: "conv1_2"
    type: "Convolution"
    bottom: "conv1_1"
    top: "conv1_2"
    param {
      lr_mult: 0
      decay_mult: 0
    }
    param {
      lr_mult: 0
      decay_mult: 0
    }
    convolution_param {
      num_output: 64
      pad: 1
      kernel_size: 3
    }
  }
  layer {
    name: "relu1_2"
    type: "ReLU"
    bottom: "conv1_2"
    top: "conv1_2"
  }
  layer {
    name: "pool1"
    type: "Pooling"
    bottom: "conv1_2"
    top: "pool1"
    pooling_param {
      pool: MAX
      kernel_size: 2
      stride: 2
    }
  }
  layer {
    name: "conv2_1"
    type: "Convolution"
    bottom: "pool1"
    top: "conv2_1"
    param {
      lr_mult: 0
      decay_mult: 0
    }
    param {
      lr_mult: 0
      decay_mult: 0
    }
    convolution_param {
      num_output: 128
      pad: 1
      kernel_size: 3
    }
  }
  layer {
    name: "relu2_1"
    type: "ReLU"
    bottom: "conv2_1"
    top: "conv2_1"
  }
  layer {
    name: "conv2_2"
    type: "Convolution"
    bottom: "conv2_1"
    top: "conv2_2"
    param {
      lr_mult: 0
      decay_mult: 0
    }
    param {
      lr_mult: 0
      decay_mult: 0
    }
    convolution_param {
      num_output: 128
      pad: 1
      kernel_size: 3
    }
  }
  layer {
    name: "relu2_2"
    type: "ReLU"
    bottom: "conv2_2"
    top: "conv2_2"
  }
  layer {
    name: "pool2"
    type: "Pooling"
    bottom: "conv2_2"
    top: "pool2"
    pooling_param {
      pool: MAX
      kernel_size: 2
      stride: 2
    }
  }
  layer {
    name: "conv3_1"
    type: "Convolution"
    bottom: "pool2"
    top: "conv3_1"
    param {
      lr_mult: 1
    }
    param {
      lr_mult: 2
    }
    convolution_param {
      num_output: 256
      pad: 1
      kernel_size: 3
    }
  }
  layer {
    name: "relu3_1"
    type: "ReLU"
    bottom: "conv3_1"
    top: "conv3_1"
  }
  layer {
    name: "conv3_2"
    type: "Convolution"
    bottom: "conv3_1"
    top: "conv3_2"
    param {
      lr_mult: 1
    }
    param {
      lr_mult: 2
    }
    convolution_param {
      num_output: 256
      pad: 1
      kernel_size: 3
    }
  }
  layer {
    name: "relu3_2"
    type: "ReLU"
    bottom: "conv3_2"
    top: "conv3_2"
  }
  layer {
    name: "conv3_3"
    type: "Convolution"
    bottom: "conv3_2"
    top: "conv3_3"
    param {
      lr_mult: 1
    }
    param {
      lr_mult: 2
    }
    convolution_param {
      num_output: 256
      pad: 1
      kernel_size: 3
    }
  }
  layer {
    name: "relu3_3"
    type: "ReLU"
    bottom: "conv3_3"
    top: "conv3_3"
  }
  layer {
    name: "pool3"
    type: "Pooling"
    bottom: "conv3_3"
    top: "pool3"
    pooling_param {
      pool: MAX
      kernel_size: 2
      stride: 2
    }
  }
  layer {
    name: "conv4_1"
    type: "Convolution"
    bottom: "pool3"
    top: "conv4_1"
    param {
      lr_mult: 1
    }
    param {
      lr_mult: 2
    }
    convolution_param {
      num_output: 512
      pad: 1
      kernel_size: 3
    }
  }
  layer {
    name: "relu4_1"
    type: "ReLU"
    bottom: "conv4_1"
    top: "conv4_1"
  }
  layer {
    name: "conv4_2"
    type: "Convolution"
    bottom: "conv4_1"
    top: "conv4_2"
    param {
      lr_mult: 1
    }
    param {
      lr_mult: 2
    }
    convolution_param {
      num_output: 512
      pad: 1
      kernel_size: 3
    }
  }
  layer {
    name: "relu4_2"
    type: "ReLU"
    bottom: "conv4_2"
    top: "conv4_2"
  }
  layer {
    name: "conv4_3"
    type: "Convolution"
    bottom: "conv4_2"
    top: "conv4_3"
    param {
      lr_mult: 1
    }
    param {
      lr_mult: 2
    }
    convolution_param {
      num_output: 512
      pad: 1
      kernel_size: 3
    }
  }
  layer {
    name: "relu4_3"
    type: "ReLU"
    bottom: "conv4_3"
    top: "conv4_3"
  }
  layer {
    name: "pool4"
    type: "Pooling"
    bottom: "conv4_3"
    top: "pool4"
    pooling_param {
      pool: MAX
      kernel_size: 2
      stride: 2
    }
  }
  layer {
    name: "conv5_1"
    type: "Convolution"
    bottom: "pool4"
    top: "conv5_1"
    param {
      lr_mult: 1
    }
    param {
      lr_mult: 2
    }
    convolution_param {
      num_output: 512
      pad: 1
      kernel_size: 3
    }
  }
  layer {
    name: "relu5_1"
    type: "ReLU"
    bottom: "conv5_1"
    top: "conv5_1"
  }
  layer {
    name: "conv5_2"
    type: "Convolution"
    bottom: "conv5_1"
    top: "conv5_2"
    param {
      lr_mult: 1
    }
    param {
      lr_mult: 2
    }
    convolution_param {
      num_output: 512
      pad: 1
      kernel_size: 3
    }
  }
  layer {
    name: "relu5_2"
    type: "ReLU"
    bottom: "conv5_2"
    top: "conv5_2"
  }
  layer {
    name: "conv5_3"
    type: "Convolution"
    bottom: "conv5_2"
    top: "conv5_3"
    param {
      lr_mult: 1
    }
    param {
      lr_mult: 2
    }
    convolution_param {
      num_output: 512
      pad: 1
      kernel_size: 3
    }
  }
  layer {
    name: "relu5_3"
    type: "ReLU"
    bottom: "conv5_3"
    top: "conv5_3"
  }
  layer {
    name: "rpn_big_conv1"
    type: "Convolution"
    bottom: "conv5_3"
    top: "rpn_big_conv1"
    param {
      lr_mult: 1
    }
    param {
      lr_mult: 2
    }
    convolution_param {
      num_output: 512
      pad: 1
      kernel_size: 3
      stride: 1
      weight_filler {
        type: "gaussian"
        mean: 0
        std: 0.01
      }
      bias_filler {
        type: "constant"
      }
    }
  }
  layer {
    name: "rpn_big_conv1_relu"
    type: "ReLU"
    bottom: "rpn_big_conv1"
    top: "rpn_big_conv1"
    relu_param {
      negative_slope: 0
    }
  }
  layer {
    name: "rpn_big_conv2"
    type: "Convolution"
    bottom: "rpn_big_conv1"
    top: "rpn_big_conv2"
    param {
      lr_mult: 1
    }
    param {
      lr_mult: 2
    }
    convolution_param {
      num_output: 512
      pad: 1
      kernel_size: 3
      stride: 1
      weight_filler {
        type: "gaussian"
        mean: 0
        std: 0.01
      }
      bias_filler {
        type: "constant"
      }
    }
  }
  layer {
    name: "rpn_big_conv2_relu"
    type: "ReLU"
    bottom: "rpn_big_conv2"
    top: "rpn_big_conv2"
    relu_param {
      negative_slope: 0
    }
  }
  layer {
    name: "rpn_big_conv2_drop"
    type: "Dropout"
    bottom: "rpn_big_conv2"
    top: "rpn_big_conv2"
    dropout_param {
      dropout_ratio: 0.3
    }
  }
  layer {
    name: "rpn_big_output_cls"
    type: "Convolution"
    bottom: "rpn_big_conv2"
    top: "rpn_big_output_cls"
    param {
      lr_mult: 1
    }
    param {
      lr_mult: 2
    }
    convolution_param {
      num_output: 384
      pad: 1
      kernel_size: 3
      stride: 1
      weight_filler {
        type: "gaussian"
        mean: 0
        std: 0.01
      }
      bias_filler {
        type: "constant"
      }
    }
  }
  layer {
    name: "rpn_big_output_cls_relu"
    type: "ReLU"
    bottom: "rpn_big_output_cls"
    top: "rpn_big_output_cls"
    relu_param {
      negative_slope: 0
    }
  }
  layer {
    name: "rpn_big_cls_score"
    type: "Convolution"
    bottom: "rpn_big_output_cls"
    top: "rpn_big_cls_score"
    param {
      lr_mult: 1
    }
    param {
      lr_mult: 2
    }
    convolution_param {
      num_output: 52
      pad: 0
      kernel_size: 3
      stride: 1
      weight_filler {
        type: "gaussian"
        mean: 0
        std: 0.01
      }
      bias_filler {
        type: "constant"
      }
    }
  }
  layer {
    name: "rpn_big_cls_score_reshape"
    type: "Reshape"
    bottom: "rpn_big_cls_score"
    top: "rpn_big_cls_score_reshape"
    reshape_param {
      shape {
        dim: 0
        dim: 2
        dim: -1
        dim: 0
      }
    }
  }
  layer {
    name: "rpn_big_cls_prob"
    type: "Softmax"
    bottom: "rpn_big_cls_score_reshape"
    top: "rpn_big_cls_prob"
  }
  layer {
    name: "rpn_big_cls_prob_reshape"
    type: "Reshape"
    bottom: "rpn_big_cls_prob"
    top: "rpn_big_cls_prob_reshape"
    reshape_param {
      shape {
        dim: 0
        dim: 52
        dim: -1
        dim: 0
      }
    }
  }
  layer {
    name: "rpn_big_anchor_target"
    type: "Python"
    bottom: "rpn_big_cls_score"
    bottom: "input_gt_boxes"
    bottom: "input_ignored_boxes"
    bottom: "input_im_info"
    bottom: "input_data"
    bottom: "rpn_big_cls_prob_reshape"
    top: "rpn_big_anchor_target_labels"
    top: "rpn_big_anchor_target_bbox_targets"
    top: "rpn_big_anchor_target_inside_weights"
    top: "rpn_big_anchor_target_outside_weights"
    python_param {
      module: "rpn.anchor_target_layer"
      layer: "AnchorTargetLayer"
      param_str: "{\"anchor_scales\": [2.5, 3.025, 3.66025, 4.4289024999999995, 5.358972025, 6.484356150249999, 7.846070941802498, 9.493745839581022, 11.487432465893036, 13.899793283730574, 16.818749873313998, 20.350687346709933, 24.62433168951902, 29.795441344318014], \"fg_fraction\": 0.5, \"batchsize\": 256, \"positive_overlap\": 0.6, \"name\": \"rpn_big\", \"tn_fraction\": 0.2, \"negative_overlap\": 0.4, \"feat_stride\": 16, \"anchor_ratios\": [1.0], \"num_classes\": 1, \"anchor_shift_num_xy\": [[2, 2], [2, 2], [2, 2], [2, 2], [1, 1], [1, 1], [1, 1], [1, 1], [1, 1], [1, 1], [1, 1], [1, 1], [1, 1], [1, 1]], \"square_targets\": true}"
    }
  }
  layer {
    name: "rpn_big_loss_cls"
    type: "SoftmaxWithLoss"
    bottom: "rpn_big_cls_score_reshape"
    bottom: "rpn_big_anchor_target_labels"
    top: "rpn_big_loss_cls"
    loss_weight: 1
    propagate_down: true
    propagate_down: false
    loss_param {
      ignore_label: -1
      normalization: VALID
    }
  }
  layer {
    name: "rpn_big_output_box"
    type: "Convolution"
    bottom: "rpn_big_conv2"
    top: "rpn_big_output_box"
    param {
      lr_mult: 1
    }
    param {
      lr_mult: 2
    }
    convolution_param {
      num_output: 384
      pad: 1
      kernel_size: 3
      stride: 1
      weight_filler {
        type: "gaussian"
        mean: 0
        std: 0.01
      }
      bias_filler {
        type: "constant"
      }
    }
  }
  layer {
    name: "rpn_big_output_box_relu"
    type: "ReLU"
    bottom: "rpn_big_output_box"
    top: "rpn_big_output_box"
    relu_param {
      negative_slope: 0
    }
  }
  layer {
    name: "rpn_big_bbox_pred"
    type: "Convolution"
    bottom: "rpn_big_output_box"
    top: "rpn_big_bbox_pred"
    param {
      lr_mult: 1
    }
    param {
      lr_mult: 2
    }
    convolution_param {
      num_output: 104
      pad: 0
      kernel_size: 3
      stride: 1
      weight_filler {
        type: "gaussian"
        mean: 0
        std: 0.01
      }
      bias_filler {
        type: "constant"
      }
    }
  }
  layer {
    name: "rpn_big_loss_bbox"
    type: "Python"
    bottom: "rpn_big_bbox_pred"
    bottom: "rpn_big_anchor_target_bbox_targets"
    bottom: "rpn_big_anchor_target_inside_weights"
    bottom: "rpn_big_anchor_target_outside_weights"
    top: "rpn_big_loss_bbox"
    loss_weight: 1
    python_param {
      module: "layers.smooth_l1_loss"
      layer: "SmoothL1LossLayer"
      param_str: "{\"sigma\": 3}"
    }
  }
  layer {
    name: "proposal"
    type: "Python"
    bottom: "rpn_big_cls_prob_reshape"
    bottom: "rpn_big_bbox_pred"
    bottom: "input_im_info"
    top: "rpn_rois"
    python_param {
      module: "rpn.proposal_layer"
      layer: "ProposalLayer"
      param_str: "{\"anchor_shift_num_xy\": [[2, 2], [2, 2], [2, 2], [2, 2], [1, 1], [1, 1], [1, 1], [1, 1], [1, 1], [1, 1], [1, 1], [1, 1], [1, 1], [1, 1]], \"feat_stride\": 16, \"num_classes\": 1, \"anchor_scales\": [2.5, 3.025, 3.66025, 4.4289024999999995, 5.358972025, 6.484356150249999, 7.846070941802498, 9.493745839581022, 11.487432465893036, 13.899793283730574, 16.818749873313998, 20.350687346709933, 24.62433168951902, 29.795441344318014], \"square_targets\": true, \"name\": \"rpn_big\", \"batchsize\": 256, \"anchor_ratios\": [1.0]}"
    }
  }
  layer {
    name: "roi-data"
    type: "Python"
    bottom: "rpn_rois"
    bottom: "input_gt_boxes"
    top: "rois"
    top: "labels"
    top: "bbox_targets"
    top: "bbox_inside_weights"
    top: "bbox_outside_weights"
    python_param {
      module: "rpn.proposal_target_layer"
      layer: "ProposalTargetLayer"
      param_str: "{\"num_classes\": 2, \"bg_aux_label\": 5532}"
    }
  }
  layer {
    name: "roi-pool"
    type: "ROIPooling"
    bottom: "conv5_3"
    bottom: "rois"
    top: "roi-pool"
    roi_pooling_param {
      pooled_h: 7
      pooled_w: 7
      spatial_scale: 0.0625
    }
  }
  layer {
    name: "fc6"
    type: "InnerProduct"
    bottom: "roi-pool"
    top: "fc6"
    param {
      lr_mult: 1
    }
    param {
      lr_mult: 2
    }
    inner_product_param {
      num_output: 4096
    }
  }
  layer {
    name: "relu6"
    type: "ReLU"
    bottom: "fc6"
    top: "fc6"
  }
  layer {
    name: "drop6"
    type: "Dropout"
    bottom: "fc6"
    top: "fc6"
    dropout_param {
      dropout_ratio: 0.5
    }
  }
  layer {
    name: "fc7"
    type: "InnerProduct"
    bottom: "fc6"
    top: "fc7"
    param {
      lr_mult: 1
    }
    param {
      lr_mult: 2
    }
    inner_product_param {
      num_output: 4096
    }
  }
  layer {
    name: "relu7"
    type: "ReLU"
    bottom: "fc7"
    top: "fc7"
  }
  layer {
    name: "drop7"
    type: "Dropout"
    bottom: "fc7"
    top: "fc7"
    dropout_param {
      dropout_ratio: 0.5
    }
  }
  layer {
    name: "bbox_pred"
    type: "InnerProduct"
    bottom: "fc7"
    top: "bbox_pred"
    param {
      lr_mult: 1
    }
    param {
      lr_mult: 2
    }
    inner_product_param {
      num_output: 8
      weight_filler {
        type: "gaussian"
        std: 0.001
      }
      bias_filler {
        type: "constant"
        value: 0
      }
    }
  }
  layer {
    name: "loss_bbox"
    type: "SmoothL1Loss"
    bottom: "bbox_pred"
    bottom: "bbox_targets"
    bottom: "bbox_inside_weights"
    bottom: "bbox_outside_weights"
    top: "loss_bbox"
    loss_weight: 1
  }
  layer {
    name: "feat"
    type: "InnerProduct"
    bottom: "fc7"
    top: "feat"
    param {
      lr_mult: 1
      decay_mult: 1
    }
    param {
      lr_mult: 2
      decay_mult: 0
    }
    inner_product_param {
      num_output: 256
      weight_filler {
        type: "gaussian"
        std: 0.01
      }
      bias_filler {
        type: "constant"
        value: 0
      }
    }
  }
  layer {
    name: "relu8"
    type: "ReLU"
    bottom: "feat"
    top: "feat"
  }
  layer {
    name: "drop8"
    type: "Dropout"
    bottom: "feat"
    top: "feat"
    dropout_param {
      dropout_ratio: 0.5
    }
  }
  layer {
    name: "pid_score"
    type: "InnerProduct"
    bottom: "feat"
    top: "pid_score"
    param {
      lr_mult: 1
      decay_mult: 1
    }
    param {
      lr_mult: 2
      decay_mult: 0
    }
    inner_product_param {
      num_output: 5533
      weight_filler {
        type: "gaussian"
        std: 0.001
      }
      bias_filler {
        type: "constant"
        value: 0
      }
    }
  }
}
average_loss: 100
stepvalue: 70000
stepvalue: 120000
iter_size: 2
I0413 19:07:03.761188 15276 solver.cpp:86] Creating training net specified in net_param.
I0413 19:07:03.761981 15276 net.cpp:58] Initializing net from parameters: 
name: "GeneratedModel_13_04_2017_19_07"
state {
  phase: TRAIN
}
layer {
  name: "input"
  type: "Python"
  top: "input_data"
  top: "input_im_info"
  top: "input_gt_boxes"
  top: "input_ignored_boxes"
  python_param {
    module: "layers.roi_data_layer"
    layer: "RoIDataLayer"
    param_str: "{\"num_classes\": 2}"
  }
}
layer {
  name: "conv1_1"
  type: "Convolution"
  bottom: "input_data"
  top: "conv1_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "relu1_1"
  type: "ReLU"
  bottom: "conv1_1"
  top: "conv1_1"
}
layer {
  name: "conv1_2"
  type: "Convolution"
  bottom: "conv1_1"
  top: "conv1_2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "relu1_2"
  type: "ReLU"
  bottom: "conv1_2"
  top: "conv1_2"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1_2"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2_1"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "relu2_1"
  type: "ReLU"
  bottom: "conv2_1"
  top: "conv2_1"
}
layer {
  name: "conv2_2"
  type: "Convolution"
  bottom: "conv2_1"
  top: "conv2_2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "relu2_2"
  type: "ReLU"
  bottom: "conv2_2"
  top: "conv2_2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2_2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv3_1"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3_1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "relu3_1"
  type: "ReLU"
  bottom: "conv3_1"
  top: "conv3_1"
}
layer {
  name: "conv3_2"
  type: "Convolution"
  bottom: "conv3_1"
  top: "conv3_2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "relu3_2"
  type: "ReLU"
  bottom: "conv3_2"
  top: "conv3_2"
}
layer {
  name: "conv3_3"
  type: "Convolution"
  bottom: "conv3_2"
  top: "conv3_3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "relu3_3"
  type: "ReLU"
  bottom: "conv3_3"
  top: "conv3_3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3_3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv4_1"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4_1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 512
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "relu4_1"
  type: "ReLU"
  bottom: "conv4_1"
  top: "conv4_1"
}
layer {
  name: "conv4_2"
  type: "Convolution"
  bottom: "conv4_1"
  top: "conv4_2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 512
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "relu4_2"
  type: "ReLU"
  bottom: "conv4_2"
  top: "conv4_2"
}
layer {
  name: "conv4_3"
  type: "Convolution"
  bottom: "conv4_2"
  top: "conv4_3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 512
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "relu4_3"
  type: "ReLU"
  bottom: "conv4_3"
  top: "conv4_3"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4_3"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv5_1"
  type: "Convolution"
  bottom: "pool4"
  top: "conv5_1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 512
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "relu5_1"
  type: "ReLU"
  bottom: "conv5_1"
  top: "conv5_1"
}
layer {
  name: "conv5_2"
  type: "Convolution"
  bottom: "conv5_1"
  top: "conv5_2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 512
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "relu5_2"
  type: "ReLU"
  bottom: "conv5_2"
  top: "conv5_2"
}
layer {
  name: "conv5_3"
  type: "Convolution"
  bottom: "conv5_2"
  top: "conv5_3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 512
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "relu5_3"
  type: "ReLU"
  bottom: "conv5_3"
  top: "conv5_3"
}
layer {
  name: "rpn_big_conv1"
  type: "Convolution"
  bottom: "conv5_3"
  top: "rpn_big_conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 512
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      mean: 0
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "rpn_big_conv1_relu"
  type: "ReLU"
  bottom: "rpn_big_conv1"
  top: "rpn_big_conv1"
  relu_param {
    negative_slope: 0
  }
}
layer {
  name: "rpn_big_conv2"
  type: "Convolution"
  bottom: "rpn_big_conv1"
  top: "rpn_big_conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 512
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      mean: 0
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "rpn_big_conv2_relu"
  type: "ReLU"
  bottom: "rpn_big_conv2"
  top: "rpn_big_conv2"
  relu_param {
    negative_slope: 0
  }
}
layer {
  name: "rpn_big_conv2_drop"
  type: "Dropout"
  bottom: "rpn_big_conv2"
  top: "rpn_big_conv2"
  dropout_param {
    dropout_ratio: 0.3
  }
}
layer {
  name: "rpn_big_output_cls"
  type: "Convolution"
  bottom: "rpn_big_conv2"
  top: "rpn_big_output_cls"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      mean: 0
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "rpn_big_output_cls_relu"
  type: "ReLU"
  bottom: "rpn_big_output_cls"
  top: "rpn_big_output_cls"
  relu_param {
    negative_slope: 0
  }
}
layer {
  name: "rpn_big_cls_score"
  type: "Convolution"
  bottom: "rpn_big_output_cls"
  top: "rpn_big_cls_score"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 52
    pad: 0
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      mean: 0
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "rpn_big_cls_score_reshape"
  type: "Reshape"
  bottom: "rpn_big_cls_score"
  top: "rpn_big_cls_score_reshape"
  reshape_param {
    shape {
      dim: 0
      dim: 2
      dim: -1
      dim: 0
    }
  }
}
layer {
  name: "rpn_big_cls_prob"
  type: "Softmax"
  bottom: "rpn_big_cls_score_reshape"
  top: "rpn_big_cls_prob"
}
layer {
  name: "rpn_big_cls_prob_reshape"
  type: "Reshape"
  bottom: "rpn_big_cls_prob"
  top: "rpn_big_cls_prob_reshape"
  reshape_param {
    shape {
      dim: 0
      dim: 52
      dim: -1
      dim: 0
    }
  }
}
layer {
  name: "rpn_big_anchor_target"
  type: "Python"
  bottom: "rpn_big_cls_score"
  bottom: "input_gt_boxes"
  bottom: "input_ignored_boxes"
  bottom: "input_im_info"
  bottom: "input_data"
  bottom: "rpn_big_cls_prob_reshape"
  top: "rpn_big_anchor_target_labels"
  top: "rpn_big_anchor_target_bbox_targets"
  top: "rpn_big_anchor_target_inside_weights"
  top: "rpn_big_anchor_target_outside_weights"
  python_param {
    module: "rpn.anchor_target_layer"
    layer: "AnchorTargetLayer"
    param_str: "{\"anchor_scales\": [2.5, 3.025, 3.66025, 4.4289024999999995, 5.358972025, 6.484356150249999, 7.846070941802498, 9.493745839581022, 11.487432465893036, 13.899793283730574, 16.818749873313998, 20.350687346709933, 24.62433168951902, 29.795441344318014], \"fg_fraction\": 0.5, \"batchsize\": 256, \"positive_overlap\": 0.6, \"name\": \"rpn_big\", \"tn_fraction\": 0.2, \"negative_overlap\": 0.4, \"feat_stride\": 16, \"anchor_ratios\": [1.0], \"num_classes\": 1, \"anchor_shift_num_xy\": [[2, 2], [2, 2], [2, 2], [2, 2], [1, 1], [1, 1], [1, 1], [1, 1], [1, 1], [1, 1], [1, 1], [1, 1], [1, 1], [1, 1]], \"square_targets\": true}"
  }
}
layer {
  name: "rpn_big_loss_cls"
  type: "SoftmaxWithLoss"
  bottom: "rpn_big_cls_score_reshape"
  bottom: "rpn_big_anchor_target_labels"
  top: "rpn_big_loss_cls"
  loss_weight: 1
  propagate_down: true
  propagate_down: false
  loss_param {
    ignore_label: -1
    normalization: VALID
  }
}
layer {
  name: "rpn_big_output_box"
  type: "Convolution"
  bottom: "rpn_big_conv2"
  top: "rpn_big_output_box"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      mean: 0
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "rpn_big_output_box_relu"
  type: "ReLU"
  bottom: "rpn_big_output_box"
  top: "rpn_big_output_box"
  relu_param {
    negative_slope: 0
  }
}
layer {
  name: "rpn_big_bbox_pred"
  type: "Convolution"
  bottom: "rpn_big_output_box"
  top: "rpn_big_bbox_pred"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 104
    pad: 0
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      mean: 0
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "rpn_big_loss_bbox"
  type: "Python"
  bottom: "rpn_big_bbox_pred"
  bottom: "rpn_big_anchor_target_bbox_targets"
  bottom: "rpn_big_anchor_target_inside_weights"
  bottom: "rpn_big_anchor_target_outside_weights"
  top: "rpn_big_loss_bbox"
  loss_weight: 1
  python_param {
    module: "layers.smooth_l1_loss"
    layer: "SmoothL1LossLayer"
    param_str: "{\"sigma\": 3}"
  }
}
layer {
  name: "proposal"
  type: "Python"
  bottom: "rpn_big_cls_prob_reshape"
  bottom: "rpn_big_bbox_pred"
  bottom: "input_im_info"
  top: "rpn_rois"
  python_param {
    module: "rpn.proposal_layer"
    layer: "ProposalLayer"
    param_str: "{\"anchor_shift_num_xy\": [[2, 2], [2, 2], [2, 2], [2, 2], [1, 1], [1, 1], [1, 1], [1, 1], [1, 1], [1, 1], [1, 1], [1, 1], [1, 1], [1, 1]], \"feat_stride\": 16, \"num_classes\": 1, \"anchor_scales\": [2.5, 3.025, 3.66025, 4.4289024999999995, 5.358972025, 6.484356150249999, 7.846070941802498, 9.493745839581022, 11.487432465893036, 13.899793283730574, 16.818749873313998, 20.350687346709933, 24.62433168951902, 29.795441344318014], \"square_targets\": true, \"name\": \"rpn_big\", \"batchsize\": 256, \"anchor_ratios\": [1.0]}"
  }
}
layer {
  name: "roi-data"
  type: "Python"
  bottom: "rpn_rois"
  bottom: "input_gt_boxes"
  top: "rois"
  top: "labels"
  top: "bbox_targets"
  top: "bbox_inside_weights"
  top: "bbox_outside_weights"
  python_param {
    module: "rpn.proposal_target_layer"
    layer: "ProposalTargetLayer"
    param_str: "{\"num_classes\": 2, \"bg_aux_label\": 5532}"
  }
}
layer {
  name: "roi-pool"
  type: "ROIPooling"
  bottom: "conv5_3"
  bottom: "rois"
  top: "roi-pool"
  roi_pooling_param {
    pooled_h: 7
    pooled_w: 7
    spatial_scale: 0.0625
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "roi-pool"
  top: "fc6"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 4096
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 4096
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "bbox_pred"
  type: "InnerProduct"
  bottom: "fc7"
  top: "bbox_pred"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 8
    weight_filler {
      type: "gaussian"
      std: 0.001
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "loss_bbox"
  type: "SmoothL1Loss"
  bottom: "bbox_pred"
  bottom: "bbox_targets"
  bottom: "bbox_inside_weights"
  bottom: "bbox_outside_weights"
  top: "loss_bbox"
  loss_weight: 1
}
layer {
  name: "feat"
  type: "InnerProduct"
  bottom: "fc7"
  top: "feat"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 256
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu8"
  type: "ReLU"
  bottom: "feat"
  top: "feat"
}
layer {
  name: "drop8"
  type: "Dropout"
  bottom: "feat"
  top: "feat"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "pid_score"
  type: "InnerProduct"
  bottom: "feat"
  top: "pid_score"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 5533
    weight_filler {
      type: "gaussian"
      std: 0.001
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
I0413 19:07:03.762406 15276 layer_factory.hpp:77] Creating layer input
I0413 19:07:03.764197 15276 net.cpp:100] Creating Layer input
I0413 19:07:03.764226 15276 net.cpp:408] input -> input_data
I0413 19:07:03.764243 15276 net.cpp:408] input -> input_im_info
I0413 19:07:03.764256 15276 net.cpp:408] input -> input_gt_boxes
I0413 19:07:03.764266 15276 net.cpp:408] input -> input_ignored_boxes
RoiDataLayer: name_to_top: {'ignored_boxes': 3, 'gt_boxes': 2, 'im_info': 1, 'data': 0}
I0413 19:07:04.089218 15276 net.cpp:150] Setting up input
I0413 19:07:04.089248 15276 net.cpp:157] Top shape: 1 3 1000 1500 (4500000)
I0413 19:07:04.089257 15276 net.cpp:157] Top shape: 1 3 (3)
I0413 19:07:04.089263 15276 net.cpp:157] Top shape: 1 5 (5)
I0413 19:07:04.089270 15276 net.cpp:157] Top shape: 1 5 (5)
I0413 19:07:04.089274 15276 net.cpp:165] Memory required for data: 18000052
I0413 19:07:04.089283 15276 layer_factory.hpp:77] Creating layer input_data_input_0_split
I0413 19:07:04.089301 15276 net.cpp:100] Creating Layer input_data_input_0_split
I0413 19:07:04.089310 15276 net.cpp:434] input_data_input_0_split <- input_data
I0413 19:07:04.089323 15276 net.cpp:408] input_data_input_0_split -> input_data_input_0_split_0
I0413 19:07:04.089339 15276 net.cpp:408] input_data_input_0_split -> input_data_input_0_split_1
I0413 19:07:04.089393 15276 net.cpp:150] Setting up input_data_input_0_split
I0413 19:07:04.089404 15276 net.cpp:157] Top shape: 1 3 1000 1500 (4500000)
I0413 19:07:04.089418 15276 net.cpp:157] Top shape: 1 3 1000 1500 (4500000)
I0413 19:07:04.089423 15276 net.cpp:165] Memory required for data: 54000052
I0413 19:07:04.089428 15276 layer_factory.hpp:77] Creating layer input_im_info_input_1_split
I0413 19:07:04.089440 15276 net.cpp:100] Creating Layer input_im_info_input_1_split
I0413 19:07:04.089447 15276 net.cpp:434] input_im_info_input_1_split <- input_im_info
I0413 19:07:04.089454 15276 net.cpp:408] input_im_info_input_1_split -> input_im_info_input_1_split_0
I0413 19:07:04.089464 15276 net.cpp:408] input_im_info_input_1_split -> input_im_info_input_1_split_1
I0413 19:07:04.089516 15276 net.cpp:150] Setting up input_im_info_input_1_split
I0413 19:07:04.089525 15276 net.cpp:157] Top shape: 1 3 (3)
I0413 19:07:04.089531 15276 net.cpp:157] Top shape: 1 3 (3)
I0413 19:07:04.089545 15276 net.cpp:165] Memory required for data: 54000076
I0413 19:07:04.089550 15276 layer_factory.hpp:77] Creating layer input_gt_boxes_input_2_split
I0413 19:07:04.089558 15276 net.cpp:100] Creating Layer input_gt_boxes_input_2_split
I0413 19:07:04.089565 15276 net.cpp:434] input_gt_boxes_input_2_split <- input_gt_boxes
I0413 19:07:04.089571 15276 net.cpp:408] input_gt_boxes_input_2_split -> input_gt_boxes_input_2_split_0
I0413 19:07:04.089584 15276 net.cpp:408] input_gt_boxes_input_2_split -> input_gt_boxes_input_2_split_1
I0413 19:07:04.089627 15276 net.cpp:150] Setting up input_gt_boxes_input_2_split
I0413 19:07:04.089637 15276 net.cpp:157] Top shape: 1 5 (5)
I0413 19:07:04.089644 15276 net.cpp:157] Top shape: 1 5 (5)
I0413 19:07:04.089650 15276 net.cpp:165] Memory required for data: 54000116
I0413 19:07:04.089655 15276 layer_factory.hpp:77] Creating layer conv1_1
I0413 19:07:04.089673 15276 net.cpp:100] Creating Layer conv1_1
I0413 19:07:04.089681 15276 net.cpp:434] conv1_1 <- input_data_input_0_split_0
I0413 19:07:04.089690 15276 net.cpp:408] conv1_1 -> conv1_1
I0413 19:07:04.093025 15276 net.cpp:150] Setting up conv1_1
I0413 19:07:04.093060 15276 net.cpp:157] Top shape: 1 64 1000 1500 (96000000)
I0413 19:07:04.093065 15276 net.cpp:165] Memory required for data: 438000116
I0413 19:07:04.093082 15276 layer_factory.hpp:77] Creating layer relu1_1
I0413 19:07:04.093094 15276 net.cpp:100] Creating Layer relu1_1
I0413 19:07:04.093101 15276 net.cpp:434] relu1_1 <- conv1_1
I0413 19:07:04.093122 15276 net.cpp:395] relu1_1 -> conv1_1 (in-place)
I0413 19:07:04.093139 15276 net.cpp:150] Setting up relu1_1
I0413 19:07:04.093148 15276 net.cpp:157] Top shape: 1 64 1000 1500 (96000000)
I0413 19:07:04.093153 15276 net.cpp:165] Memory required for data: 822000116
I0413 19:07:04.093158 15276 layer_factory.hpp:77] Creating layer conv1_2
I0413 19:07:04.093173 15276 net.cpp:100] Creating Layer conv1_2
I0413 19:07:04.093180 15276 net.cpp:434] conv1_2 <- conv1_1
I0413 19:07:04.093189 15276 net.cpp:408] conv1_2 -> conv1_2
I0413 19:07:04.096612 15276 net.cpp:150] Setting up conv1_2
I0413 19:07:04.096644 15276 net.cpp:157] Top shape: 1 64 1000 1500 (96000000)
I0413 19:07:04.096657 15276 net.cpp:165] Memory required for data: 1206000116
I0413 19:07:04.096673 15276 layer_factory.hpp:77] Creating layer relu1_2
I0413 19:07:04.096689 15276 net.cpp:100] Creating Layer relu1_2
I0413 19:07:04.096698 15276 net.cpp:434] relu1_2 <- conv1_2
I0413 19:07:04.096711 15276 net.cpp:395] relu1_2 -> conv1_2 (in-place)
I0413 19:07:04.096726 15276 net.cpp:150] Setting up relu1_2
I0413 19:07:04.096735 15276 net.cpp:157] Top shape: 1 64 1000 1500 (96000000)
I0413 19:07:04.096740 15276 net.cpp:165] Memory required for data: 1590000116
I0413 19:07:04.096745 15276 layer_factory.hpp:77] Creating layer pool1
I0413 19:07:04.096760 15276 net.cpp:100] Creating Layer pool1
I0413 19:07:04.096767 15276 net.cpp:434] pool1 <- conv1_2
I0413 19:07:04.096778 15276 net.cpp:408] pool1 -> pool1
I0413 19:07:04.096866 15276 net.cpp:150] Setting up pool1
I0413 19:07:04.096879 15276 net.cpp:157] Top shape: 1 64 500 750 (24000000)
I0413 19:07:04.096884 15276 net.cpp:165] Memory required for data: 1686000116
I0413 19:07:04.096889 15276 layer_factory.hpp:77] Creating layer conv2_1
I0413 19:07:04.096920 15276 net.cpp:100] Creating Layer conv2_1
I0413 19:07:04.096930 15276 net.cpp:434] conv2_1 <- pool1
I0413 19:07:04.096938 15276 net.cpp:408] conv2_1 -> conv2_1
I0413 19:07:04.098352 15276 net.cpp:150] Setting up conv2_1
I0413 19:07:04.098376 15276 net.cpp:157] Top shape: 1 128 500 750 (48000000)
I0413 19:07:04.098381 15276 net.cpp:165] Memory required for data: 1878000116
I0413 19:07:04.098395 15276 layer_factory.hpp:77] Creating layer relu2_1
I0413 19:07:04.098409 15276 net.cpp:100] Creating Layer relu2_1
I0413 19:07:04.098417 15276 net.cpp:434] relu2_1 <- conv2_1
I0413 19:07:04.098426 15276 net.cpp:395] relu2_1 -> conv2_1 (in-place)
I0413 19:07:04.098438 15276 net.cpp:150] Setting up relu2_1
I0413 19:07:04.098445 15276 net.cpp:157] Top shape: 1 128 500 750 (48000000)
I0413 19:07:04.098451 15276 net.cpp:165] Memory required for data: 2070000116
I0413 19:07:04.098458 15276 layer_factory.hpp:77] Creating layer conv2_2
I0413 19:07:04.098470 15276 net.cpp:100] Creating Layer conv2_2
I0413 19:07:04.098479 15276 net.cpp:434] conv2_2 <- conv2_1
I0413 19:07:04.098489 15276 net.cpp:408] conv2_2 -> conv2_2
I0413 19:07:04.099584 15276 net.cpp:150] Setting up conv2_2
I0413 19:07:04.099611 15276 net.cpp:157] Top shape: 1 128 500 750 (48000000)
I0413 19:07:04.099616 15276 net.cpp:165] Memory required for data: 2262000116
I0413 19:07:04.099625 15276 layer_factory.hpp:77] Creating layer relu2_2
I0413 19:07:04.099634 15276 net.cpp:100] Creating Layer relu2_2
I0413 19:07:04.099638 15276 net.cpp:434] relu2_2 <- conv2_2
I0413 19:07:04.099647 15276 net.cpp:395] relu2_2 -> conv2_2 (in-place)
I0413 19:07:04.099660 15276 net.cpp:150] Setting up relu2_2
I0413 19:07:04.099669 15276 net.cpp:157] Top shape: 1 128 500 750 (48000000)
I0413 19:07:04.099674 15276 net.cpp:165] Memory required for data: 2454000116
I0413 19:07:04.099679 15276 layer_factory.hpp:77] Creating layer pool2
I0413 19:07:04.099687 15276 net.cpp:100] Creating Layer pool2
I0413 19:07:04.099694 15276 net.cpp:434] pool2 <- conv2_2
I0413 19:07:04.099702 15276 net.cpp:408] pool2 -> pool2
I0413 19:07:04.099767 15276 net.cpp:150] Setting up pool2
I0413 19:07:04.099777 15276 net.cpp:157] Top shape: 1 128 250 375 (12000000)
I0413 19:07:04.099782 15276 net.cpp:165] Memory required for data: 2502000116
I0413 19:07:04.099787 15276 layer_factory.hpp:77] Creating layer conv3_1
I0413 19:07:04.099802 15276 net.cpp:100] Creating Layer conv3_1
I0413 19:07:04.099808 15276 net.cpp:434] conv3_1 <- pool2
I0413 19:07:04.099817 15276 net.cpp:408] conv3_1 -> conv3_1
I0413 19:07:04.101222 15276 net.cpp:150] Setting up conv3_1
I0413 19:07:04.101253 15276 net.cpp:157] Top shape: 1 256 250 375 (24000000)
I0413 19:07:04.101266 15276 net.cpp:165] Memory required for data: 2598000116
I0413 19:07:04.101281 15276 layer_factory.hpp:77] Creating layer relu3_1
I0413 19:07:04.101296 15276 net.cpp:100] Creating Layer relu3_1
I0413 19:07:04.101305 15276 net.cpp:434] relu3_1 <- conv3_1
I0413 19:07:04.101315 15276 net.cpp:395] relu3_1 -> conv3_1 (in-place)
I0413 19:07:04.101330 15276 net.cpp:150] Setting up relu3_1
I0413 19:07:04.101338 15276 net.cpp:157] Top shape: 1 256 250 375 (24000000)
I0413 19:07:04.101344 15276 net.cpp:165] Memory required for data: 2694000116
I0413 19:07:04.101349 15276 layer_factory.hpp:77] Creating layer conv3_2
I0413 19:07:04.101366 15276 net.cpp:100] Creating Layer conv3_2
I0413 19:07:04.101372 15276 net.cpp:434] conv3_2 <- conv3_1
I0413 19:07:04.101382 15276 net.cpp:408] conv3_2 -> conv3_2
I0413 19:07:04.102890 15276 net.cpp:150] Setting up conv3_2
I0413 19:07:04.102921 15276 net.cpp:157] Top shape: 1 256 250 375 (24000000)
I0413 19:07:04.102924 15276 net.cpp:165] Memory required for data: 2790000116
I0413 19:07:04.102936 15276 layer_factory.hpp:77] Creating layer relu3_2
I0413 19:07:04.102951 15276 net.cpp:100] Creating Layer relu3_2
I0413 19:07:04.102959 15276 net.cpp:434] relu3_2 <- conv3_2
I0413 19:07:04.102969 15276 net.cpp:395] relu3_2 -> conv3_2 (in-place)
I0413 19:07:04.102982 15276 net.cpp:150] Setting up relu3_2
I0413 19:07:04.102990 15276 net.cpp:157] Top shape: 1 256 250 375 (24000000)
I0413 19:07:04.103044 15276 net.cpp:165] Memory required for data: 2886000116
I0413 19:07:04.103055 15276 layer_factory.hpp:77] Creating layer conv3_3
I0413 19:07:04.103070 15276 net.cpp:100] Creating Layer conv3_3
I0413 19:07:04.103077 15276 net.cpp:434] conv3_3 <- conv3_2
I0413 19:07:04.103086 15276 net.cpp:408] conv3_3 -> conv3_3
I0413 19:07:04.104946 15276 net.cpp:150] Setting up conv3_3
I0413 19:07:04.104977 15276 net.cpp:157] Top shape: 1 256 250 375 (24000000)
I0413 19:07:04.104981 15276 net.cpp:165] Memory required for data: 2982000116
I0413 19:07:04.104992 15276 layer_factory.hpp:77] Creating layer relu3_3
I0413 19:07:04.105003 15276 net.cpp:100] Creating Layer relu3_3
I0413 19:07:04.105007 15276 net.cpp:434] relu3_3 <- conv3_3
I0413 19:07:04.105015 15276 net.cpp:395] relu3_3 -> conv3_3 (in-place)
I0413 19:07:04.105024 15276 net.cpp:150] Setting up relu3_3
I0413 19:07:04.105028 15276 net.cpp:157] Top shape: 1 256 250 375 (24000000)
I0413 19:07:04.105031 15276 net.cpp:165] Memory required for data: 3078000116
I0413 19:07:04.105034 15276 layer_factory.hpp:77] Creating layer pool3
I0413 19:07:04.105043 15276 net.cpp:100] Creating Layer pool3
I0413 19:07:04.105046 15276 net.cpp:434] pool3 <- conv3_3
I0413 19:07:04.105052 15276 net.cpp:408] pool3 -> pool3
I0413 19:07:04.105094 15276 net.cpp:150] Setting up pool3
I0413 19:07:04.105101 15276 net.cpp:157] Top shape: 1 256 125 188 (6016000)
I0413 19:07:04.105104 15276 net.cpp:165] Memory required for data: 3102064116
I0413 19:07:04.105108 15276 layer_factory.hpp:77] Creating layer conv4_1
I0413 19:07:04.105118 15276 net.cpp:100] Creating Layer conv4_1
I0413 19:07:04.105121 15276 net.cpp:434] conv4_1 <- pool3
I0413 19:07:04.105126 15276 net.cpp:408] conv4_1 -> conv4_1
I0413 19:07:04.108175 15276 net.cpp:150] Setting up conv4_1
I0413 19:07:04.108206 15276 net.cpp:157] Top shape: 1 512 125 188 (12032000)
I0413 19:07:04.108212 15276 net.cpp:165] Memory required for data: 3150192116
I0413 19:07:04.108228 15276 layer_factory.hpp:77] Creating layer relu4_1
I0413 19:07:04.108245 15276 net.cpp:100] Creating Layer relu4_1
I0413 19:07:04.108253 15276 net.cpp:434] relu4_1 <- conv4_1
I0413 19:07:04.108266 15276 net.cpp:395] relu4_1 -> conv4_1 (in-place)
I0413 19:07:04.108280 15276 net.cpp:150] Setting up relu4_1
I0413 19:07:04.108289 15276 net.cpp:157] Top shape: 1 512 125 188 (12032000)
I0413 19:07:04.108294 15276 net.cpp:165] Memory required for data: 3198320116
I0413 19:07:04.108301 15276 layer_factory.hpp:77] Creating layer conv4_2
I0413 19:07:04.108316 15276 net.cpp:100] Creating Layer conv4_2
I0413 19:07:04.108324 15276 net.cpp:434] conv4_2 <- conv4_1
I0413 19:07:04.108336 15276 net.cpp:408] conv4_2 -> conv4_2
I0413 19:07:04.113049 15276 net.cpp:150] Setting up conv4_2
I0413 19:07:04.113086 15276 net.cpp:157] Top shape: 1 512 125 188 (12032000)
I0413 19:07:04.113108 15276 net.cpp:165] Memory required for data: 3246448116
I0413 19:07:04.113133 15276 layer_factory.hpp:77] Creating layer relu4_2
I0413 19:07:04.113150 15276 net.cpp:100] Creating Layer relu4_2
I0413 19:07:04.113158 15276 net.cpp:434] relu4_2 <- conv4_2
I0413 19:07:04.113173 15276 net.cpp:395] relu4_2 -> conv4_2 (in-place)
I0413 19:07:04.113193 15276 net.cpp:150] Setting up relu4_2
I0413 19:07:04.113204 15276 net.cpp:157] Top shape: 1 512 125 188 (12032000)
I0413 19:07:04.113210 15276 net.cpp:165] Memory required for data: 3294576116
I0413 19:07:04.113216 15276 layer_factory.hpp:77] Creating layer conv4_3
I0413 19:07:04.113230 15276 net.cpp:100] Creating Layer conv4_3
I0413 19:07:04.113237 15276 net.cpp:434] conv4_3 <- conv4_2
I0413 19:07:04.113251 15276 net.cpp:408] conv4_3 -> conv4_3
I0413 19:07:04.117475 15276 net.cpp:150] Setting up conv4_3
I0413 19:07:04.117509 15276 net.cpp:157] Top shape: 1 512 125 188 (12032000)
I0413 19:07:04.117514 15276 net.cpp:165] Memory required for data: 3342704116
I0413 19:07:04.117537 15276 layer_factory.hpp:77] Creating layer relu4_3
I0413 19:07:04.117553 15276 net.cpp:100] Creating Layer relu4_3
I0413 19:07:04.117566 15276 net.cpp:434] relu4_3 <- conv4_3
I0413 19:07:04.117579 15276 net.cpp:395] relu4_3 -> conv4_3 (in-place)
I0413 19:07:04.117595 15276 net.cpp:150] Setting up relu4_3
I0413 19:07:04.117604 15276 net.cpp:157] Top shape: 1 512 125 188 (12032000)
I0413 19:07:04.117610 15276 net.cpp:165] Memory required for data: 3390832116
I0413 19:07:04.117617 15276 layer_factory.hpp:77] Creating layer pool4
I0413 19:07:04.117630 15276 net.cpp:100] Creating Layer pool4
I0413 19:07:04.117637 15276 net.cpp:434] pool4 <- conv4_3
I0413 19:07:04.117646 15276 net.cpp:408] pool4 -> pool4
I0413 19:07:04.117694 15276 net.cpp:150] Setting up pool4
I0413 19:07:04.117704 15276 net.cpp:157] Top shape: 1 512 63 94 (3032064)
I0413 19:07:04.117710 15276 net.cpp:165] Memory required for data: 3402960372
I0413 19:07:04.117717 15276 layer_factory.hpp:77] Creating layer conv5_1
I0413 19:07:04.117734 15276 net.cpp:100] Creating Layer conv5_1
I0413 19:07:04.117741 15276 net.cpp:434] conv5_1 <- pool4
I0413 19:07:04.117763 15276 net.cpp:408] conv5_1 -> conv5_1
I0413 19:07:04.122428 15276 net.cpp:150] Setting up conv5_1
I0413 19:07:04.122463 15276 net.cpp:157] Top shape: 1 512 63 94 (3032064)
I0413 19:07:04.122469 15276 net.cpp:165] Memory required for data: 3415088628
I0413 19:07:04.122484 15276 layer_factory.hpp:77] Creating layer relu5_1
I0413 19:07:04.122503 15276 net.cpp:100] Creating Layer relu5_1
I0413 19:07:04.122511 15276 net.cpp:434] relu5_1 <- conv5_1
I0413 19:07:04.122524 15276 net.cpp:395] relu5_1 -> conv5_1 (in-place)
I0413 19:07:04.122537 15276 net.cpp:150] Setting up relu5_1
I0413 19:07:04.122546 15276 net.cpp:157] Top shape: 1 512 63 94 (3032064)
I0413 19:07:04.122553 15276 net.cpp:165] Memory required for data: 3427216884
I0413 19:07:04.122560 15276 layer_factory.hpp:77] Creating layer conv5_2
I0413 19:07:04.122578 15276 net.cpp:100] Creating Layer conv5_2
I0413 19:07:04.122584 15276 net.cpp:434] conv5_2 <- conv5_1
I0413 19:07:04.122597 15276 net.cpp:408] conv5_2 -> conv5_2
I0413 19:07:04.126942 15276 net.cpp:150] Setting up conv5_2
I0413 19:07:04.126979 15276 net.cpp:157] Top shape: 1 512 63 94 (3032064)
I0413 19:07:04.126986 15276 net.cpp:165] Memory required for data: 3439345140
I0413 19:07:04.127001 15276 layer_factory.hpp:77] Creating layer relu5_2
I0413 19:07:04.127015 15276 net.cpp:100] Creating Layer relu5_2
I0413 19:07:04.127022 15276 net.cpp:434] relu5_2 <- conv5_2
I0413 19:07:04.127037 15276 net.cpp:395] relu5_2 -> conv5_2 (in-place)
I0413 19:07:04.127050 15276 net.cpp:150] Setting up relu5_2
I0413 19:07:04.127058 15276 net.cpp:157] Top shape: 1 512 63 94 (3032064)
I0413 19:07:04.127064 15276 net.cpp:165] Memory required for data: 3451473396
I0413 19:07:04.127070 15276 layer_factory.hpp:77] Creating layer conv5_3
I0413 19:07:04.127094 15276 net.cpp:100] Creating Layer conv5_3
I0413 19:07:04.127109 15276 net.cpp:434] conv5_3 <- conv5_2
I0413 19:07:04.127120 15276 net.cpp:408] conv5_3 -> conv5_3
I0413 19:07:04.131438 15276 net.cpp:150] Setting up conv5_3
I0413 19:07:04.131486 15276 net.cpp:157] Top shape: 1 512 63 94 (3032064)
I0413 19:07:04.131492 15276 net.cpp:165] Memory required for data: 3463601652
I0413 19:07:04.131506 15276 layer_factory.hpp:77] Creating layer relu5_3
I0413 19:07:04.131520 15276 net.cpp:100] Creating Layer relu5_3
I0413 19:07:04.131525 15276 net.cpp:434] relu5_3 <- conv5_3
I0413 19:07:04.131536 15276 net.cpp:395] relu5_3 -> conv5_3 (in-place)
I0413 19:07:04.131551 15276 net.cpp:150] Setting up relu5_3
I0413 19:07:04.131558 15276 net.cpp:157] Top shape: 1 512 63 94 (3032064)
I0413 19:07:04.131564 15276 net.cpp:165] Memory required for data: 3475729908
I0413 19:07:04.131570 15276 layer_factory.hpp:77] Creating layer conv5_3_relu5_3_0_split
I0413 19:07:04.131582 15276 net.cpp:100] Creating Layer conv5_3_relu5_3_0_split
I0413 19:07:04.131588 15276 net.cpp:434] conv5_3_relu5_3_0_split <- conv5_3
I0413 19:07:04.131597 15276 net.cpp:408] conv5_3_relu5_3_0_split -> conv5_3_relu5_3_0_split_0
I0413 19:07:04.131609 15276 net.cpp:408] conv5_3_relu5_3_0_split -> conv5_3_relu5_3_0_split_1
I0413 19:07:04.131661 15276 net.cpp:150] Setting up conv5_3_relu5_3_0_split
I0413 19:07:04.131674 15276 net.cpp:157] Top shape: 1 512 63 94 (3032064)
I0413 19:07:04.131681 15276 net.cpp:157] Top shape: 1 512 63 94 (3032064)
I0413 19:07:04.131687 15276 net.cpp:165] Memory required for data: 3499986420
I0413 19:07:04.131695 15276 layer_factory.hpp:77] Creating layer rpn_big_conv1
I0413 19:07:04.131714 15276 net.cpp:100] Creating Layer rpn_big_conv1
I0413 19:07:04.131723 15276 net.cpp:434] rpn_big_conv1 <- conv5_3_relu5_3_0_split_0
I0413 19:07:04.131737 15276 net.cpp:408] rpn_big_conv1 -> rpn_big_conv1
I0413 19:07:04.156430 15276 net.cpp:150] Setting up rpn_big_conv1
I0413 19:07:04.156462 15276 net.cpp:157] Top shape: 1 512 63 94 (3032064)
I0413 19:07:04.156468 15276 net.cpp:165] Memory required for data: 3512114676
I0413 19:07:04.156491 15276 layer_factory.hpp:77] Creating layer rpn_big_conv1_relu
I0413 19:07:04.156507 15276 net.cpp:100] Creating Layer rpn_big_conv1_relu
I0413 19:07:04.156518 15276 net.cpp:434] rpn_big_conv1_relu <- rpn_big_conv1
I0413 19:07:04.156535 15276 net.cpp:395] rpn_big_conv1_relu -> rpn_big_conv1 (in-place)
I0413 19:07:04.156550 15276 net.cpp:150] Setting up rpn_big_conv1_relu
I0413 19:07:04.156560 15276 net.cpp:157] Top shape: 1 512 63 94 (3032064)
I0413 19:07:04.156566 15276 net.cpp:165] Memory required for data: 3524242932
I0413 19:07:04.156574 15276 layer_factory.hpp:77] Creating layer rpn_big_conv2
I0413 19:07:04.156594 15276 net.cpp:100] Creating Layer rpn_big_conv2
I0413 19:07:04.156599 15276 net.cpp:434] rpn_big_conv2 <- rpn_big_conv1
I0413 19:07:04.156612 15276 net.cpp:408] rpn_big_conv2 -> rpn_big_conv2
I0413 19:07:04.181605 15276 net.cpp:150] Setting up rpn_big_conv2
I0413 19:07:04.181635 15276 net.cpp:157] Top shape: 1 512 63 94 (3032064)
I0413 19:07:04.181640 15276 net.cpp:165] Memory required for data: 3536371188
I0413 19:07:04.181663 15276 layer_factory.hpp:77] Creating layer rpn_big_conv2_relu
I0413 19:07:04.181677 15276 net.cpp:100] Creating Layer rpn_big_conv2_relu
I0413 19:07:04.181687 15276 net.cpp:434] rpn_big_conv2_relu <- rpn_big_conv2
I0413 19:07:04.181704 15276 net.cpp:395] rpn_big_conv2_relu -> rpn_big_conv2 (in-place)
I0413 19:07:04.181720 15276 net.cpp:150] Setting up rpn_big_conv2_relu
I0413 19:07:04.181727 15276 net.cpp:157] Top shape: 1 512 63 94 (3032064)
I0413 19:07:04.181733 15276 net.cpp:165] Memory required for data: 3548499444
I0413 19:07:04.181740 15276 layer_factory.hpp:77] Creating layer rpn_big_conv2_drop
I0413 19:07:04.181751 15276 net.cpp:100] Creating Layer rpn_big_conv2_drop
I0413 19:07:04.181757 15276 net.cpp:434] rpn_big_conv2_drop <- rpn_big_conv2
I0413 19:07:04.181769 15276 net.cpp:395] rpn_big_conv2_drop -> rpn_big_conv2 (in-place)
I0413 19:07:04.181802 15276 net.cpp:150] Setting up rpn_big_conv2_drop
I0413 19:07:04.181812 15276 net.cpp:157] Top shape: 1 512 63 94 (3032064)
I0413 19:07:04.181818 15276 net.cpp:165] Memory required for data: 3560627700
I0413 19:07:04.181823 15276 layer_factory.hpp:77] Creating layer rpn_big_conv2_rpn_big_conv2_drop_0_split
I0413 19:07:04.181838 15276 net.cpp:100] Creating Layer rpn_big_conv2_rpn_big_conv2_drop_0_split
I0413 19:07:04.181844 15276 net.cpp:434] rpn_big_conv2_rpn_big_conv2_drop_0_split <- rpn_big_conv2
I0413 19:07:04.181854 15276 net.cpp:408] rpn_big_conv2_rpn_big_conv2_drop_0_split -> rpn_big_conv2_rpn_big_conv2_drop_0_split_0
I0413 19:07:04.181870 15276 net.cpp:408] rpn_big_conv2_rpn_big_conv2_drop_0_split -> rpn_big_conv2_rpn_big_conv2_drop_0_split_1
I0413 19:07:04.181916 15276 net.cpp:150] Setting up rpn_big_conv2_rpn_big_conv2_drop_0_split
I0413 19:07:04.181926 15276 net.cpp:157] Top shape: 1 512 63 94 (3032064)
I0413 19:07:04.181936 15276 net.cpp:157] Top shape: 1 512 63 94 (3032064)
I0413 19:07:04.181941 15276 net.cpp:165] Memory required for data: 3584884212
I0413 19:07:04.181948 15276 layer_factory.hpp:77] Creating layer rpn_big_output_cls
I0413 19:07:04.181967 15276 net.cpp:100] Creating Layer rpn_big_output_cls
I0413 19:07:04.181974 15276 net.cpp:434] rpn_big_output_cls <- rpn_big_conv2_rpn_big_conv2_drop_0_split_0
I0413 19:07:04.181985 15276 net.cpp:408] rpn_big_output_cls -> rpn_big_output_cls
I0413 19:07:04.201139 15276 net.cpp:150] Setting up rpn_big_output_cls
I0413 19:07:04.201174 15276 net.cpp:157] Top shape: 1 384 63 94 (2274048)
I0413 19:07:04.201181 15276 net.cpp:165] Memory required for data: 3593980404
I0413 19:07:04.201195 15276 layer_factory.hpp:77] Creating layer rpn_big_output_cls_relu
I0413 19:07:04.201213 15276 net.cpp:100] Creating Layer rpn_big_output_cls_relu
I0413 19:07:04.201221 15276 net.cpp:434] rpn_big_output_cls_relu <- rpn_big_output_cls
I0413 19:07:04.201234 15276 net.cpp:395] rpn_big_output_cls_relu -> rpn_big_output_cls (in-place)
I0413 19:07:04.201251 15276 net.cpp:150] Setting up rpn_big_output_cls_relu
I0413 19:07:04.201262 15276 net.cpp:157] Top shape: 1 384 63 94 (2274048)
I0413 19:07:04.201268 15276 net.cpp:165] Memory required for data: 3603076596
I0413 19:07:04.201274 15276 layer_factory.hpp:77] Creating layer rpn_big_cls_score
I0413 19:07:04.201293 15276 net.cpp:100] Creating Layer rpn_big_cls_score
I0413 19:07:04.201303 15276 net.cpp:434] rpn_big_cls_score <- rpn_big_output_cls
I0413 19:07:04.201318 15276 net.cpp:408] rpn_big_cls_score -> rpn_big_cls_score
I0413 19:07:04.203685 15276 net.cpp:150] Setting up rpn_big_cls_score
I0413 19:07:04.203706 15276 net.cpp:157] Top shape: 1 52 61 92 (291824)
I0413 19:07:04.203711 15276 net.cpp:165] Memory required for data: 3604243892
I0413 19:07:04.203742 15276 layer_factory.hpp:77] Creating layer rpn_big_cls_score_rpn_big_cls_score_0_split
I0413 19:07:04.203758 15276 net.cpp:100] Creating Layer rpn_big_cls_score_rpn_big_cls_score_0_split
I0413 19:07:04.203766 15276 net.cpp:434] rpn_big_cls_score_rpn_big_cls_score_0_split <- rpn_big_cls_score
I0413 19:07:04.203778 15276 net.cpp:408] rpn_big_cls_score_rpn_big_cls_score_0_split -> rpn_big_cls_score_rpn_big_cls_score_0_split_0
I0413 19:07:04.203794 15276 net.cpp:408] rpn_big_cls_score_rpn_big_cls_score_0_split -> rpn_big_cls_score_rpn_big_cls_score_0_split_1
I0413 19:07:04.203840 15276 net.cpp:150] Setting up rpn_big_cls_score_rpn_big_cls_score_0_split
I0413 19:07:04.203850 15276 net.cpp:157] Top shape: 1 52 61 92 (291824)
I0413 19:07:04.203858 15276 net.cpp:157] Top shape: 1 52 61 92 (291824)
I0413 19:07:04.203865 15276 net.cpp:165] Memory required for data: 3606578484
I0413 19:07:04.203871 15276 layer_factory.hpp:77] Creating layer rpn_big_cls_score_reshape
I0413 19:07:04.203882 15276 net.cpp:100] Creating Layer rpn_big_cls_score_reshape
I0413 19:07:04.203889 15276 net.cpp:434] rpn_big_cls_score_reshape <- rpn_big_cls_score_rpn_big_cls_score_0_split_0
I0413 19:07:04.203902 15276 net.cpp:408] rpn_big_cls_score_reshape -> rpn_big_cls_score_reshape
I0413 19:07:04.203935 15276 net.cpp:150] Setting up rpn_big_cls_score_reshape
I0413 19:07:04.203945 15276 net.cpp:157] Top shape: 1 2 1586 92 (291824)
I0413 19:07:04.203951 15276 net.cpp:165] Memory required for data: 3607745780
I0413 19:07:04.203958 15276 layer_factory.hpp:77] Creating layer rpn_big_cls_score_reshape_rpn_big_cls_score_reshape_0_split
I0413 19:07:04.203968 15276 net.cpp:100] Creating Layer rpn_big_cls_score_reshape_rpn_big_cls_score_reshape_0_split
I0413 19:07:04.203975 15276 net.cpp:434] rpn_big_cls_score_reshape_rpn_big_cls_score_reshape_0_split <- rpn_big_cls_score_reshape
I0413 19:07:04.203987 15276 net.cpp:408] rpn_big_cls_score_reshape_rpn_big_cls_score_reshape_0_split -> rpn_big_cls_score_reshape_rpn_big_cls_score_reshape_0_split_0
I0413 19:07:04.204000 15276 net.cpp:408] rpn_big_cls_score_reshape_rpn_big_cls_score_reshape_0_split -> rpn_big_cls_score_reshape_rpn_big_cls_score_reshape_0_split_1
I0413 19:07:04.204042 15276 net.cpp:150] Setting up rpn_big_cls_score_reshape_rpn_big_cls_score_reshape_0_split
I0413 19:07:04.204051 15276 net.cpp:157] Top shape: 1 2 1586 92 (291824)
I0413 19:07:04.204059 15276 net.cpp:157] Top shape: 1 2 1586 92 (291824)
I0413 19:07:04.204066 15276 net.cpp:165] Memory required for data: 3610080372
I0413 19:07:04.204073 15276 layer_factory.hpp:77] Creating layer rpn_big_cls_prob
I0413 19:07:04.204085 15276 net.cpp:100] Creating Layer rpn_big_cls_prob
I0413 19:07:04.204092 15276 net.cpp:434] rpn_big_cls_prob <- rpn_big_cls_score_reshape_rpn_big_cls_score_reshape_0_split_0
I0413 19:07:04.204104 15276 net.cpp:408] rpn_big_cls_prob -> rpn_big_cls_prob
I0413 19:07:04.204169 15276 net.cpp:150] Setting up rpn_big_cls_prob
I0413 19:07:04.204180 15276 net.cpp:157] Top shape: 1 2 1586 92 (291824)
I0413 19:07:04.204186 15276 net.cpp:165] Memory required for data: 3611247668
I0413 19:07:04.204191 15276 layer_factory.hpp:77] Creating layer rpn_big_cls_prob_reshape
I0413 19:07:04.204198 15276 net.cpp:100] Creating Layer rpn_big_cls_prob_reshape
I0413 19:07:04.204205 15276 net.cpp:434] rpn_big_cls_prob_reshape <- rpn_big_cls_prob
I0413 19:07:04.204213 15276 net.cpp:408] rpn_big_cls_prob_reshape -> rpn_big_cls_prob_reshape
I0413 19:07:04.204253 15276 net.cpp:150] Setting up rpn_big_cls_prob_reshape
I0413 19:07:04.204264 15276 net.cpp:157] Top shape: 1 52 61 92 (291824)
I0413 19:07:04.204273 15276 net.cpp:165] Memory required for data: 3612414964
I0413 19:07:04.204278 15276 layer_factory.hpp:77] Creating layer rpn_big_cls_prob_reshape_rpn_big_cls_prob_reshape_0_split
I0413 19:07:04.204286 15276 net.cpp:100] Creating Layer rpn_big_cls_prob_reshape_rpn_big_cls_prob_reshape_0_split
I0413 19:07:04.204293 15276 net.cpp:434] rpn_big_cls_prob_reshape_rpn_big_cls_prob_reshape_0_split <- rpn_big_cls_prob_reshape
I0413 19:07:04.204303 15276 net.cpp:408] rpn_big_cls_prob_reshape_rpn_big_cls_prob_reshape_0_split -> rpn_big_cls_prob_reshape_rpn_big_cls_prob_reshape_0_split_0
I0413 19:07:04.204313 15276 net.cpp:408] rpn_big_cls_prob_reshape_rpn_big_cls_prob_reshape_0_split -> rpn_big_cls_prob_reshape_rpn_big_cls_prob_reshape_0_split_1
I0413 19:07:04.204358 15276 net.cpp:150] Setting up rpn_big_cls_prob_reshape_rpn_big_cls_prob_reshape_0_split
I0413 19:07:04.204370 15276 net.cpp:157] Top shape: 1 52 61 92 (291824)
I0413 19:07:04.204380 15276 net.cpp:157] Top shape: 1 52 61 92 (291824)
I0413 19:07:04.204385 15276 net.cpp:165] Memory required for data: 3614749556
I0413 19:07:04.204391 15276 layer_factory.hpp:77] Creating layer rpn_big_anchor_target
I0413 19:07:04.206328 15276 net.cpp:100] Creating Layer rpn_big_anchor_target
I0413 19:07:04.206354 15276 net.cpp:434] rpn_big_anchor_target <- rpn_big_cls_score_rpn_big_cls_score_0_split_1
I0413 19:07:04.206375 15276 net.cpp:434] rpn_big_anchor_target <- input_gt_boxes_input_2_split_0
I0413 19:07:04.206384 15276 net.cpp:434] rpn_big_anchor_target <- input_ignored_boxes
I0413 19:07:04.206392 15276 net.cpp:434] rpn_big_anchor_target <- input_im_info_input_1_split_0
I0413 19:07:04.206399 15276 net.cpp:434] rpn_big_anchor_target <- input_data_input_0_split_1
I0413 19:07:04.206409 15276 net.cpp:434] rpn_big_anchor_target <- rpn_big_cls_prob_reshape_rpn_big_cls_prob_reshape_0_split_0
I0413 19:07:04.206423 15276 net.cpp:408] rpn_big_anchor_target -> rpn_big_anchor_target_labels
I0413 19:07:04.206440 15276 net.cpp:408] rpn_big_anchor_target -> rpn_big_anchor_target_bbox_targets
I0413 19:07:04.206456 15276 net.cpp:408] rpn_big_anchor_target -> rpn_big_anchor_target_inside_weights
I0413 19:07:04.206472 15276 net.cpp:408] rpn_big_anchor_target -> rpn_big_anchor_target_outside_weights
I0413 19:07:04.214078 15276 net.cpp:150] Setting up rpn_big_anchor_target
I0413 19:07:04.214107 15276 net.cpp:157] Top shape: 1 1 1586 92 (145912)
I0413 19:07:04.214120 15276 net.cpp:157] Top shape: 1 104 61 92 (583648)
I0413 19:07:04.214124 15276 net.cpp:157] Top shape: 1 104 61 92 (583648)
I0413 19:07:04.214128 15276 net.cpp:157] Top shape: 1 104 61 92 (583648)
I0413 19:07:04.214131 15276 net.cpp:165] Memory required for data: 3622336980
I0413 19:07:04.214138 15276 layer_factory.hpp:77] Creating layer rpn_big_loss_cls
I0413 19:07:04.214149 15276 net.cpp:100] Creating Layer rpn_big_loss_cls
I0413 19:07:04.214155 15276 net.cpp:434] rpn_big_loss_cls <- rpn_big_cls_score_reshape_rpn_big_cls_score_reshape_0_split_1
I0413 19:07:04.214162 15276 net.cpp:434] rpn_big_loss_cls <- rpn_big_anchor_target_labels
I0413 19:07:04.214169 15276 net.cpp:408] rpn_big_loss_cls -> rpn_big_loss_cls
I0413 19:07:04.214182 15276 layer_factory.hpp:77] Creating layer rpn_big_loss_cls
I0413 19:07:04.214815 15276 net.cpp:150] Setting up rpn_big_loss_cls
I0413 19:07:04.214829 15276 net.cpp:157] Top shape: (1)
I0413 19:07:04.214833 15276 net.cpp:160]     with loss weight 1
I0413 19:07:04.214853 15276 net.cpp:165] Memory required for data: 3622336984
I0413 19:07:04.214857 15276 layer_factory.hpp:77] Creating layer rpn_big_output_box
I0413 19:07:04.214872 15276 net.cpp:100] Creating Layer rpn_big_output_box
I0413 19:07:04.214877 15276 net.cpp:434] rpn_big_output_box <- rpn_big_conv2_rpn_big_conv2_drop_0_split_1
I0413 19:07:04.214884 15276 net.cpp:408] rpn_big_output_box -> rpn_big_output_box
I0413 19:07:04.233914 15276 net.cpp:150] Setting up rpn_big_output_box
I0413 19:07:04.233944 15276 net.cpp:157] Top shape: 1 384 63 94 (2274048)
I0413 19:07:04.233947 15276 net.cpp:165] Memory required for data: 3631433176
I0413 19:07:04.233958 15276 layer_factory.hpp:77] Creating layer rpn_big_output_box_relu
I0413 19:07:04.233968 15276 net.cpp:100] Creating Layer rpn_big_output_box_relu
I0413 19:07:04.233974 15276 net.cpp:434] rpn_big_output_box_relu <- rpn_big_output_box
I0413 19:07:04.233983 15276 net.cpp:395] rpn_big_output_box_relu -> rpn_big_output_box (in-place)
I0413 19:07:04.233992 15276 net.cpp:150] Setting up rpn_big_output_box_relu
I0413 19:07:04.233997 15276 net.cpp:157] Top shape: 1 384 63 94 (2274048)
I0413 19:07:04.234000 15276 net.cpp:165] Memory required for data: 3640529368
I0413 19:07:04.234004 15276 layer_factory.hpp:77] Creating layer rpn_big_bbox_pred
I0413 19:07:04.234014 15276 net.cpp:100] Creating Layer rpn_big_bbox_pred
I0413 19:07:04.234017 15276 net.cpp:434] rpn_big_bbox_pred <- rpn_big_output_box
I0413 19:07:04.234025 15276 net.cpp:408] rpn_big_bbox_pred -> rpn_big_bbox_pred
I0413 19:07:04.238095 15276 net.cpp:150] Setting up rpn_big_bbox_pred
I0413 19:07:04.238122 15276 net.cpp:157] Top shape: 1 104 61 92 (583648)
I0413 19:07:04.238131 15276 net.cpp:165] Memory required for data: 3642863960
I0413 19:07:04.238143 15276 layer_factory.hpp:77] Creating layer rpn_big_bbox_pred_rpn_big_bbox_pred_0_split
I0413 19:07:04.238158 15276 net.cpp:100] Creating Layer rpn_big_bbox_pred_rpn_big_bbox_pred_0_split
I0413 19:07:04.238168 15276 net.cpp:434] rpn_big_bbox_pred_rpn_big_bbox_pred_0_split <- rpn_big_bbox_pred
I0413 19:07:04.238179 15276 net.cpp:408] rpn_big_bbox_pred_rpn_big_bbox_pred_0_split -> rpn_big_bbox_pred_rpn_big_bbox_pred_0_split_0
I0413 19:07:04.238193 15276 net.cpp:408] rpn_big_bbox_pred_rpn_big_bbox_pred_0_split -> rpn_big_bbox_pred_rpn_big_bbox_pred_0_split_1
I0413 19:07:04.238243 15276 net.cpp:150] Setting up rpn_big_bbox_pred_rpn_big_bbox_pred_0_split
I0413 19:07:04.238255 15276 net.cpp:157] Top shape: 1 104 61 92 (583648)
I0413 19:07:04.238276 15276 net.cpp:157] Top shape: 1 104 61 92 (583648)
I0413 19:07:04.238281 15276 net.cpp:165] Memory required for data: 3647533144
I0413 19:07:04.238288 15276 layer_factory.hpp:77] Creating layer rpn_big_loss_bbox
I0413 19:07:04.238705 15276 net.cpp:100] Creating Layer rpn_big_loss_bbox
I0413 19:07:04.238718 15276 net.cpp:434] rpn_big_loss_bbox <- rpn_big_bbox_pred_rpn_big_bbox_pred_0_split_0
I0413 19:07:04.238724 15276 net.cpp:434] rpn_big_loss_bbox <- rpn_big_anchor_target_bbox_targets
I0413 19:07:04.238729 15276 net.cpp:434] rpn_big_loss_bbox <- rpn_big_anchor_target_inside_weights
I0413 19:07:04.238734 15276 net.cpp:434] rpn_big_loss_bbox <- rpn_big_anchor_target_outside_weights
I0413 19:07:04.238740 15276 net.cpp:408] rpn_big_loss_bbox -> rpn_big_loss_bbox
I0413 19:07:04.240753 15276 net.cpp:150] Setting up rpn_big_loss_bbox
I0413 19:07:04.240782 15276 net.cpp:157] Top shape: 1 (1)
I0413 19:07:04.240787 15276 net.cpp:160]     with loss weight 1
I0413 19:07:04.240805 15276 net.cpp:165] Memory required for data: 3647533148
I0413 19:07:04.240813 15276 layer_factory.hpp:77] Creating layer proposal
I0413 19:07:04.241225 15276 net.cpp:100] Creating Layer proposal
I0413 19:07:04.241236 15276 net.cpp:434] proposal <- rpn_big_cls_prob_reshape_rpn_big_cls_prob_reshape_0_split_1
I0413 19:07:04.241245 15276 net.cpp:434] proposal <- rpn_big_bbox_pred_rpn_big_bbox_pred_0_split_1
I0413 19:07:04.241250 15276 net.cpp:434] proposal <- input_im_info_input_1_split_1
I0413 19:07:04.241256 15276 net.cpp:408] proposal -> rpn_rois
I0413 19:07:04.245406 15276 net.cpp:150] Setting up proposal
I0413 19:07:04.245426 15276 net.cpp:157] Top shape: 1 5 (5)
I0413 19:07:04.245430 15276 net.cpp:165] Memory required for data: 3647533168
I0413 19:07:04.245434 15276 layer_factory.hpp:77] Creating layer roi-data
I0413 19:07:04.245762 15276 net.cpp:100] Creating Layer roi-data
I0413 19:07:04.245774 15276 net.cpp:434] roi-data <- rpn_rois
I0413 19:07:04.245780 15276 net.cpp:434] roi-data <- input_gt_boxes_input_2_split_1
I0413 19:07:04.245789 15276 net.cpp:408] roi-data -> rois
I0413 19:07:04.245797 15276 net.cpp:408] roi-data -> labels
I0413 19:07:04.245806 15276 net.cpp:408] roi-data -> bbox_targets
I0413 19:07:04.245815 15276 net.cpp:408] roi-data -> bbox_inside_weights
I0413 19:07:04.245821 15276 net.cpp:408] roi-data -> bbox_outside_weights
I0413 19:07:04.246752 15276 net.cpp:150] Setting up roi-data
I0413 19:07:04.246767 15276 net.cpp:157] Top shape: 1 5 (5)
I0413 19:07:04.246772 15276 net.cpp:157] Top shape: 1 1 (1)
I0413 19:07:04.246774 15276 net.cpp:157] Top shape: 1 8 (8)
I0413 19:07:04.246778 15276 net.cpp:157] Top shape: 1 8 (8)
I0413 19:07:04.246781 15276 net.cpp:157] Top shape: 1 8 (8)
I0413 19:07:04.246784 15276 net.cpp:165] Memory required for data: 3647533288
I0413 19:07:04.246788 15276 layer_factory.hpp:77] Creating layer roi-pool
I0413 19:07:04.246795 15276 net.cpp:100] Creating Layer roi-pool
I0413 19:07:04.246799 15276 net.cpp:434] roi-pool <- conv5_3_relu5_3_0_split_1
I0413 19:07:04.246805 15276 net.cpp:434] roi-pool <- rois
I0413 19:07:04.246810 15276 net.cpp:408] roi-pool -> roi-pool
I0413 19:07:04.246821 15276 roi_pooling_layer.cpp:33] Spatial scale: 0.0625
I0413 19:07:04.246832 15276 roi_pooling_layer.cpp:34] Box scale: 1
I0413 19:07:04.246870 15276 net.cpp:150] Setting up roi-pool
I0413 19:07:04.246877 15276 net.cpp:157] Top shape: 1 512 7 7 (25088)
I0413 19:07:04.246881 15276 net.cpp:165] Memory required for data: 3647633640
I0413 19:07:04.246883 15276 layer_factory.hpp:77] Creating layer fc6
I0413 19:07:04.246891 15276 net.cpp:100] Creating Layer fc6
I0413 19:07:04.246896 15276 net.cpp:434] fc6 <- roi-pool
I0413 19:07:04.246901 15276 net.cpp:408] fc6 -> fc6
I0413 19:07:04.410985 15276 net.cpp:150] Setting up fc6
I0413 19:07:04.411020 15276 net.cpp:157] Top shape: 1 4096 (4096)
I0413 19:07:04.411033 15276 net.cpp:165] Memory required for data: 3647650024
I0413 19:07:04.411046 15276 layer_factory.hpp:77] Creating layer relu6
I0413 19:07:04.411056 15276 net.cpp:100] Creating Layer relu6
I0413 19:07:04.411061 15276 net.cpp:434] relu6 <- fc6
I0413 19:07:04.411069 15276 net.cpp:395] relu6 -> fc6 (in-place)
I0413 19:07:04.411080 15276 net.cpp:150] Setting up relu6
I0413 19:07:04.411084 15276 net.cpp:157] Top shape: 1 4096 (4096)
I0413 19:07:04.411087 15276 net.cpp:165] Memory required for data: 3647666408
I0413 19:07:04.411090 15276 layer_factory.hpp:77] Creating layer drop6
I0413 19:07:04.411097 15276 net.cpp:100] Creating Layer drop6
I0413 19:07:04.411100 15276 net.cpp:434] drop6 <- fc6
I0413 19:07:04.411105 15276 net.cpp:395] drop6 -> fc6 (in-place)
I0413 19:07:04.411128 15276 net.cpp:150] Setting up drop6
I0413 19:07:04.411134 15276 net.cpp:157] Top shape: 1 4096 (4096)
I0413 19:07:04.411137 15276 net.cpp:165] Memory required for data: 3647682792
I0413 19:07:04.411140 15276 layer_factory.hpp:77] Creating layer fc7
I0413 19:07:04.411149 15276 net.cpp:100] Creating Layer fc7
I0413 19:07:04.411152 15276 net.cpp:434] fc7 <- fc6
I0413 19:07:04.411157 15276 net.cpp:408] fc7 -> fc7
I0413 19:07:04.437862 15276 net.cpp:150] Setting up fc7
I0413 19:07:04.437897 15276 net.cpp:157] Top shape: 1 4096 (4096)
I0413 19:07:04.437909 15276 net.cpp:165] Memory required for data: 3647699176
I0413 19:07:04.437922 15276 layer_factory.hpp:77] Creating layer relu7
I0413 19:07:04.437949 15276 net.cpp:100] Creating Layer relu7
I0413 19:07:04.437955 15276 net.cpp:434] relu7 <- fc7
I0413 19:07:04.437963 15276 net.cpp:395] relu7 -> fc7 (in-place)
I0413 19:07:04.437973 15276 net.cpp:150] Setting up relu7
I0413 19:07:04.437978 15276 net.cpp:157] Top shape: 1 4096 (4096)
I0413 19:07:04.437980 15276 net.cpp:165] Memory required for data: 3647715560
I0413 19:07:04.437983 15276 layer_factory.hpp:77] Creating layer drop7
I0413 19:07:04.437989 15276 net.cpp:100] Creating Layer drop7
I0413 19:07:04.437994 15276 net.cpp:434] drop7 <- fc7
I0413 19:07:04.437997 15276 net.cpp:395] drop7 -> fc7 (in-place)
I0413 19:07:04.438019 15276 net.cpp:150] Setting up drop7
I0413 19:07:04.438024 15276 net.cpp:157] Top shape: 1 4096 (4096)
I0413 19:07:04.438027 15276 net.cpp:165] Memory required for data: 3647731944
I0413 19:07:04.438030 15276 layer_factory.hpp:77] Creating layer fc7_drop7_0_split
I0413 19:07:04.438036 15276 net.cpp:100] Creating Layer fc7_drop7_0_split
I0413 19:07:04.438040 15276 net.cpp:434] fc7_drop7_0_split <- fc7
I0413 19:07:04.438045 15276 net.cpp:408] fc7_drop7_0_split -> fc7_drop7_0_split_0
I0413 19:07:04.438052 15276 net.cpp:408] fc7_drop7_0_split -> fc7_drop7_0_split_1
I0413 19:07:04.438091 15276 net.cpp:150] Setting up fc7_drop7_0_split
I0413 19:07:04.438097 15276 net.cpp:157] Top shape: 1 4096 (4096)
I0413 19:07:04.438102 15276 net.cpp:157] Top shape: 1 4096 (4096)
I0413 19:07:04.438104 15276 net.cpp:165] Memory required for data: 3647764712
I0413 19:07:04.438108 15276 layer_factory.hpp:77] Creating layer bbox_pred
I0413 19:07:04.438117 15276 net.cpp:100] Creating Layer bbox_pred
I0413 19:07:04.438122 15276 net.cpp:434] bbox_pred <- fc7_drop7_0_split_0
I0413 19:07:04.438128 15276 net.cpp:408] bbox_pred -> bbox_pred
I0413 19:07:04.438546 15276 net.cpp:150] Setting up bbox_pred
I0413 19:07:04.438555 15276 net.cpp:157] Top shape: 1 8 (8)
I0413 19:07:04.438557 15276 net.cpp:165] Memory required for data: 3647764744
I0413 19:07:04.438563 15276 layer_factory.hpp:77] Creating layer loss_bbox
I0413 19:07:04.438570 15276 net.cpp:100] Creating Layer loss_bbox
I0413 19:07:04.438575 15276 net.cpp:434] loss_bbox <- bbox_pred
I0413 19:07:04.438578 15276 net.cpp:434] loss_bbox <- bbox_targets
I0413 19:07:04.438582 15276 net.cpp:434] loss_bbox <- bbox_inside_weights
I0413 19:07:04.438587 15276 net.cpp:434] loss_bbox <- bbox_outside_weights
I0413 19:07:04.438592 15276 net.cpp:408] loss_bbox -> loss_bbox
I0413 19:07:04.438657 15276 net.cpp:150] Setting up loss_bbox
I0413 19:07:04.438664 15276 net.cpp:157] Top shape: (1)
I0413 19:07:04.438668 15276 net.cpp:160]     with loss weight 1
I0413 19:07:04.438683 15276 net.cpp:165] Memory required for data: 3647764748
I0413 19:07:04.438685 15276 layer_factory.hpp:77] Creating layer feat
I0413 19:07:04.438693 15276 net.cpp:100] Creating Layer feat
I0413 19:07:04.438696 15276 net.cpp:434] feat <- fc7_drop7_0_split_1
I0413 19:07:04.438702 15276 net.cpp:408] feat -> feat
I0413 19:07:04.449766 15276 net.cpp:150] Setting up feat
I0413 19:07:04.449795 15276 net.cpp:157] Top shape: 1 256 (256)
I0413 19:07:04.449797 15276 net.cpp:165] Memory required for data: 3647765772
I0413 19:07:04.449808 15276 layer_factory.hpp:77] Creating layer relu8
I0413 19:07:04.449817 15276 net.cpp:100] Creating Layer relu8
I0413 19:07:04.449822 15276 net.cpp:434] relu8 <- feat
I0413 19:07:04.449829 15276 net.cpp:395] relu8 -> feat (in-place)
I0413 19:07:04.449838 15276 net.cpp:150] Setting up relu8
I0413 19:07:04.449843 15276 net.cpp:157] Top shape: 1 256 (256)
I0413 19:07:04.449846 15276 net.cpp:165] Memory required for data: 3647766796
I0413 19:07:04.449848 15276 layer_factory.hpp:77] Creating layer drop8
I0413 19:07:04.449856 15276 net.cpp:100] Creating Layer drop8
I0413 19:07:04.449859 15276 net.cpp:434] drop8 <- feat
I0413 19:07:04.449863 15276 net.cpp:395] drop8 -> feat (in-place)
I0413 19:07:04.449883 15276 net.cpp:150] Setting up drop8
I0413 19:07:04.449888 15276 net.cpp:157] Top shape: 1 256 (256)
I0413 19:07:04.449892 15276 net.cpp:165] Memory required for data: 3647767820
I0413 19:07:04.449894 15276 layer_factory.hpp:77] Creating layer pid_score
I0413 19:07:04.449901 15276 net.cpp:100] Creating Layer pid_score
I0413 19:07:04.449904 15276 net.cpp:434] pid_score <- feat
I0413 19:07:04.449911 15276 net.cpp:408] pid_score -> pid_score
I0413 19:07:04.464670 15276 net.cpp:150] Setting up pid_score
I0413 19:07:04.464697 15276 net.cpp:157] Top shape: 1 5533 (5533)
I0413 19:07:04.464701 15276 net.cpp:165] Memory required for data: 3647789952
I0413 19:07:04.464721 15276 net.cpp:228] pid_score does not need backward computation.
I0413 19:07:04.464726 15276 net.cpp:228] drop8 does not need backward computation.
I0413 19:07:04.464730 15276 net.cpp:228] relu8 does not need backward computation.
I0413 19:07:04.464733 15276 net.cpp:228] feat does not need backward computation.
I0413 19:07:04.464736 15276 net.cpp:226] loss_bbox needs backward computation.
I0413 19:07:04.464741 15276 net.cpp:226] bbox_pred needs backward computation.
I0413 19:07:04.464745 15276 net.cpp:226] fc7_drop7_0_split needs backward computation.
I0413 19:07:04.464748 15276 net.cpp:226] drop7 needs backward computation.
I0413 19:07:04.464751 15276 net.cpp:226] relu7 needs backward computation.
I0413 19:07:04.464754 15276 net.cpp:226] fc7 needs backward computation.
I0413 19:07:04.464758 15276 net.cpp:226] drop6 needs backward computation.
I0413 19:07:04.464761 15276 net.cpp:226] relu6 needs backward computation.
I0413 19:07:04.464764 15276 net.cpp:226] fc6 needs backward computation.
I0413 19:07:04.464767 15276 net.cpp:226] roi-pool needs backward computation.
I0413 19:07:04.464771 15276 net.cpp:226] roi-data needs backward computation.
I0413 19:07:04.464774 15276 net.cpp:226] proposal needs backward computation.
I0413 19:07:04.464779 15276 net.cpp:226] rpn_big_loss_bbox needs backward computation.
I0413 19:07:04.464784 15276 net.cpp:226] rpn_big_bbox_pred_rpn_big_bbox_pred_0_split needs backward computation.
I0413 19:07:04.464790 15276 net.cpp:226] rpn_big_bbox_pred needs backward computation.
I0413 19:07:04.464794 15276 net.cpp:226] rpn_big_output_box_relu needs backward computation.
I0413 19:07:04.464797 15276 net.cpp:226] rpn_big_output_box needs backward computation.
I0413 19:07:04.464802 15276 net.cpp:226] rpn_big_loss_cls needs backward computation.
I0413 19:07:04.464805 15276 net.cpp:226] rpn_big_anchor_target needs backward computation.
I0413 19:07:04.464813 15276 net.cpp:226] rpn_big_cls_prob_reshape_rpn_big_cls_prob_reshape_0_split needs backward computation.
I0413 19:07:04.464818 15276 net.cpp:226] rpn_big_cls_prob_reshape needs backward computation.
I0413 19:07:04.464821 15276 net.cpp:226] rpn_big_cls_prob needs backward computation.
I0413 19:07:04.464825 15276 net.cpp:226] rpn_big_cls_score_reshape_rpn_big_cls_score_reshape_0_split needs backward computation.
I0413 19:07:04.464829 15276 net.cpp:226] rpn_big_cls_score_reshape needs backward computation.
I0413 19:07:04.464833 15276 net.cpp:226] rpn_big_cls_score_rpn_big_cls_score_0_split needs backward computation.
I0413 19:07:04.464836 15276 net.cpp:226] rpn_big_cls_score needs backward computation.
I0413 19:07:04.464839 15276 net.cpp:226] rpn_big_output_cls_relu needs backward computation.
I0413 19:07:04.464843 15276 net.cpp:226] rpn_big_output_cls needs backward computation.
I0413 19:07:04.464846 15276 net.cpp:226] rpn_big_conv2_rpn_big_conv2_drop_0_split needs backward computation.
I0413 19:07:04.464850 15276 net.cpp:226] rpn_big_conv2_drop needs backward computation.
I0413 19:07:04.464854 15276 net.cpp:226] rpn_big_conv2_relu needs backward computation.
I0413 19:07:04.464856 15276 net.cpp:226] rpn_big_conv2 needs backward computation.
I0413 19:07:04.464860 15276 net.cpp:226] rpn_big_conv1_relu needs backward computation.
I0413 19:07:04.464864 15276 net.cpp:226] rpn_big_conv1 needs backward computation.
I0413 19:07:04.464866 15276 net.cpp:226] conv5_3_relu5_3_0_split needs backward computation.
I0413 19:07:04.464870 15276 net.cpp:226] relu5_3 needs backward computation.
I0413 19:07:04.464874 15276 net.cpp:226] conv5_3 needs backward computation.
I0413 19:07:04.464876 15276 net.cpp:226] relu5_2 needs backward computation.
I0413 19:07:04.464879 15276 net.cpp:226] conv5_2 needs backward computation.
I0413 19:07:04.464884 15276 net.cpp:226] relu5_1 needs backward computation.
I0413 19:07:04.464886 15276 net.cpp:226] conv5_1 needs backward computation.
I0413 19:07:04.464890 15276 net.cpp:226] pool4 needs backward computation.
I0413 19:07:04.464892 15276 net.cpp:226] relu4_3 needs backward computation.
I0413 19:07:04.464895 15276 net.cpp:226] conv4_3 needs backward computation.
I0413 19:07:04.464900 15276 net.cpp:226] relu4_2 needs backward computation.
I0413 19:07:04.464913 15276 net.cpp:226] conv4_2 needs backward computation.
I0413 19:07:04.464917 15276 net.cpp:226] relu4_1 needs backward computation.
I0413 19:07:04.464921 15276 net.cpp:226] conv4_1 needs backward computation.
I0413 19:07:04.464925 15276 net.cpp:226] pool3 needs backward computation.
I0413 19:07:04.464927 15276 net.cpp:226] relu3_3 needs backward computation.
I0413 19:07:04.464931 15276 net.cpp:226] conv3_3 needs backward computation.
I0413 19:07:04.464936 15276 net.cpp:226] relu3_2 needs backward computation.
I0413 19:07:04.464939 15276 net.cpp:226] conv3_2 needs backward computation.
I0413 19:07:04.464942 15276 net.cpp:226] relu3_1 needs backward computation.
I0413 19:07:04.464946 15276 net.cpp:226] conv3_1 needs backward computation.
I0413 19:07:04.464949 15276 net.cpp:228] pool2 does not need backward computation.
I0413 19:07:04.464952 15276 net.cpp:228] relu2_2 does not need backward computation.
I0413 19:07:04.464956 15276 net.cpp:228] conv2_2 does not need backward computation.
I0413 19:07:04.464959 15276 net.cpp:228] relu2_1 does not need backward computation.
I0413 19:07:04.464962 15276 net.cpp:228] conv2_1 does not need backward computation.
I0413 19:07:04.464967 15276 net.cpp:228] pool1 does not need backward computation.
I0413 19:07:04.464969 15276 net.cpp:228] relu1_2 does not need backward computation.
I0413 19:07:04.464972 15276 net.cpp:228] conv1_2 does not need backward computation.
I0413 19:07:04.464977 15276 net.cpp:228] relu1_1 does not need backward computation.
I0413 19:07:04.464979 15276 net.cpp:228] conv1_1 does not need backward computation.
I0413 19:07:04.464983 15276 net.cpp:228] input_gt_boxes_input_2_split does not need backward computation.
I0413 19:07:04.464987 15276 net.cpp:228] input_im_info_input_1_split does not need backward computation.
I0413 19:07:04.464992 15276 net.cpp:228] input_data_input_0_split does not need backward computation.
I0413 19:07:04.464996 15276 net.cpp:228] input does not need backward computation.
I0413 19:07:04.464998 15276 net.cpp:270] This network produces output labels
I0413 19:07:04.465003 15276 net.cpp:270] This network produces output loss_bbox
I0413 19:07:04.465006 15276 net.cpp:270] This network produces output pid_score
I0413 19:07:04.465009 15276 net.cpp:270] This network produces output rpn_big_loss_bbox
I0413 19:07:04.465013 15276 net.cpp:270] This network produces output rpn_big_loss_cls
I0413 19:07:04.465060 15276 net.cpp:283] Network initialization done.
I0413 19:07:04.465209 15276 solver.cpp:60] Solver scaffolding done.
Loading pretrained model weights from /home/mopkobka/CourseWork/DockerShared/gml-nn/data/NET/VGG16.v2.caffemodel
[libprotobuf WARNING google/protobuf/io/coded_stream.cc:537] Reading dangerously large protocol message.  If the message turns out to be larger than 2147483647 bytes, parsing will be halted for security reasons.  To increase the limit (or to disable these warnings), see CodedInputStream::SetTotalBytesLimit() in google/protobuf/io/coded_stream.h.
[libprotobuf WARNING google/protobuf/io/coded_stream.cc:78] The total number of bytes read was 553432430
I0413 19:07:04.809778 15276 upgrade_proto.cpp:67] Attempting to upgrade input file specified using deprecated input fields: /home/mopkobka/CourseWork/DockerShared/gml-nn/data/NET/VGG16.v2.caffemodel
I0413 19:07:04.809804 15276 upgrade_proto.cpp:70] Successfully upgraded file specified using deprecated input fields.
W0413 19:07:04.809813 15276 upgrade_proto.cpp:72] Note that future Caffe releases will only support input layers and not input fields.
I0413 19:07:04.819994 15276 net.cpp:761] Ignoring source layer pool5
I0413 19:07:04.899852 15276 net.cpp:761] Ignoring source layer fc8
I0413 19:07:04.899883 15276 net.cpp:761] Ignoring source layer prob
Solving...
F0413 19:07:05.159803 15276 syncedmem.cpp:56] Check failed: error == cudaSuccess (2 vs. 0)  out of memory
*** Check failure stack trace: ***
exps/train_model.sh: line 26: 15276 Aborted                 (core dumped) python3 ./tools/train_net.py --gpu 0 --cfg exps/${EXP_NAME}/config.yml --exp_dir ${EXP_NAME} ${EXTRA_ARGS}
